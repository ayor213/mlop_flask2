2022-10-14 16:57:02,006:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-14 16:57:02,006:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-14 16:57:02,006:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-14 16:57:02,006:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-14 16:57:04,304:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-14 16:58:48,165:INFO:PyCaret RegressionExperiment
2022-10-14 16:58:48,167:INFO:Logging name: reg-default-name
2022-10-14 16:58:48,167:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-14 16:58:48,167:INFO:version 3.0.0.rc4
2022-10-14 16:58:48,167:INFO:Initializing setup()
2022-10-14 16:58:48,167:INFO:self.USI: 10ac
2022-10-14 16:58:48,167:INFO:self.variable_keys: {'idx', 'exp_id', 'exp_name_log', 'data', 'y_test', 'logging_param', 'pipeline', '_all_models', 'target_param', '_all_models_internal', 'y', '_all_metrics', 'fold_shuffle_param', 'html_param', 'variable_keys', 'transform_target_param', 'master_model_container', 'n_jobs_param', '_ml_usecase', 'gpu_param', 'X_test', 'fold_groups_param', 'log_plots_param', 'display_container', 'seed', 'X_train', '_available_plots', '_gpu_n_jobs_param', 'memory', 'y_train', 'fold_generator', 'transform_target_method_param', 'X', 'USI'}
2022-10-14 16:58:48,167:INFO:Checking environment
2022-10-14 16:58:48,167:INFO:python_version: 3.9.7
2022-10-14 16:58:48,167:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-14 16:58:48,167:INFO:machine: x86_64
2022-10-14 16:58:48,168:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-14 16:58:48,168:INFO:Memory: svmem(total=8589934592, available=2638471168, percent=69.3, used=4803727360, free=43106304, active=2597199872, inactive=2594213888, wired=2206527488)
2022-10-14 16:58:48,168:INFO:Physical Core: 2
2022-10-14 16:58:48,168:INFO:Logical Core: 4
2022-10-14 16:58:48,168:INFO:Checking libraries
2022-10-14 16:58:48,168:INFO:System:
2022-10-14 16:58:48,168:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-14 16:58:48,168:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-14 16:58:48,168:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-14 16:58:48,168:INFO:PyCaret required dependencies:
2022-10-14 16:58:48,168:INFO:                 pip: 21.2.4
2022-10-14 16:58:48,168:INFO:          setuptools: 58.0.4
2022-10-14 16:58:48,169:INFO:             pycaret: 3.0.0rc4
2022-10-14 16:58:48,169:INFO:             IPython: 7.29.0
2022-10-14 16:58:48,169:INFO:          ipywidgets: 7.6.5
2022-10-14 16:58:48,169:INFO:                tqdm: 4.62.3
2022-10-14 16:58:48,169:INFO:               numpy: 1.22.4
2022-10-14 16:58:48,169:INFO:              pandas: 1.4.4
2022-10-14 16:58:48,169:INFO:              jinja2: 3.1.2
2022-10-14 16:58:48,169:INFO:               scipy: 1.8.1
2022-10-14 16:58:48,169:INFO:              joblib: 1.1.0
2022-10-14 16:58:48,169:INFO:             sklearn: 1.0.2
2022-10-14 16:58:48,169:INFO:                pyod: 1.0.5
2022-10-14 16:58:48,169:INFO:            imblearn: 0.9.0
2022-10-14 16:58:48,169:INFO:   category_encoders: 2.5.1.post0
2022-10-14 16:58:48,170:INFO:            lightgbm: 3.3.2
2022-10-14 16:58:48,170:INFO:               numba: 0.55.2
2022-10-14 16:58:48,170:INFO:            requests: 2.28.1
2022-10-14 16:58:48,171:INFO:          matplotlib: 3.4.3
2022-10-14 16:58:48,172:INFO:          scikitplot: 0.3.7
2022-10-14 16:58:48,173:INFO:         yellowbrick: 1.4
2022-10-14 16:58:48,173:INFO:              plotly: 5.5.0
2022-10-14 16:58:48,174:INFO:             kaleido: 0.2.1
2022-10-14 16:58:48,174:INFO:         statsmodels: 0.13.2
2022-10-14 16:58:48,174:INFO:              sktime: 0.13.4
2022-10-14 16:58:48,174:INFO:               tbats: 1.1.1
2022-10-14 16:58:48,174:INFO:            pmdarima: 1.8.5
2022-10-14 16:58:48,174:INFO:              psutil: 5.9.2
2022-10-14 16:58:48,174:INFO:PyCaret optional dependencies:
2022-10-14 16:58:48,188:INFO:                shap: 0.41.0
2022-10-14 16:58:48,188:INFO:           interpret: Not installed
2022-10-14 16:58:48,188:INFO:                umap: Not installed
2022-10-14 16:58:48,189:INFO:    pandas_profiling: Not installed
2022-10-14 16:58:48,189:INFO:  explainerdashboard: Not installed
2022-10-14 16:58:48,189:INFO:             autoviz: Not installed
2022-10-14 16:58:48,189:INFO:           fairlearn: Not installed
2022-10-14 16:58:48,189:INFO:             xgboost: Not installed
2022-10-14 16:58:48,189:INFO:            catboost: 1.1
2022-10-14 16:58:48,189:INFO:              kmodes: Not installed
2022-10-14 16:58:48,189:INFO:             mlxtend: Not installed
2022-10-14 16:58:48,189:INFO:       statsforecast: 1.1.1
2022-10-14 16:58:48,189:INFO:        tune_sklearn: Not installed
2022-10-14 16:58:48,189:INFO:                 ray: Not installed
2022-10-14 16:58:48,189:INFO:            hyperopt: Not installed
2022-10-14 16:58:48,189:INFO:              optuna: Not installed
2022-10-14 16:58:48,189:INFO:               skopt: Not installed
2022-10-14 16:58:48,189:INFO:              mlflow: 1.29.0
2022-10-14 16:58:48,189:INFO:              gradio: Not installed
2022-10-14 16:58:48,190:INFO:             fastapi: Not installed
2022-10-14 16:58:48,190:INFO:             uvicorn: Not installed
2022-10-14 16:58:48,190:INFO:              m2cgen: Not installed
2022-10-14 16:58:48,190:INFO:           evidently: Not installed
2022-10-14 16:58:48,190:INFO:                nltk: 3.6.5
2022-10-14 16:58:48,190:INFO:            pyLDAvis: Not installed
2022-10-14 16:58:48,190:INFO:              gensim: Not installed
2022-10-14 16:58:48,190:INFO:               spacy: Not installed
2022-10-14 16:58:48,190:INFO:           wordcloud: Not installed
2022-10-14 16:58:48,190:INFO:            textblob: Not installed
2022-10-14 16:58:48,190:INFO:               fugue: Not installed
2022-10-14 16:58:48,190:INFO:           streamlit: Not installed
2022-10-14 16:58:48,190:INFO:             prophet: 1.1.1
2022-10-14 16:58:48,190:INFO:None
2022-10-14 16:58:48,191:INFO:Set up data.
2022-10-14 16:58:48,205:INFO:Set up train/test split.
2022-10-14 16:58:48,218:INFO:Set up index.
2022-10-14 16:58:48,220:INFO:Set up folding strategy.
2022-10-14 16:58:48,220:INFO:Assigning column types.
2022-10-14 16:58:48,226:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-14 16:58:48,227:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-14 16:58:48,237:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-14 16:58:48,247:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-14 16:58:48,355:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-14 16:58:48,436:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-14 16:58:48,438:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-14 17:00:49,889:INFO:PyCaret RegressionExperiment
2022-10-14 17:00:49,889:INFO:Logging name: reg-default-name
2022-10-14 17:00:49,890:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-14 17:00:49,890:INFO:version 3.0.0.rc4
2022-10-14 17:00:49,890:INFO:Initializing setup()
2022-10-14 17:00:49,890:INFO:self.USI: 7c7f
2022-10-14 17:00:49,890:INFO:self.variable_keys: {'idx', 'exp_id', 'exp_name_log', 'data', 'y_test', 'logging_param', 'pipeline', '_all_models', 'target_param', '_all_models_internal', 'y', '_all_metrics', 'fold_shuffle_param', 'html_param', 'variable_keys', 'transform_target_param', 'master_model_container', 'n_jobs_param', '_ml_usecase', 'gpu_param', 'X_test', 'fold_groups_param', 'log_plots_param', 'display_container', 'seed', 'X_train', '_available_plots', '_gpu_n_jobs_param', 'memory', 'y_train', 'fold_generator', 'transform_target_method_param', 'X', 'USI'}
2022-10-14 17:00:49,890:INFO:Checking environment
2022-10-14 17:00:49,890:INFO:python_version: 3.9.7
2022-10-14 17:00:49,891:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-14 17:00:49,891:INFO:machine: x86_64
2022-10-14 17:00:49,892:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:00:49,892:INFO:Memory: svmem(total=8589934592, available=2659471360, percent=69.0, used=4820893696, free=27742208, active=2634186752, inactive=2624282624, wired=2186706944)
2022-10-14 17:00:49,892:INFO:Physical Core: 2
2022-10-14 17:00:49,893:INFO:Logical Core: 4
2022-10-14 17:00:49,893:INFO:Checking libraries
2022-10-14 17:00:49,893:INFO:System:
2022-10-14 17:00:49,893:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-14 17:00:49,893:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-14 17:00:49,893:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:00:49,893:INFO:PyCaret required dependencies:
2022-10-14 17:00:49,893:INFO:                 pip: 21.2.4
2022-10-14 17:00:49,893:INFO:          setuptools: 58.0.4
2022-10-14 17:00:49,894:INFO:             pycaret: 3.0.0rc4
2022-10-14 17:00:49,894:INFO:             IPython: 7.29.0
2022-10-14 17:00:49,894:INFO:          ipywidgets: 7.6.5
2022-10-14 17:00:49,894:INFO:                tqdm: 4.62.3
2022-10-14 17:00:49,894:INFO:               numpy: 1.22.4
2022-10-14 17:00:49,894:INFO:              pandas: 1.4.4
2022-10-14 17:00:49,894:INFO:              jinja2: 3.1.2
2022-10-14 17:00:49,894:INFO:               scipy: 1.8.1
2022-10-14 17:00:49,894:INFO:              joblib: 1.1.0
2022-10-14 17:00:49,894:INFO:             sklearn: 1.0.2
2022-10-14 17:00:49,894:INFO:                pyod: 1.0.5
2022-10-14 17:00:49,894:INFO:            imblearn: 0.9.0
2022-10-14 17:00:49,894:INFO:   category_encoders: 2.5.1.post0
2022-10-14 17:00:49,895:INFO:            lightgbm: 3.3.2
2022-10-14 17:00:49,895:INFO:               numba: 0.55.2
2022-10-14 17:00:49,895:INFO:            requests: 2.28.1
2022-10-14 17:00:49,895:INFO:          matplotlib: 3.4.3
2022-10-14 17:00:49,895:INFO:          scikitplot: 0.3.7
2022-10-14 17:00:49,895:INFO:         yellowbrick: 1.4
2022-10-14 17:00:49,895:INFO:              plotly: 5.5.0
2022-10-14 17:00:49,895:INFO:             kaleido: 0.2.1
2022-10-14 17:00:49,895:INFO:         statsmodels: 0.13.2
2022-10-14 17:00:49,895:INFO:              sktime: 0.13.4
2022-10-14 17:00:49,895:INFO:               tbats: 1.1.1
2022-10-14 17:00:49,895:INFO:            pmdarima: 1.8.5
2022-10-14 17:00:49,895:INFO:              psutil: 5.9.2
2022-10-14 17:00:49,896:INFO:PyCaret optional dependencies:
2022-10-14 17:00:49,896:INFO:                shap: 0.41.0
2022-10-14 17:00:49,896:INFO:           interpret: Not installed
2022-10-14 17:00:49,896:INFO:                umap: Not installed
2022-10-14 17:00:49,896:INFO:    pandas_profiling: Not installed
2022-10-14 17:00:49,896:INFO:  explainerdashboard: Not installed
2022-10-14 17:00:49,896:INFO:             autoviz: Not installed
2022-10-14 17:00:49,896:INFO:           fairlearn: Not installed
2022-10-14 17:00:49,896:INFO:             xgboost: Not installed
2022-10-14 17:00:49,897:INFO:            catboost: 1.1
2022-10-14 17:00:49,897:INFO:              kmodes: Not installed
2022-10-14 17:00:49,897:INFO:             mlxtend: Not installed
2022-10-14 17:00:49,897:INFO:       statsforecast: 1.1.1
2022-10-14 17:00:49,897:INFO:        tune_sklearn: Not installed
2022-10-14 17:00:49,897:INFO:                 ray: Not installed
2022-10-14 17:00:49,897:INFO:            hyperopt: Not installed
2022-10-14 17:00:49,897:INFO:              optuna: Not installed
2022-10-14 17:00:49,897:INFO:               skopt: Not installed
2022-10-14 17:00:49,897:INFO:              mlflow: 1.29.0
2022-10-14 17:00:49,897:INFO:              gradio: Not installed
2022-10-14 17:00:49,897:INFO:             fastapi: Not installed
2022-10-14 17:00:49,897:INFO:             uvicorn: Not installed
2022-10-14 17:00:49,897:INFO:              m2cgen: Not installed
2022-10-14 17:00:49,897:INFO:           evidently: Not installed
2022-10-14 17:00:49,897:INFO:                nltk: 3.6.5
2022-10-14 17:00:49,897:INFO:            pyLDAvis: Not installed
2022-10-14 17:00:49,897:INFO:              gensim: Not installed
2022-10-14 17:00:49,897:INFO:               spacy: Not installed
2022-10-14 17:00:49,897:INFO:           wordcloud: Not installed
2022-10-14 17:00:49,898:INFO:            textblob: Not installed
2022-10-14 17:00:49,898:INFO:               fugue: Not installed
2022-10-14 17:00:49,898:INFO:           streamlit: Not installed
2022-10-14 17:00:49,898:INFO:             prophet: 1.1.1
2022-10-14 17:00:49,898:INFO:None
2022-10-14 17:00:49,898:INFO:Set up data.
2022-10-14 17:00:49,913:INFO:Set up train/test split.
2022-10-14 17:00:49,921:INFO:Set up index.
2022-10-14 17:00:49,922:INFO:Set up folding strategy.
2022-10-14 17:00:49,922:INFO:Assigning column types.
2022-10-14 17:00:49,929:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-14 17:00:49,930:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-14 17:00:49,939:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-14 17:00:49,948:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-14 17:00:50,051:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-14 17:00:50,130:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-14 17:00:50,131:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-14 17:04:43,282:INFO:PyCaret RegressionExperiment
2022-10-14 17:04:43,287:INFO:Logging name: reg-default-name
2022-10-14 17:04:43,287:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-14 17:04:43,287:INFO:version 3.0.0.rc4
2022-10-14 17:04:43,287:INFO:Initializing setup()
2022-10-14 17:04:43,287:INFO:self.USI: b712
2022-10-14 17:04:43,287:INFO:self.variable_keys: {'idx', 'exp_id', 'exp_name_log', 'data', 'y_test', 'logging_param', 'pipeline', '_all_models', 'target_param', '_all_models_internal', 'y', '_all_metrics', 'fold_shuffle_param', 'html_param', 'variable_keys', 'transform_target_param', 'master_model_container', 'n_jobs_param', '_ml_usecase', 'gpu_param', 'X_test', 'fold_groups_param', 'log_plots_param', 'display_container', 'seed', 'X_train', '_available_plots', '_gpu_n_jobs_param', 'memory', 'y_train', 'fold_generator', 'transform_target_method_param', 'X', 'USI'}
2022-10-14 17:04:43,287:INFO:Checking environment
2022-10-14 17:04:43,288:INFO:python_version: 3.9.7
2022-10-14 17:04:43,288:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-14 17:04:43,288:INFO:machine: x86_64
2022-10-14 17:04:43,288:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:04:43,288:INFO:Memory: svmem(total=8589934592, available=2236502016, percent=74.0, used=4456341504, free=32059392, active=2207014912, inactive=2201419776, wired=2249326592)
2022-10-14 17:04:43,288:INFO:Physical Core: 2
2022-10-14 17:04:43,288:INFO:Logical Core: 4
2022-10-14 17:04:43,289:INFO:Checking libraries
2022-10-14 17:04:43,289:INFO:System:
2022-10-14 17:04:43,289:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-14 17:04:43,289:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-14 17:04:43,289:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:04:43,289:INFO:PyCaret required dependencies:
2022-10-14 17:04:43,289:INFO:                 pip: 21.2.4
2022-10-14 17:04:43,289:INFO:          setuptools: 58.0.4
2022-10-14 17:04:43,289:INFO:             pycaret: 3.0.0rc4
2022-10-14 17:04:43,289:INFO:             IPython: 7.29.0
2022-10-14 17:04:43,290:INFO:          ipywidgets: 7.6.5
2022-10-14 17:04:43,290:INFO:                tqdm: 4.62.3
2022-10-14 17:04:43,290:INFO:               numpy: 1.22.4
2022-10-14 17:04:43,290:INFO:              pandas: 1.4.4
2022-10-14 17:04:43,290:INFO:              jinja2: 3.1.2
2022-10-14 17:04:43,290:INFO:               scipy: 1.8.1
2022-10-14 17:04:43,290:INFO:              joblib: 1.1.0
2022-10-14 17:04:43,290:INFO:             sklearn: 1.0.2
2022-10-14 17:04:43,290:INFO:                pyod: 1.0.5
2022-10-14 17:04:43,290:INFO:            imblearn: 0.9.0
2022-10-14 17:04:43,290:INFO:   category_encoders: 2.5.1.post0
2022-10-14 17:04:43,290:INFO:            lightgbm: 3.3.2
2022-10-14 17:04:43,290:INFO:               numba: 0.55.2
2022-10-14 17:04:43,291:INFO:            requests: 2.28.1
2022-10-14 17:04:43,291:INFO:          matplotlib: 3.4.3
2022-10-14 17:04:43,291:INFO:          scikitplot: 0.3.7
2022-10-14 17:04:43,291:INFO:         yellowbrick: 1.4
2022-10-14 17:04:43,292:INFO:              plotly: 5.5.0
2022-10-14 17:04:43,292:INFO:             kaleido: 0.2.1
2022-10-14 17:04:43,292:INFO:         statsmodels: 0.13.2
2022-10-14 17:04:43,292:INFO:              sktime: 0.13.4
2022-10-14 17:04:43,292:INFO:               tbats: 1.1.1
2022-10-14 17:04:43,294:INFO:            pmdarima: 1.8.5
2022-10-14 17:04:43,294:INFO:              psutil: 5.9.2
2022-10-14 17:04:43,294:INFO:PyCaret optional dependencies:
2022-10-14 17:04:43,296:INFO:                shap: 0.41.0
2022-10-14 17:04:43,297:INFO:           interpret: Not installed
2022-10-14 17:04:43,297:INFO:                umap: Not installed
2022-10-14 17:04:43,297:INFO:    pandas_profiling: Not installed
2022-10-14 17:04:43,297:INFO:  explainerdashboard: Not installed
2022-10-14 17:04:43,297:INFO:             autoviz: Not installed
2022-10-14 17:04:43,297:INFO:           fairlearn: Not installed
2022-10-14 17:04:43,297:INFO:             xgboost: Not installed
2022-10-14 17:04:43,297:INFO:            catboost: 1.1
2022-10-14 17:04:43,297:INFO:              kmodes: Not installed
2022-10-14 17:04:43,297:INFO:             mlxtend: Not installed
2022-10-14 17:04:43,298:INFO:       statsforecast: 1.1.1
2022-10-14 17:04:43,298:INFO:        tune_sklearn: Not installed
2022-10-14 17:04:43,298:INFO:                 ray: Not installed
2022-10-14 17:04:43,298:INFO:            hyperopt: Not installed
2022-10-14 17:04:43,298:INFO:              optuna: Not installed
2022-10-14 17:04:43,298:INFO:               skopt: Not installed
2022-10-14 17:04:43,298:INFO:              mlflow: 1.29.0
2022-10-14 17:04:43,299:INFO:              gradio: Not installed
2022-10-14 17:04:43,300:INFO:             fastapi: Not installed
2022-10-14 17:04:43,300:INFO:             uvicorn: Not installed
2022-10-14 17:04:43,300:INFO:              m2cgen: Not installed
2022-10-14 17:04:43,300:INFO:           evidently: Not installed
2022-10-14 17:04:43,300:INFO:                nltk: 3.6.5
2022-10-14 17:04:43,300:INFO:            pyLDAvis: Not installed
2022-10-14 17:04:43,300:INFO:              gensim: Not installed
2022-10-14 17:04:43,300:INFO:               spacy: Not installed
2022-10-14 17:04:43,300:INFO:           wordcloud: Not installed
2022-10-14 17:04:43,300:INFO:            textblob: Not installed
2022-10-14 17:04:43,301:INFO:               fugue: Not installed
2022-10-14 17:04:43,301:INFO:           streamlit: Not installed
2022-10-14 17:04:43,301:INFO:             prophet: 1.1.1
2022-10-14 17:04:43,301:INFO:None
2022-10-14 17:04:43,301:INFO:Set up data.
2022-10-14 17:04:43,318:INFO:Set up train/test split.
2022-10-14 17:04:43,328:INFO:Set up index.
2022-10-14 17:04:43,328:INFO:Set up folding strategy.
2022-10-14 17:04:43,329:INFO:Assigning column types.
2022-10-14 17:04:43,333:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-14 17:04:43,334:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-14 17:04:43,341:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-14 17:04:43,348:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-14 17:04:43,435:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-14 17:04:43,502:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-14 17:04:43,503:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-14 17:05:12,932:INFO:PyCaret RegressionExperiment
2022-10-14 17:05:12,934:INFO:Logging name: reg-default-name
2022-10-14 17:05:12,934:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-14 17:05:12,934:INFO:version 3.0.0.rc4
2022-10-14 17:05:12,934:INFO:Initializing setup()
2022-10-14 17:05:12,935:INFO:self.USI: 819d
2022-10-14 17:05:12,935:INFO:self.variable_keys: {'idx', 'exp_id', 'exp_name_log', 'data', 'y_test', 'logging_param', 'pipeline', '_all_models', 'target_param', '_all_models_internal', 'y', '_all_metrics', 'fold_shuffle_param', 'html_param', 'variable_keys', 'transform_target_param', 'master_model_container', 'n_jobs_param', '_ml_usecase', 'gpu_param', 'X_test', 'fold_groups_param', 'log_plots_param', 'display_container', 'seed', 'X_train', '_available_plots', '_gpu_n_jobs_param', 'memory', 'y_train', 'fold_generator', 'transform_target_method_param', 'X', 'USI'}
2022-10-14 17:05:12,935:INFO:Checking environment
2022-10-14 17:05:12,935:INFO:python_version: 3.9.7
2022-10-14 17:05:12,935:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-14 17:05:12,935:INFO:machine: x86_64
2022-10-14 17:05:12,935:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:05:12,935:INFO:Memory: svmem(total=8589934592, available=2206355456, percent=74.3, used=4419706880, free=36020224, active=2172076032, inactive=2168901632, wired=2247630848)
2022-10-14 17:05:12,936:INFO:Physical Core: 2
2022-10-14 17:05:12,936:INFO:Logical Core: 4
2022-10-14 17:05:12,936:INFO:Checking libraries
2022-10-14 17:05:12,936:INFO:System:
2022-10-14 17:05:12,936:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-14 17:05:12,936:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-14 17:05:12,936:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:05:12,936:INFO:PyCaret required dependencies:
2022-10-14 17:05:12,936:INFO:                 pip: 21.2.4
2022-10-14 17:05:12,936:INFO:          setuptools: 58.0.4
2022-10-14 17:05:12,936:INFO:             pycaret: 3.0.0rc4
2022-10-14 17:05:12,936:INFO:             IPython: 7.29.0
2022-10-14 17:05:12,936:INFO:          ipywidgets: 7.6.5
2022-10-14 17:05:12,936:INFO:                tqdm: 4.62.3
2022-10-14 17:05:12,936:INFO:               numpy: 1.22.4
2022-10-14 17:05:12,936:INFO:              pandas: 1.4.4
2022-10-14 17:05:12,936:INFO:              jinja2: 3.1.2
2022-10-14 17:05:12,937:INFO:               scipy: 1.8.1
2022-10-14 17:05:12,937:INFO:              joblib: 1.1.0
2022-10-14 17:05:12,937:INFO:             sklearn: 1.0.2
2022-10-14 17:05:12,937:INFO:                pyod: 1.0.5
2022-10-14 17:05:12,937:INFO:            imblearn: 0.9.0
2022-10-14 17:05:12,937:INFO:   category_encoders: 2.5.1.post0
2022-10-14 17:05:12,937:INFO:            lightgbm: 3.3.2
2022-10-14 17:05:12,937:INFO:               numba: 0.55.2
2022-10-14 17:05:12,937:INFO:            requests: 2.28.1
2022-10-14 17:05:12,937:INFO:          matplotlib: 3.4.3
2022-10-14 17:05:12,937:INFO:          scikitplot: 0.3.7
2022-10-14 17:05:12,937:INFO:         yellowbrick: 1.4
2022-10-14 17:05:12,937:INFO:              plotly: 5.5.0
2022-10-14 17:05:12,937:INFO:             kaleido: 0.2.1
2022-10-14 17:05:12,937:INFO:         statsmodels: 0.13.2
2022-10-14 17:05:12,937:INFO:              sktime: 0.13.4
2022-10-14 17:05:12,938:INFO:               tbats: 1.1.1
2022-10-14 17:05:12,939:INFO:            pmdarima: 1.8.5
2022-10-14 17:05:12,943:INFO:              psutil: 5.9.2
2022-10-14 17:05:12,943:INFO:PyCaret optional dependencies:
2022-10-14 17:05:12,944:INFO:                shap: 0.41.0
2022-10-14 17:05:12,944:INFO:           interpret: Not installed
2022-10-14 17:05:12,944:INFO:                umap: Not installed
2022-10-14 17:05:12,944:INFO:    pandas_profiling: Not installed
2022-10-14 17:05:12,944:INFO:  explainerdashboard: Not installed
2022-10-14 17:05:12,944:INFO:             autoviz: Not installed
2022-10-14 17:05:12,944:INFO:           fairlearn: Not installed
2022-10-14 17:05:12,944:INFO:             xgboost: Not installed
2022-10-14 17:05:12,944:INFO:            catboost: 1.1
2022-10-14 17:05:12,944:INFO:              kmodes: Not installed
2022-10-14 17:05:12,945:INFO:             mlxtend: Not installed
2022-10-14 17:05:12,945:INFO:       statsforecast: 1.1.1
2022-10-14 17:05:12,945:INFO:        tune_sklearn: Not installed
2022-10-14 17:05:12,945:INFO:                 ray: Not installed
2022-10-14 17:05:12,945:INFO:            hyperopt: Not installed
2022-10-14 17:05:12,945:INFO:              optuna: Not installed
2022-10-14 17:05:12,945:INFO:               skopt: Not installed
2022-10-14 17:05:12,945:INFO:              mlflow: 1.29.0
2022-10-14 17:05:12,946:INFO:              gradio: Not installed
2022-10-14 17:05:12,946:INFO:             fastapi: Not installed
2022-10-14 17:05:12,946:INFO:             uvicorn: Not installed
2022-10-14 17:05:12,946:INFO:              m2cgen: Not installed
2022-10-14 17:05:12,946:INFO:           evidently: Not installed
2022-10-14 17:05:12,946:INFO:                nltk: 3.6.5
2022-10-14 17:05:12,946:INFO:            pyLDAvis: Not installed
2022-10-14 17:05:12,946:INFO:              gensim: Not installed
2022-10-14 17:05:12,946:INFO:               spacy: Not installed
2022-10-14 17:05:12,946:INFO:           wordcloud: Not installed
2022-10-14 17:05:12,946:INFO:            textblob: Not installed
2022-10-14 17:05:12,946:INFO:               fugue: Not installed
2022-10-14 17:05:12,946:INFO:           streamlit: Not installed
2022-10-14 17:05:12,946:INFO:             prophet: 1.1.1
2022-10-14 17:05:12,946:INFO:None
2022-10-14 17:05:12,947:INFO:Set up data.
2022-10-14 17:05:12,959:INFO:Set up train/test split.
2022-10-14 17:05:12,969:INFO:Set up index.
2022-10-14 17:05:12,969:INFO:Set up folding strategy.
2022-10-14 17:05:12,970:INFO:Assigning column types.
2022-10-14 17:05:12,974:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-14 17:05:12,974:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-14 17:05:12,981:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-14 17:05:12,987:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-14 17:05:13,076:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-14 17:05:13,140:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-14 17:05:13,141:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-14 17:05:31,710:INFO:PyCaret RegressionExperiment
2022-10-14 17:05:31,713:INFO:Logging name: reg-default-name
2022-10-14 17:05:31,713:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-14 17:05:31,714:INFO:version 3.0.0.rc4
2022-10-14 17:05:31,714:INFO:Initializing setup()
2022-10-14 17:05:31,715:INFO:self.USI: ea9d
2022-10-14 17:05:31,716:INFO:self.variable_keys: {'idx', 'exp_id', 'exp_name_log', 'data', 'y_test', 'logging_param', 'pipeline', '_all_models', 'target_param', '_all_models_internal', 'y', '_all_metrics', 'fold_shuffle_param', 'html_param', 'variable_keys', 'transform_target_param', 'master_model_container', 'n_jobs_param', '_ml_usecase', 'gpu_param', 'X_test', 'fold_groups_param', 'log_plots_param', 'display_container', 'seed', 'X_train', '_available_plots', '_gpu_n_jobs_param', 'memory', 'y_train', 'fold_generator', 'transform_target_method_param', 'X', 'USI'}
2022-10-14 17:05:31,716:INFO:Checking environment
2022-10-14 17:05:31,716:INFO:python_version: 3.9.7
2022-10-14 17:05:31,717:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-14 17:05:31,717:INFO:machine: x86_64
2022-10-14 17:05:31,717:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:05:31,717:INFO:Memory: svmem(total=8589934592, available=2184794112, percent=74.6, used=4417961984, free=15065088, active=2171621376, inactive=2168856576, wired=2246340608)
2022-10-14 17:05:31,717:INFO:Physical Core: 2
2022-10-14 17:05:31,718:INFO:Logical Core: 4
2022-10-14 17:05:31,718:INFO:Checking libraries
2022-10-14 17:05:31,718:INFO:System:
2022-10-14 17:05:31,718:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-14 17:05:31,718:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-14 17:05:31,718:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-14 17:05:31,718:INFO:PyCaret required dependencies:
2022-10-14 17:05:31,719:INFO:                 pip: 21.2.4
2022-10-14 17:05:31,719:INFO:          setuptools: 58.0.4
2022-10-14 17:05:31,719:INFO:             pycaret: 3.0.0rc4
2022-10-14 17:05:31,719:INFO:             IPython: 7.29.0
2022-10-14 17:05:31,719:INFO:          ipywidgets: 7.6.5
2022-10-14 17:05:31,722:INFO:                tqdm: 4.62.3
2022-10-14 17:05:31,722:INFO:               numpy: 1.22.4
2022-10-14 17:05:31,722:INFO:              pandas: 1.4.4
2022-10-14 17:05:31,722:INFO:              jinja2: 3.1.2
2022-10-14 17:05:31,723:INFO:               scipy: 1.8.1
2022-10-14 17:05:31,723:INFO:              joblib: 1.1.0
2022-10-14 17:05:31,723:INFO:             sklearn: 1.0.2
2022-10-14 17:05:31,723:INFO:                pyod: 1.0.5
2022-10-14 17:05:31,723:INFO:            imblearn: 0.9.0
2022-10-14 17:05:31,723:INFO:   category_encoders: 2.5.1.post0
2022-10-14 17:05:31,723:INFO:            lightgbm: 3.3.2
2022-10-14 17:05:31,723:INFO:               numba: 0.55.2
2022-10-14 17:05:31,723:INFO:            requests: 2.28.1
2022-10-14 17:05:31,723:INFO:          matplotlib: 3.4.3
2022-10-14 17:05:31,723:INFO:          scikitplot: 0.3.7
2022-10-14 17:05:31,723:INFO:         yellowbrick: 1.4
2022-10-14 17:05:31,724:INFO:              plotly: 5.5.0
2022-10-14 17:05:31,724:INFO:             kaleido: 0.2.1
2022-10-14 17:05:31,724:INFO:         statsmodels: 0.13.2
2022-10-14 17:05:31,724:INFO:              sktime: 0.13.4
2022-10-14 17:05:31,724:INFO:               tbats: 1.1.1
2022-10-14 17:05:31,725:INFO:            pmdarima: 1.8.5
2022-10-14 17:05:31,725:INFO:              psutil: 5.9.2
2022-10-14 17:05:31,725:INFO:PyCaret optional dependencies:
2022-10-14 17:05:31,725:INFO:                shap: 0.41.0
2022-10-14 17:05:31,725:INFO:           interpret: Not installed
2022-10-14 17:05:31,725:INFO:                umap: Not installed
2022-10-14 17:05:31,725:INFO:    pandas_profiling: Not installed
2022-10-14 17:05:31,725:INFO:  explainerdashboard: Not installed
2022-10-14 17:05:31,726:INFO:             autoviz: Not installed
2022-10-14 17:05:31,726:INFO:           fairlearn: Not installed
2022-10-14 17:05:31,726:INFO:             xgboost: Not installed
2022-10-14 17:05:31,726:INFO:            catboost: 1.1
2022-10-14 17:05:31,726:INFO:              kmodes: Not installed
2022-10-14 17:05:31,726:INFO:             mlxtend: Not installed
2022-10-14 17:05:31,726:INFO:       statsforecast: 1.1.1
2022-10-14 17:05:31,726:INFO:        tune_sklearn: Not installed
2022-10-14 17:05:31,726:INFO:                 ray: Not installed
2022-10-14 17:05:31,726:INFO:            hyperopt: Not installed
2022-10-14 17:05:31,726:INFO:              optuna: Not installed
2022-10-14 17:05:31,727:INFO:               skopt: Not installed
2022-10-14 17:05:31,727:INFO:              mlflow: 1.29.0
2022-10-14 17:05:31,727:INFO:              gradio: Not installed
2022-10-14 17:05:31,727:INFO:             fastapi: Not installed
2022-10-14 17:05:31,727:INFO:             uvicorn: Not installed
2022-10-14 17:05:31,727:INFO:              m2cgen: Not installed
2022-10-14 17:05:31,727:INFO:           evidently: Not installed
2022-10-14 17:05:31,727:INFO:                nltk: 3.6.5
2022-10-14 17:05:31,727:INFO:            pyLDAvis: Not installed
2022-10-14 17:05:31,727:INFO:              gensim: Not installed
2022-10-14 17:05:31,727:INFO:               spacy: Not installed
2022-10-14 17:05:31,727:INFO:           wordcloud: Not installed
2022-10-14 17:05:31,727:INFO:            textblob: Not installed
2022-10-14 17:05:31,727:INFO:               fugue: Not installed
2022-10-14 17:05:31,728:INFO:           streamlit: Not installed
2022-10-14 17:05:31,728:INFO:             prophet: 1.1.1
2022-10-14 17:05:31,728:INFO:None
2022-10-14 17:05:31,728:INFO:Set up data.
2022-10-14 17:05:31,741:INFO:Set up train/test split.
2022-10-14 17:05:31,747:INFO:Set up index.
2022-10-14 17:05:31,748:INFO:Set up folding strategy.
2022-10-14 17:05:31,748:INFO:Assigning column types.
2022-10-14 17:05:31,754:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-14 17:05:31,755:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-14 17:05:31,761:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-14 17:05:31,769:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-14 17:05:31,862:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-14 17:05:31,937:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-14 17:05:31,938:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-20 13:46:28,008:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:46:28,010:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:46:28,010:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:46:28,011:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:46:31,582:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 13:46:33,171:INFO:Initializing load_model()
2022-10-20 13:46:33,172:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-20 13:58:15,404:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:58:15,405:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:58:15,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:58:15,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 13:58:18,511:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 13:58:20,026:INFO:Initializing load_model()
2022-10-20 13:58:20,026:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-20 14:00:43,326:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:00:43,327:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:00:43,327:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:00:43,327:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:00:46,465:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 14:00:58,413:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:00:58,414:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:00:58,415:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:00:58,415:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:01:00,948:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 14:05:10,822:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:05:10,823:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:05:10,824:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:05:10,824:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:05:13,131:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 14:06:12,507:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:12,507:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:12,508:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:12,508:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:14,695:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 14:06:54,956:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:54,957:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:54,957:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:54,957:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:06:57,194:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 14:09:33,619:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:09:33,620:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:09:33,620:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:09:33,620:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:09:36,175:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 14:12:38,703:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:12:38,703:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:12:38,704:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:12:38,704:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:12:40,894:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-20 14:24:05,497:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:24:05,498:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:24:05,498:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:24:05,498:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-20 14:24:07,990:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 11:36:13,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:36:13,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:36:13,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:36:13,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:36:16,859:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 11:38:23,184:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:38:23,185:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:38:23,185:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:38:23,185:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:38:25,468:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 11:39:07,086:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:39:07,087:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:39:07,087:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:39:07,087:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:39:09,295:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 11:40:33,255:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:40:33,255:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:40:33,256:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:40:33,256:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:40:35,449:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 11:41:04,798:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:04,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:04,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:04,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:07,119:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 11:41:48,441:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:48,441:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:48,441:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:48,441:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:41:50,757:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 11:44:37,700:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:44:37,700:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:44:37,700:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:44:37,700:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 11:44:40,142:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 12:27:28,204:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:27:28,205:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:27:28,205:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:27:28,205:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:27:31,061:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 12:29:26,561:INFO:PyCaret RegressionExperiment
2022-10-21 12:29:26,563:INFO:Logging name: reg-default-name
2022-10-21 12:29:26,563:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:29:26,563:INFO:version 3.0.0.rc4
2022-10-21 12:29:26,564:INFO:Initializing setup()
2022-10-21 12:29:26,564:INFO:self.USI: 76f1
2022-10-21 12:29:26,564:INFO:self.variable_keys: {'pipeline', 'USI', 'X', 'exp_name_log', 'memory', '_gpu_n_jobs_param', 'idx', 'X_test', '_all_models_internal', '_all_metrics', 'logging_param', 'gpu_param', 'master_model_container', 'transform_target_method_param', 'html_param', 'seed', 'log_plots_param', 'y', '_all_models', 'X_train', 'fold_generator', 'fold_shuffle_param', 'target_param', 'transform_target_param', '_ml_usecase', 'data', 'display_container', 'n_jobs_param', 'y_test', 'fold_groups_param', 'variable_keys', 'y_train', '_available_plots', 'exp_id'}
2022-10-21 12:29:26,564:INFO:Checking environment
2022-10-21 12:29:26,564:INFO:python_version: 3.9.7
2022-10-21 12:29:26,565:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:29:26,565:INFO:machine: x86_64
2022-10-21 12:29:26,565:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:29:26,565:INFO:Memory: svmem(total=8589934592, available=2134220800, percent=75.2, used=3981713408, free=15867904, active=2123329536, inactive=2117636096, wired=1858383872)
2022-10-21 12:29:26,565:INFO:Physical Core: 2
2022-10-21 12:29:26,565:INFO:Logical Core: 4
2022-10-21 12:29:26,565:INFO:Checking libraries
2022-10-21 12:29:26,565:INFO:System:
2022-10-21 12:29:26,565:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:29:26,565:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:29:26,565:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:29:26,565:INFO:PyCaret required dependencies:
2022-10-21 12:29:26,565:INFO:                 pip: 21.2.4
2022-10-21 12:29:26,565:INFO:          setuptools: 58.0.4
2022-10-21 12:29:26,565:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:29:26,566:INFO:             IPython: 7.29.0
2022-10-21 12:29:26,566:INFO:          ipywidgets: 7.6.5
2022-10-21 12:29:26,566:INFO:                tqdm: 4.62.3
2022-10-21 12:29:26,566:INFO:               numpy: 1.22.4
2022-10-21 12:29:26,566:INFO:              pandas: 1.4.4
2022-10-21 12:29:26,566:INFO:              jinja2: 3.1.2
2022-10-21 12:29:26,566:INFO:               scipy: 1.8.1
2022-10-21 12:29:26,566:INFO:              joblib: 1.1.0
2022-10-21 12:29:26,566:INFO:             sklearn: 1.0.2
2022-10-21 12:29:26,566:INFO:                pyod: 1.0.5
2022-10-21 12:29:26,566:INFO:            imblearn: 0.9.0
2022-10-21 12:29:26,566:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:29:26,566:INFO:            lightgbm: 3.3.2
2022-10-21 12:29:26,566:INFO:               numba: 0.55.2
2022-10-21 12:29:26,566:INFO:            requests: 2.28.1
2022-10-21 12:29:26,566:INFO:          matplotlib: 3.4.3
2022-10-21 12:29:26,566:INFO:          scikitplot: 0.3.7
2022-10-21 12:29:26,566:INFO:         yellowbrick: 1.4
2022-10-21 12:29:26,566:INFO:              plotly: 5.5.0
2022-10-21 12:29:26,567:INFO:             kaleido: 0.2.1
2022-10-21 12:29:26,567:INFO:         statsmodels: 0.13.2
2022-10-21 12:29:26,567:INFO:              sktime: 0.13.4
2022-10-21 12:29:26,567:INFO:               tbats: 1.1.1
2022-10-21 12:29:26,568:INFO:            pmdarima: 1.8.5
2022-10-21 12:29:26,568:INFO:              psutil: 5.9.2
2022-10-21 12:29:26,568:INFO:PyCaret optional dependencies:
2022-10-21 12:29:26,577:INFO:                shap: 0.41.0
2022-10-21 12:29:26,578:INFO:           interpret: Not installed
2022-10-21 12:29:26,578:INFO:                umap: Not installed
2022-10-21 12:29:26,578:INFO:    pandas_profiling: Not installed
2022-10-21 12:29:26,578:INFO:  explainerdashboard: Not installed
2022-10-21 12:29:26,578:INFO:             autoviz: Not installed
2022-10-21 12:29:26,578:INFO:           fairlearn: Not installed
2022-10-21 12:29:26,578:INFO:             xgboost: Not installed
2022-10-21 12:29:26,579:INFO:            catboost: 1.1
2022-10-21 12:29:26,579:INFO:              kmodes: Not installed
2022-10-21 12:29:26,579:INFO:             mlxtend: Not installed
2022-10-21 12:29:26,579:INFO:       statsforecast: 1.1.1
2022-10-21 12:29:26,579:INFO:        tune_sklearn: Not installed
2022-10-21 12:29:26,579:INFO:                 ray: Not installed
2022-10-21 12:29:26,579:INFO:            hyperopt: Not installed
2022-10-21 12:29:26,579:INFO:              optuna: Not installed
2022-10-21 12:29:26,579:INFO:               skopt: Not installed
2022-10-21 12:29:26,582:INFO:              mlflow: 1.29.0
2022-10-21 12:29:26,582:INFO:              gradio: Not installed
2022-10-21 12:29:26,582:INFO:             fastapi: Not installed
2022-10-21 12:29:26,582:INFO:             uvicorn: Not installed
2022-10-21 12:29:26,582:INFO:              m2cgen: Not installed
2022-10-21 12:29:26,582:INFO:           evidently: Not installed
2022-10-21 12:29:26,582:INFO:                nltk: 3.6.5
2022-10-21 12:29:26,582:INFO:            pyLDAvis: Not installed
2022-10-21 12:29:26,582:INFO:              gensim: Not installed
2022-10-21 12:29:26,582:INFO:               spacy: Not installed
2022-10-21 12:29:26,582:INFO:           wordcloud: Not installed
2022-10-21 12:29:26,582:INFO:            textblob: Not installed
2022-10-21 12:29:26,582:INFO:               fugue: Not installed
2022-10-21 12:29:26,582:INFO:           streamlit: Not installed
2022-10-21 12:29:26,582:INFO:             prophet: 1.1.1
2022-10-21 12:29:26,582:INFO:None
2022-10-21 12:29:26,582:INFO:Set up data.
2022-10-21 12:29:26,606:INFO:Set up train/test split.
2022-10-21 12:29:26,627:INFO:Set up index.
2022-10-21 12:29:26,629:INFO:Set up folding strategy.
2022-10-21 12:29:26,629:INFO:Assigning column types.
2022-10-21 12:29:26,709:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:29:26,710:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:29:26,733:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:29:26,765:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:29:27,374:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:29:27,756:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:29:27,758:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 12:30:52,272:INFO:PyCaret RegressionExperiment
2022-10-21 12:30:52,273:INFO:Logging name: reg-default-name
2022-10-21 12:30:52,273:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:30:52,273:INFO:version 3.0.0.rc4
2022-10-21 12:30:52,274:INFO:Initializing setup()
2022-10-21 12:30:52,274:INFO:self.USI: b74d
2022-10-21 12:30:52,274:INFO:self.variable_keys: {'pipeline', 'USI', 'X', 'exp_name_log', 'memory', '_gpu_n_jobs_param', 'idx', 'X_test', '_all_models_internal', '_all_metrics', 'logging_param', 'gpu_param', 'master_model_container', 'transform_target_method_param', 'html_param', 'seed', 'log_plots_param', 'y', '_all_models', 'X_train', 'fold_generator', 'fold_shuffle_param', 'target_param', 'transform_target_param', '_ml_usecase', 'data', 'display_container', 'n_jobs_param', 'y_test', 'fold_groups_param', 'variable_keys', 'y_train', '_available_plots', 'exp_id'}
2022-10-21 12:30:52,274:INFO:Checking environment
2022-10-21 12:30:52,274:INFO:python_version: 3.9.7
2022-10-21 12:30:52,275:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:30:52,275:INFO:machine: x86_64
2022-10-21 12:30:52,275:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:30:52,275:INFO:Memory: svmem(total=8589934592, available=2372382720, percent=72.4, used=4132909056, free=168919040, active=2205200384, inactive=2196938752, wired=1927708672)
2022-10-21 12:30:52,275:INFO:Physical Core: 2
2022-10-21 12:30:52,275:INFO:Logical Core: 4
2022-10-21 12:30:52,275:INFO:Checking libraries
2022-10-21 12:30:52,275:INFO:System:
2022-10-21 12:30:52,275:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:30:52,276:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:30:52,276:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:30:52,276:INFO:PyCaret required dependencies:
2022-10-21 12:30:52,276:INFO:                 pip: 21.2.4
2022-10-21 12:30:52,276:INFO:          setuptools: 58.0.4
2022-10-21 12:30:52,276:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:30:52,276:INFO:             IPython: 7.29.0
2022-10-21 12:30:52,276:INFO:          ipywidgets: 7.6.5
2022-10-21 12:30:52,276:INFO:                tqdm: 4.62.3
2022-10-21 12:30:52,276:INFO:               numpy: 1.22.4
2022-10-21 12:30:52,276:INFO:              pandas: 1.4.4
2022-10-21 12:30:52,276:INFO:              jinja2: 3.1.2
2022-10-21 12:30:52,277:INFO:               scipy: 1.8.1
2022-10-21 12:30:52,277:INFO:              joblib: 1.1.0
2022-10-21 12:30:52,277:INFO:             sklearn: 1.0.2
2022-10-21 12:30:52,277:INFO:                pyod: 1.0.5
2022-10-21 12:30:52,277:INFO:            imblearn: 0.9.0
2022-10-21 12:30:52,277:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:30:52,277:INFO:            lightgbm: 3.3.2
2022-10-21 12:30:52,277:INFO:               numba: 0.55.2
2022-10-21 12:30:52,277:INFO:            requests: 2.28.1
2022-10-21 12:30:52,278:INFO:          matplotlib: 3.4.3
2022-10-21 12:30:52,278:INFO:          scikitplot: 0.3.7
2022-10-21 12:30:52,278:INFO:         yellowbrick: 1.4
2022-10-21 12:30:52,278:INFO:              plotly: 5.5.0
2022-10-21 12:30:52,278:INFO:             kaleido: 0.2.1
2022-10-21 12:30:52,279:INFO:         statsmodels: 0.13.2
2022-10-21 12:30:52,279:INFO:              sktime: 0.13.4
2022-10-21 12:30:52,279:INFO:               tbats: 1.1.1
2022-10-21 12:30:52,279:INFO:            pmdarima: 1.8.5
2022-10-21 12:30:52,279:INFO:              psutil: 5.9.2
2022-10-21 12:30:52,279:INFO:PyCaret optional dependencies:
2022-10-21 12:30:52,280:INFO:                shap: 0.41.0
2022-10-21 12:30:52,280:INFO:           interpret: Not installed
2022-10-21 12:30:52,280:INFO:                umap: Not installed
2022-10-21 12:30:52,280:INFO:    pandas_profiling: Not installed
2022-10-21 12:30:52,280:INFO:  explainerdashboard: Not installed
2022-10-21 12:30:52,280:INFO:             autoviz: Not installed
2022-10-21 12:30:52,280:INFO:           fairlearn: Not installed
2022-10-21 12:30:52,280:INFO:             xgboost: Not installed
2022-10-21 12:30:52,280:INFO:            catboost: 1.1
2022-10-21 12:30:52,280:INFO:              kmodes: Not installed
2022-10-21 12:30:52,280:INFO:             mlxtend: Not installed
2022-10-21 12:30:52,280:INFO:       statsforecast: 1.1.1
2022-10-21 12:30:52,280:INFO:        tune_sklearn: Not installed
2022-10-21 12:30:52,280:INFO:                 ray: Not installed
2022-10-21 12:30:52,280:INFO:            hyperopt: Not installed
2022-10-21 12:30:52,280:INFO:              optuna: Not installed
2022-10-21 12:30:52,281:INFO:               skopt: Not installed
2022-10-21 12:30:52,281:INFO:              mlflow: 1.29.0
2022-10-21 12:30:52,281:INFO:              gradio: Not installed
2022-10-21 12:30:52,281:INFO:             fastapi: Not installed
2022-10-21 12:30:52,281:INFO:             uvicorn: Not installed
2022-10-21 12:30:52,281:INFO:              m2cgen: Not installed
2022-10-21 12:30:52,281:INFO:           evidently: Not installed
2022-10-21 12:30:52,286:INFO:                nltk: 3.6.5
2022-10-21 12:30:52,286:INFO:            pyLDAvis: Not installed
2022-10-21 12:30:52,286:INFO:              gensim: Not installed
2022-10-21 12:30:52,287:INFO:               spacy: Not installed
2022-10-21 12:30:52,287:INFO:           wordcloud: Not installed
2022-10-21 12:30:52,287:INFO:            textblob: Not installed
2022-10-21 12:30:52,287:INFO:               fugue: Not installed
2022-10-21 12:30:52,287:INFO:           streamlit: Not installed
2022-10-21 12:30:52,287:INFO:             prophet: 1.1.1
2022-10-21 12:30:52,287:INFO:None
2022-10-21 12:30:52,287:INFO:Set up data.
2022-10-21 12:30:52,304:INFO:Set up train/test split.
2022-10-21 12:30:52,325:INFO:Set up index.
2022-10-21 12:30:52,325:INFO:Set up folding strategy.
2022-10-21 12:30:52,325:INFO:Assigning column types.
2022-10-21 12:30:52,339:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:30:52,340:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:30:52,362:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:30:52,430:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:30:52,625:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:30:53,089:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:30:53,094:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 12:31:17,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:31:17,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:31:17,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:31:17,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:31:20,311:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 12:31:21,281:INFO:PyCaret RegressionExperiment
2022-10-21 12:31:21,282:INFO:Logging name: reg-default-name
2022-10-21 12:31:21,282:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:31:21,282:INFO:version 3.0.0.rc4
2022-10-21 12:31:21,282:INFO:Initializing setup()
2022-10-21 12:31:21,282:INFO:self.USI: 2308
2022-10-21 12:31:21,282:INFO:self.variable_keys: {'variable_keys', '_available_plots', 'n_jobs_param', 'y_test', 'fold_generator', 'transform_target_method_param', 'log_plots_param', 'idx', '_gpu_n_jobs_param', 'pipeline', 'y_train', 'transform_target_param', 'USI', 'fold_shuffle_param', 'X_train', '_all_models', 'html_param', 'logging_param', 'memory', 'y', 'X_test', 'seed', '_all_models_internal', 'X', 'exp_name_log', 'master_model_container', 'target_param', 'exp_id', 'fold_groups_param', 'gpu_param', 'data', '_all_metrics', 'display_container', '_ml_usecase'}
2022-10-21 12:31:21,282:INFO:Checking environment
2022-10-21 12:31:21,282:INFO:python_version: 3.9.7
2022-10-21 12:31:21,282:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:31:21,283:INFO:machine: x86_64
2022-10-21 12:31:21,283:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:31:21,283:INFO:Memory: svmem(total=8589934592, available=2310197248, percent=73.1, used=4234559488, free=20979712, active=2290040832, inactive=2238517248, wired=1944518656)
2022-10-21 12:31:21,283:INFO:Physical Core: 2
2022-10-21 12:31:21,283:INFO:Logical Core: 4
2022-10-21 12:31:21,283:INFO:Checking libraries
2022-10-21 12:31:21,283:INFO:System:
2022-10-21 12:31:21,283:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:31:21,283:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:31:21,283:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:31:21,283:INFO:PyCaret required dependencies:
2022-10-21 12:31:21,283:INFO:                 pip: 21.2.4
2022-10-21 12:31:21,284:INFO:          setuptools: 58.0.4
2022-10-21 12:31:21,284:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:31:21,284:INFO:             IPython: 7.29.0
2022-10-21 12:31:21,284:INFO:          ipywidgets: 7.6.5
2022-10-21 12:31:21,284:INFO:                tqdm: 4.62.3
2022-10-21 12:31:21,284:INFO:               numpy: 1.22.4
2022-10-21 12:31:21,284:INFO:              pandas: 1.4.4
2022-10-21 12:31:21,284:INFO:              jinja2: 3.1.2
2022-10-21 12:31:21,284:INFO:               scipy: 1.8.1
2022-10-21 12:31:21,284:INFO:              joblib: 1.1.0
2022-10-21 12:31:21,284:INFO:             sklearn: 1.0.2
2022-10-21 12:31:21,285:INFO:                pyod: 1.0.5
2022-10-21 12:31:21,285:INFO:            imblearn: 0.9.0
2022-10-21 12:31:21,285:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:31:21,285:INFO:            lightgbm: 3.3.2
2022-10-21 12:31:21,285:INFO:               numba: 0.55.2
2022-10-21 12:31:21,286:INFO:            requests: 2.28.1
2022-10-21 12:31:21,286:INFO:          matplotlib: 3.4.3
2022-10-21 12:31:21,286:INFO:          scikitplot: 0.3.7
2022-10-21 12:31:21,286:INFO:         yellowbrick: 1.4
2022-10-21 12:31:21,286:INFO:              plotly: 5.5.0
2022-10-21 12:31:21,286:INFO:             kaleido: 0.2.1
2022-10-21 12:31:21,286:INFO:         statsmodels: 0.13.2
2022-10-21 12:31:21,286:INFO:              sktime: 0.13.4
2022-10-21 12:31:21,286:INFO:               tbats: 1.1.1
2022-10-21 12:31:21,286:INFO:            pmdarima: 1.8.5
2022-10-21 12:31:21,286:INFO:              psutil: 5.9.2
2022-10-21 12:31:21,286:INFO:PyCaret optional dependencies:
2022-10-21 12:31:21,296:INFO:                shap: 0.41.0
2022-10-21 12:31:21,296:INFO:           interpret: Not installed
2022-10-21 12:31:21,296:INFO:                umap: Not installed
2022-10-21 12:31:21,296:INFO:    pandas_profiling: Not installed
2022-10-21 12:31:21,296:INFO:  explainerdashboard: Not installed
2022-10-21 12:31:21,296:INFO:             autoviz: Not installed
2022-10-21 12:31:21,296:INFO:           fairlearn: Not installed
2022-10-21 12:31:21,296:INFO:             xgboost: Not installed
2022-10-21 12:31:21,296:INFO:            catboost: 1.1
2022-10-21 12:31:21,296:INFO:              kmodes: Not installed
2022-10-21 12:31:21,296:INFO:             mlxtend: Not installed
2022-10-21 12:31:21,296:INFO:       statsforecast: 1.1.1
2022-10-21 12:31:21,296:INFO:        tune_sklearn: Not installed
2022-10-21 12:31:21,296:INFO:                 ray: Not installed
2022-10-21 12:31:21,297:INFO:            hyperopt: Not installed
2022-10-21 12:31:21,297:INFO:              optuna: Not installed
2022-10-21 12:31:21,297:INFO:               skopt: Not installed
2022-10-21 12:31:21,297:INFO:              mlflow: 1.29.0
2022-10-21 12:31:21,297:INFO:              gradio: Not installed
2022-10-21 12:31:21,297:INFO:             fastapi: Not installed
2022-10-21 12:31:21,297:INFO:             uvicorn: Not installed
2022-10-21 12:31:21,297:INFO:              m2cgen: Not installed
2022-10-21 12:31:21,297:INFO:           evidently: Not installed
2022-10-21 12:31:21,297:INFO:                nltk: 3.6.5
2022-10-21 12:31:21,297:INFO:            pyLDAvis: Not installed
2022-10-21 12:31:21,297:INFO:              gensim: Not installed
2022-10-21 12:31:21,297:INFO:               spacy: Not installed
2022-10-21 12:31:21,297:INFO:           wordcloud: Not installed
2022-10-21 12:31:21,297:INFO:            textblob: Not installed
2022-10-21 12:31:21,297:INFO:               fugue: Not installed
2022-10-21 12:31:21,297:INFO:           streamlit: Not installed
2022-10-21 12:31:21,297:INFO:             prophet: 1.1.1
2022-10-21 12:31:21,297:INFO:None
2022-10-21 12:31:21,297:INFO:Set up data.
2022-10-21 12:31:21,309:INFO:Set up train/test split.
2022-10-21 12:31:21,316:INFO:Set up index.
2022-10-21 12:31:21,316:INFO:Set up folding strategy.
2022-10-21 12:31:21,316:INFO:Assigning column types.
2022-10-21 12:31:21,322:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:31:21,323:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:31:21,330:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:31:21,338:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:31:21,432:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:31:21,506:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:31:21,507:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 12:31:31,500:INFO:PyCaret RegressionExperiment
2022-10-21 12:31:31,500:INFO:Logging name: reg-default-name
2022-10-21 12:31:31,500:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:31:31,500:INFO:version 3.0.0.rc4
2022-10-21 12:31:31,500:INFO:Initializing setup()
2022-10-21 12:31:31,500:INFO:self.USI: 09e8
2022-10-21 12:31:31,500:INFO:self.variable_keys: {'variable_keys', '_available_plots', 'n_jobs_param', 'y_test', 'fold_generator', 'transform_target_method_param', 'log_plots_param', 'idx', '_gpu_n_jobs_param', 'pipeline', 'y_train', 'transform_target_param', 'USI', 'fold_shuffle_param', 'X_train', '_all_models', 'html_param', 'logging_param', 'memory', 'y', 'X_test', 'seed', '_all_models_internal', 'X', 'exp_name_log', 'master_model_container', 'target_param', 'exp_id', 'fold_groups_param', 'gpu_param', 'data', '_all_metrics', 'display_container', '_ml_usecase'}
2022-10-21 12:31:31,500:INFO:Checking environment
2022-10-21 12:31:31,501:INFO:python_version: 3.9.7
2022-10-21 12:31:31,501:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:31:31,505:INFO:machine: x86_64
2022-10-21 12:31:31,506:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:31:31,506:INFO:Memory: svmem(total=8589934592, available=2313994240, percent=73.1, used=4229959680, free=20656128, active=2293370880, inactive=2280755200, wired=1936588800)
2022-10-21 12:31:31,506:INFO:Physical Core: 2
2022-10-21 12:31:31,506:INFO:Logical Core: 4
2022-10-21 12:31:31,506:INFO:Checking libraries
2022-10-21 12:31:31,506:INFO:System:
2022-10-21 12:31:31,506:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:31:31,506:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:31:31,506:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:31:31,506:INFO:PyCaret required dependencies:
2022-10-21 12:31:31,506:INFO:                 pip: 21.2.4
2022-10-21 12:31:31,506:INFO:          setuptools: 58.0.4
2022-10-21 12:31:31,507:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:31:31,507:INFO:             IPython: 7.29.0
2022-10-21 12:31:31,507:INFO:          ipywidgets: 7.6.5
2022-10-21 12:31:31,507:INFO:                tqdm: 4.62.3
2022-10-21 12:31:31,507:INFO:               numpy: 1.22.4
2022-10-21 12:31:31,507:INFO:              pandas: 1.4.4
2022-10-21 12:31:31,507:INFO:              jinja2: 3.1.2
2022-10-21 12:31:31,507:INFO:               scipy: 1.8.1
2022-10-21 12:31:31,507:INFO:              joblib: 1.1.0
2022-10-21 12:31:31,507:INFO:             sklearn: 1.0.2
2022-10-21 12:31:31,507:INFO:                pyod: 1.0.5
2022-10-21 12:31:31,507:INFO:            imblearn: 0.9.0
2022-10-21 12:31:31,507:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:31:31,507:INFO:            lightgbm: 3.3.2
2022-10-21 12:31:31,507:INFO:               numba: 0.55.2
2022-10-21 12:31:31,507:INFO:            requests: 2.28.1
2022-10-21 12:31:31,507:INFO:          matplotlib: 3.4.3
2022-10-21 12:31:31,507:INFO:          scikitplot: 0.3.7
2022-10-21 12:31:31,507:INFO:         yellowbrick: 1.4
2022-10-21 12:31:31,508:INFO:              plotly: 5.5.0
2022-10-21 12:31:31,508:INFO:             kaleido: 0.2.1
2022-10-21 12:31:31,508:INFO:         statsmodels: 0.13.2
2022-10-21 12:31:31,508:INFO:              sktime: 0.13.4
2022-10-21 12:31:31,508:INFO:               tbats: 1.1.1
2022-10-21 12:31:31,508:INFO:            pmdarima: 1.8.5
2022-10-21 12:31:31,508:INFO:              psutil: 5.9.2
2022-10-21 12:31:31,508:INFO:PyCaret optional dependencies:
2022-10-21 12:31:31,508:INFO:                shap: 0.41.0
2022-10-21 12:31:31,508:INFO:           interpret: Not installed
2022-10-21 12:31:31,508:INFO:                umap: Not installed
2022-10-21 12:31:31,508:INFO:    pandas_profiling: Not installed
2022-10-21 12:31:31,508:INFO:  explainerdashboard: Not installed
2022-10-21 12:31:31,508:INFO:             autoviz: Not installed
2022-10-21 12:31:31,509:INFO:           fairlearn: Not installed
2022-10-21 12:31:31,509:INFO:             xgboost: Not installed
2022-10-21 12:31:31,509:INFO:            catboost: 1.1
2022-10-21 12:31:31,509:INFO:              kmodes: Not installed
2022-10-21 12:31:31,509:INFO:             mlxtend: Not installed
2022-10-21 12:31:31,509:INFO:       statsforecast: 1.1.1
2022-10-21 12:31:31,510:INFO:        tune_sklearn: Not installed
2022-10-21 12:31:31,510:INFO:                 ray: Not installed
2022-10-21 12:31:31,510:INFO:            hyperopt: Not installed
2022-10-21 12:31:31,510:INFO:              optuna: Not installed
2022-10-21 12:31:31,510:INFO:               skopt: Not installed
2022-10-21 12:31:31,510:INFO:              mlflow: 1.29.0
2022-10-21 12:31:31,513:INFO:              gradio: Not installed
2022-10-21 12:31:31,513:INFO:             fastapi: Not installed
2022-10-21 12:31:31,513:INFO:             uvicorn: Not installed
2022-10-21 12:31:31,514:INFO:              m2cgen: Not installed
2022-10-21 12:31:31,514:INFO:           evidently: Not installed
2022-10-21 12:31:31,514:INFO:                nltk: 3.6.5
2022-10-21 12:31:31,514:INFO:            pyLDAvis: Not installed
2022-10-21 12:31:31,514:INFO:              gensim: Not installed
2022-10-21 12:31:31,514:INFO:               spacy: Not installed
2022-10-21 12:31:31,514:INFO:           wordcloud: Not installed
2022-10-21 12:31:31,514:INFO:            textblob: Not installed
2022-10-21 12:31:31,514:INFO:               fugue: Not installed
2022-10-21 12:31:31,514:INFO:           streamlit: Not installed
2022-10-21 12:31:31,514:INFO:             prophet: 1.1.1
2022-10-21 12:31:31,514:INFO:None
2022-10-21 12:31:31,514:INFO:Set up data.
2022-10-21 12:31:31,529:INFO:Set up train/test split.
2022-10-21 12:31:31,548:INFO:Set up index.
2022-10-21 12:31:31,553:INFO:Set up folding strategy.
2022-10-21 12:31:31,553:INFO:Assigning column types.
2022-10-21 12:31:31,564:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:31:31,564:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:31:31,601:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:31:31,613:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:31:31,768:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:31:32,270:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:31:32,272:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 12:33:01,878:INFO:PyCaret RegressionExperiment
2022-10-21 12:33:01,879:INFO:Logging name: reg-default-name
2022-10-21 12:33:01,879:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:33:01,879:INFO:version 3.0.0.rc4
2022-10-21 12:33:01,879:INFO:Initializing setup()
2022-10-21 12:33:01,879:INFO:self.USI: 1055
2022-10-21 12:33:01,879:INFO:self.variable_keys: {'variable_keys', '_available_plots', 'n_jobs_param', 'y_test', 'fold_generator', 'transform_target_method_param', 'log_plots_param', 'idx', '_gpu_n_jobs_param', 'pipeline', 'y_train', 'transform_target_param', 'USI', 'fold_shuffle_param', 'X_train', '_all_models', 'html_param', 'logging_param', 'memory', 'y', 'X_test', 'seed', '_all_models_internal', 'X', 'exp_name_log', 'master_model_container', 'target_param', 'exp_id', 'fold_groups_param', 'gpu_param', 'data', '_all_metrics', 'display_container', '_ml_usecase'}
2022-10-21 12:33:01,880:INFO:Checking environment
2022-10-21 12:33:01,880:INFO:python_version: 3.9.7
2022-10-21 12:33:01,880:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:33:01,880:INFO:machine: x86_64
2022-10-21 12:33:01,880:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:33:01,880:INFO:Memory: svmem(total=8589934592, available=2362884096, percent=72.5, used=4270772224, free=29306880, active=2335350784, inactive=2329452544, wired=1935421440)
2022-10-21 12:33:01,880:INFO:Physical Core: 2
2022-10-21 12:33:01,880:INFO:Logical Core: 4
2022-10-21 12:33:01,880:INFO:Checking libraries
2022-10-21 12:33:01,880:INFO:System:
2022-10-21 12:33:01,880:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:33:01,881:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:33:01,881:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:33:01,881:INFO:PyCaret required dependencies:
2022-10-21 12:33:01,881:INFO:                 pip: 21.2.4
2022-10-21 12:33:01,881:INFO:          setuptools: 58.0.4
2022-10-21 12:33:01,881:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:33:01,881:INFO:             IPython: 7.29.0
2022-10-21 12:33:01,881:INFO:          ipywidgets: 7.6.5
2022-10-21 12:33:01,881:INFO:                tqdm: 4.62.3
2022-10-21 12:33:01,881:INFO:               numpy: 1.22.4
2022-10-21 12:33:01,881:INFO:              pandas: 1.4.4
2022-10-21 12:33:01,881:INFO:              jinja2: 3.1.2
2022-10-21 12:33:01,881:INFO:               scipy: 1.8.1
2022-10-21 12:33:01,881:INFO:              joblib: 1.1.0
2022-10-21 12:33:01,882:INFO:             sklearn: 1.0.2
2022-10-21 12:33:01,882:INFO:                pyod: 1.0.5
2022-10-21 12:33:01,882:INFO:            imblearn: 0.9.0
2022-10-21 12:33:01,882:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:33:01,882:INFO:            lightgbm: 3.3.2
2022-10-21 12:33:01,882:INFO:               numba: 0.55.2
2022-10-21 12:33:01,882:INFO:            requests: 2.28.1
2022-10-21 12:33:01,884:INFO:          matplotlib: 3.4.3
2022-10-21 12:33:01,885:INFO:          scikitplot: 0.3.7
2022-10-21 12:33:01,885:INFO:         yellowbrick: 1.4
2022-10-21 12:33:01,885:INFO:              plotly: 5.5.0
2022-10-21 12:33:01,885:INFO:             kaleido: 0.2.1
2022-10-21 12:33:01,885:INFO:         statsmodels: 0.13.2
2022-10-21 12:33:01,885:INFO:              sktime: 0.13.4
2022-10-21 12:33:01,885:INFO:               tbats: 1.1.1
2022-10-21 12:33:01,886:INFO:            pmdarima: 1.8.5
2022-10-21 12:33:01,886:INFO:              psutil: 5.9.2
2022-10-21 12:33:01,886:INFO:PyCaret optional dependencies:
2022-10-21 12:33:01,886:INFO:                shap: 0.41.0
2022-10-21 12:33:01,886:INFO:           interpret: Not installed
2022-10-21 12:33:01,886:INFO:                umap: Not installed
2022-10-21 12:33:01,886:INFO:    pandas_profiling: Not installed
2022-10-21 12:33:01,886:INFO:  explainerdashboard: Not installed
2022-10-21 12:33:01,886:INFO:             autoviz: Not installed
2022-10-21 12:33:01,886:INFO:           fairlearn: Not installed
2022-10-21 12:33:01,886:INFO:             xgboost: Not installed
2022-10-21 12:33:01,887:INFO:            catboost: 1.1
2022-10-21 12:33:01,887:INFO:              kmodes: Not installed
2022-10-21 12:33:01,887:INFO:             mlxtend: Not installed
2022-10-21 12:33:01,887:INFO:       statsforecast: 1.1.1
2022-10-21 12:33:01,887:INFO:        tune_sklearn: Not installed
2022-10-21 12:33:01,887:INFO:                 ray: Not installed
2022-10-21 12:33:01,887:INFO:            hyperopt: Not installed
2022-10-21 12:33:01,887:INFO:              optuna: Not installed
2022-10-21 12:33:01,887:INFO:               skopt: Not installed
2022-10-21 12:33:01,887:INFO:              mlflow: 1.29.0
2022-10-21 12:33:01,887:INFO:              gradio: Not installed
2022-10-21 12:33:01,887:INFO:             fastapi: Not installed
2022-10-21 12:33:01,887:INFO:             uvicorn: Not installed
2022-10-21 12:33:01,887:INFO:              m2cgen: Not installed
2022-10-21 12:33:01,887:INFO:           evidently: Not installed
2022-10-21 12:33:01,887:INFO:                nltk: 3.6.5
2022-10-21 12:33:01,888:INFO:            pyLDAvis: Not installed
2022-10-21 12:33:01,888:INFO:              gensim: Not installed
2022-10-21 12:33:01,888:INFO:               spacy: Not installed
2022-10-21 12:33:01,888:INFO:           wordcloud: Not installed
2022-10-21 12:33:01,888:INFO:            textblob: Not installed
2022-10-21 12:33:01,888:INFO:               fugue: Not installed
2022-10-21 12:33:01,888:INFO:           streamlit: Not installed
2022-10-21 12:33:01,888:INFO:             prophet: 1.1.1
2022-10-21 12:33:01,888:INFO:None
2022-10-21 12:33:01,888:INFO:Set up data.
2022-10-21 12:33:01,899:INFO:Set up train/test split.
2022-10-21 12:33:01,910:INFO:Set up index.
2022-10-21 12:33:01,911:INFO:Set up folding strategy.
2022-10-21 12:33:01,911:INFO:Assigning column types.
2022-10-21 12:33:01,919:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:33:01,920:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:33:01,928:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:33:01,937:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:33:02,111:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:33:02,616:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:33:02,618:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 12:33:47,247:INFO:PyCaret RegressionExperiment
2022-10-21 12:33:47,247:INFO:Logging name: reg-default-name
2022-10-21 12:33:47,247:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:33:47,248:INFO:version 3.0.0.rc4
2022-10-21 12:33:47,248:INFO:Initializing setup()
2022-10-21 12:33:47,248:INFO:self.USI: f11e
2022-10-21 12:33:47,248:INFO:self.variable_keys: {'variable_keys', '_available_plots', 'n_jobs_param', 'y_test', 'fold_generator', 'transform_target_method_param', 'log_plots_param', 'idx', '_gpu_n_jobs_param', 'pipeline', 'y_train', 'transform_target_param', 'USI', 'fold_shuffle_param', 'X_train', '_all_models', 'html_param', 'logging_param', 'memory', 'y', 'X_test', 'seed', '_all_models_internal', 'X', 'exp_name_log', 'master_model_container', 'target_param', 'exp_id', 'fold_groups_param', 'gpu_param', 'data', '_all_metrics', 'display_container', '_ml_usecase'}
2022-10-21 12:33:47,248:INFO:Checking environment
2022-10-21 12:33:47,248:INFO:python_version: 3.9.7
2022-10-21 12:33:47,248:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:33:47,248:INFO:machine: x86_64
2022-10-21 12:33:47,248:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:33:47,248:INFO:Memory: svmem(total=8589934592, available=2398851072, percent=72.1, used=4230324224, free=55967744, active=2348007424, inactive=2337087488, wired=1882316800)
2022-10-21 12:33:47,249:INFO:Physical Core: 2
2022-10-21 12:33:47,249:INFO:Logical Core: 4
2022-10-21 12:33:47,249:INFO:Checking libraries
2022-10-21 12:33:47,249:INFO:System:
2022-10-21 12:33:47,249:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:33:47,249:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:33:47,249:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:33:47,249:INFO:PyCaret required dependencies:
2022-10-21 12:33:47,249:INFO:                 pip: 21.2.4
2022-10-21 12:33:47,249:INFO:          setuptools: 58.0.4
2022-10-21 12:33:47,250:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:33:47,250:INFO:             IPython: 7.29.0
2022-10-21 12:33:47,250:INFO:          ipywidgets: 7.6.5
2022-10-21 12:33:47,250:INFO:                tqdm: 4.62.3
2022-10-21 12:33:47,250:INFO:               numpy: 1.22.4
2022-10-21 12:33:47,250:INFO:              pandas: 1.4.4
2022-10-21 12:33:47,250:INFO:              jinja2: 3.1.2
2022-10-21 12:33:47,250:INFO:               scipy: 1.8.1
2022-10-21 12:33:47,250:INFO:              joblib: 1.1.0
2022-10-21 12:33:47,251:INFO:             sklearn: 1.0.2
2022-10-21 12:33:47,251:INFO:                pyod: 1.0.5
2022-10-21 12:33:47,251:INFO:            imblearn: 0.9.0
2022-10-21 12:33:47,251:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:33:47,251:INFO:            lightgbm: 3.3.2
2022-10-21 12:33:47,251:INFO:               numba: 0.55.2
2022-10-21 12:33:47,251:INFO:            requests: 2.28.1
2022-10-21 12:33:47,252:INFO:          matplotlib: 3.4.3
2022-10-21 12:33:47,252:INFO:          scikitplot: 0.3.7
2022-10-21 12:33:47,252:INFO:         yellowbrick: 1.4
2022-10-21 12:33:47,252:INFO:              plotly: 5.5.0
2022-10-21 12:33:47,252:INFO:             kaleido: 0.2.1
2022-10-21 12:33:47,253:INFO:         statsmodels: 0.13.2
2022-10-21 12:33:47,253:INFO:              sktime: 0.13.4
2022-10-21 12:33:47,253:INFO:               tbats: 1.1.1
2022-10-21 12:33:47,253:INFO:            pmdarima: 1.8.5
2022-10-21 12:33:47,255:INFO:              psutil: 5.9.2
2022-10-21 12:33:47,255:INFO:PyCaret optional dependencies:
2022-10-21 12:33:47,255:INFO:                shap: 0.41.0
2022-10-21 12:33:47,256:INFO:           interpret: Not installed
2022-10-21 12:33:47,256:INFO:                umap: Not installed
2022-10-21 12:33:47,256:INFO:    pandas_profiling: Not installed
2022-10-21 12:33:47,256:INFO:  explainerdashboard: Not installed
2022-10-21 12:33:47,256:INFO:             autoviz: Not installed
2022-10-21 12:33:47,256:INFO:           fairlearn: Not installed
2022-10-21 12:33:47,256:INFO:             xgboost: Not installed
2022-10-21 12:33:47,256:INFO:            catboost: 1.1
2022-10-21 12:33:47,256:INFO:              kmodes: Not installed
2022-10-21 12:33:47,256:INFO:             mlxtend: Not installed
2022-10-21 12:33:47,256:INFO:       statsforecast: 1.1.1
2022-10-21 12:33:47,256:INFO:        tune_sklearn: Not installed
2022-10-21 12:33:47,256:INFO:                 ray: Not installed
2022-10-21 12:33:47,256:INFO:            hyperopt: Not installed
2022-10-21 12:33:47,256:INFO:              optuna: Not installed
2022-10-21 12:33:47,256:INFO:               skopt: Not installed
2022-10-21 12:33:47,257:INFO:              mlflow: 1.29.0
2022-10-21 12:33:47,257:INFO:              gradio: Not installed
2022-10-21 12:33:47,257:INFO:             fastapi: Not installed
2022-10-21 12:33:47,257:INFO:             uvicorn: Not installed
2022-10-21 12:33:47,257:INFO:              m2cgen: Not installed
2022-10-21 12:33:47,257:INFO:           evidently: Not installed
2022-10-21 12:33:47,257:INFO:                nltk: 3.6.5
2022-10-21 12:33:47,257:INFO:            pyLDAvis: Not installed
2022-10-21 12:33:47,257:INFO:              gensim: Not installed
2022-10-21 12:33:47,257:INFO:               spacy: Not installed
2022-10-21 12:33:47,257:INFO:           wordcloud: Not installed
2022-10-21 12:33:47,257:INFO:            textblob: Not installed
2022-10-21 12:33:47,258:INFO:               fugue: Not installed
2022-10-21 12:33:47,258:INFO:           streamlit: Not installed
2022-10-21 12:33:47,259:INFO:             prophet: 1.1.1
2022-10-21 12:33:47,259:INFO:None
2022-10-21 12:33:47,259:INFO:Set up data.
2022-10-21 12:33:47,277:INFO:Set up train/test split.
2022-10-21 12:33:47,291:INFO:Set up index.
2022-10-21 12:33:47,293:INFO:Set up folding strategy.
2022-10-21 12:33:47,293:INFO:Assigning column types.
2022-10-21 12:33:47,321:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:33:47,322:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:33:47,352:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:33:47,365:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:33:47,760:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:33:48,116:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:33:48,118:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 12:38:06,441:INFO:PyCaret RegressionExperiment
2022-10-21 12:38:06,442:INFO:Logging name: reg-default-name
2022-10-21 12:38:06,442:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:38:06,442:INFO:version 3.0.0.rc4
2022-10-21 12:38:06,442:INFO:Initializing setup()
2022-10-21 12:38:06,442:INFO:self.USI: 7bdc
2022-10-21 12:38:06,442:INFO:self.variable_keys: {'variable_keys', '_available_plots', 'n_jobs_param', 'y_test', 'fold_generator', 'transform_target_method_param', 'log_plots_param', 'idx', '_gpu_n_jobs_param', 'pipeline', 'y_train', 'transform_target_param', 'USI', 'fold_shuffle_param', 'X_train', '_all_models', 'html_param', 'logging_param', 'memory', 'y', 'X_test', 'seed', '_all_models_internal', 'X', 'exp_name_log', 'master_model_container', 'target_param', 'exp_id', 'fold_groups_param', 'gpu_param', 'data', '_all_metrics', 'display_container', '_ml_usecase'}
2022-10-21 12:38:06,442:INFO:Checking environment
2022-10-21 12:38:06,442:INFO:python_version: 3.9.7
2022-10-21 12:38:06,442:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:38:06,442:INFO:machine: x86_64
2022-10-21 12:38:06,442:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:38:06,443:INFO:Memory: svmem(total=8589934592, available=2296901632, percent=73.3, used=4165808128, free=14921728, active=2284019712, inactive=2280251392, wired=1881788416)
2022-10-21 12:38:06,443:INFO:Physical Core: 2
2022-10-21 12:38:06,443:INFO:Logical Core: 4
2022-10-21 12:38:06,443:INFO:Checking libraries
2022-10-21 12:38:06,453:INFO:System:
2022-10-21 12:38:06,453:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:38:06,453:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:38:06,454:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:38:06,454:INFO:PyCaret required dependencies:
2022-10-21 12:38:06,454:INFO:                 pip: 21.2.4
2022-10-21 12:38:06,454:INFO:          setuptools: 58.0.4
2022-10-21 12:38:06,454:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:38:06,454:INFO:             IPython: 7.29.0
2022-10-21 12:38:06,454:INFO:          ipywidgets: 7.6.5
2022-10-21 12:38:06,454:INFO:                tqdm: 4.62.3
2022-10-21 12:38:06,454:INFO:               numpy: 1.22.4
2022-10-21 12:38:06,454:INFO:              pandas: 1.4.4
2022-10-21 12:38:06,454:INFO:              jinja2: 3.1.2
2022-10-21 12:38:06,454:INFO:               scipy: 1.8.1
2022-10-21 12:38:06,454:INFO:              joblib: 1.1.0
2022-10-21 12:38:06,454:INFO:             sklearn: 1.0.2
2022-10-21 12:38:06,454:INFO:                pyod: 1.0.5
2022-10-21 12:38:06,454:INFO:            imblearn: 0.9.0
2022-10-21 12:38:06,454:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:38:06,455:INFO:            lightgbm: 3.3.2
2022-10-21 12:38:06,455:INFO:               numba: 0.55.2
2022-10-21 12:38:06,455:INFO:            requests: 2.28.1
2022-10-21 12:38:06,455:INFO:          matplotlib: 3.4.3
2022-10-21 12:38:06,455:INFO:          scikitplot: 0.3.7
2022-10-21 12:38:06,455:INFO:         yellowbrick: 1.4
2022-10-21 12:38:06,455:INFO:              plotly: 5.5.0
2022-10-21 12:38:06,455:INFO:             kaleido: 0.2.1
2022-10-21 12:38:06,455:INFO:         statsmodels: 0.13.2
2022-10-21 12:38:06,455:INFO:              sktime: 0.13.4
2022-10-21 12:38:06,455:INFO:               tbats: 1.1.1
2022-10-21 12:38:06,455:INFO:            pmdarima: 1.8.5
2022-10-21 12:38:06,455:INFO:              psutil: 5.9.2
2022-10-21 12:38:06,455:INFO:PyCaret optional dependencies:
2022-10-21 12:38:06,455:INFO:                shap: 0.41.0
2022-10-21 12:38:06,455:INFO:           interpret: Not installed
2022-10-21 12:38:06,455:INFO:                umap: Not installed
2022-10-21 12:38:06,455:INFO:    pandas_profiling: Not installed
2022-10-21 12:38:06,455:INFO:  explainerdashboard: Not installed
2022-10-21 12:38:06,456:INFO:             autoviz: Not installed
2022-10-21 12:38:06,456:INFO:           fairlearn: Not installed
2022-10-21 12:38:06,456:INFO:             xgboost: Not installed
2022-10-21 12:38:06,456:INFO:            catboost: 1.1
2022-10-21 12:38:06,456:INFO:              kmodes: Not installed
2022-10-21 12:38:06,456:INFO:             mlxtend: Not installed
2022-10-21 12:38:06,456:INFO:       statsforecast: 1.1.1
2022-10-21 12:38:06,456:INFO:        tune_sklearn: Not installed
2022-10-21 12:38:06,456:INFO:                 ray: Not installed
2022-10-21 12:38:06,456:INFO:            hyperopt: Not installed
2022-10-21 12:38:06,456:INFO:              optuna: Not installed
2022-10-21 12:38:06,456:INFO:               skopt: Not installed
2022-10-21 12:38:06,456:INFO:              mlflow: 1.29.0
2022-10-21 12:38:06,456:INFO:              gradio: Not installed
2022-10-21 12:38:06,456:INFO:             fastapi: Not installed
2022-10-21 12:38:06,456:INFO:             uvicorn: Not installed
2022-10-21 12:38:06,456:INFO:              m2cgen: Not installed
2022-10-21 12:38:06,456:INFO:           evidently: Not installed
2022-10-21 12:38:06,456:INFO:                nltk: 3.6.5
2022-10-21 12:38:06,456:INFO:            pyLDAvis: Not installed
2022-10-21 12:38:06,457:INFO:              gensim: Not installed
2022-10-21 12:38:06,457:INFO:               spacy: Not installed
2022-10-21 12:38:06,457:INFO:           wordcloud: Not installed
2022-10-21 12:38:06,457:INFO:            textblob: Not installed
2022-10-21 12:38:06,457:INFO:               fugue: Not installed
2022-10-21 12:38:06,457:INFO:           streamlit: Not installed
2022-10-21 12:38:06,457:INFO:             prophet: 1.1.1
2022-10-21 12:38:06,457:INFO:None
2022-10-21 12:38:06,457:INFO:Set up data.
2022-10-21 12:38:06,471:INFO:Set up train/test split.
2022-10-21 12:38:06,486:INFO:Set up index.
2022-10-21 12:38:06,487:INFO:Set up folding strategy.
2022-10-21 12:38:06,487:INFO:Assigning column types.
2022-10-21 12:38:06,502:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:38:06,502:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:38:06,515:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:38:06,525:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:38:06,989:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:38:07,402:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:38:07,405:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 12:45:36,108:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:45:36,110:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:45:36,110:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:45:36,110:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 12:45:38,796:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 12:45:39,778:INFO:PyCaret RegressionExperiment
2022-10-21 12:45:39,779:INFO:Logging name: reg-default-name
2022-10-21 12:45:39,779:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 12:45:39,779:INFO:version 3.0.0.rc4
2022-10-21 12:45:39,779:INFO:Initializing setup()
2022-10-21 12:45:39,779:INFO:self.USI: b1b3
2022-10-21 12:45:39,779:INFO:self.variable_keys: {'X_train', 'n_jobs_param', 'seed', '_available_plots', 'html_param', 'fold_generator', '_all_models_internal', 'X_test', 'fold_groups_param', 'gpu_param', 'fold_shuffle_param', '_all_models', 'exp_id', '_gpu_n_jobs_param', 'data', 'X', 'log_plots_param', 'y_test', 'exp_name_log', '_ml_usecase', 'USI', 'transform_target_param', 'y_train', 'variable_keys', '_all_metrics', 'pipeline', 'y', 'idx', 'target_param', 'master_model_container', 'memory', 'display_container', 'logging_param', 'transform_target_method_param'}
2022-10-21 12:45:39,779:INFO:Checking environment
2022-10-21 12:45:39,779:INFO:python_version: 3.9.7
2022-10-21 12:45:39,779:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 12:45:39,779:INFO:machine: x86_64
2022-10-21 12:45:39,779:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:45:39,779:INFO:Memory: svmem(total=8589934592, available=2234859520, percent=74.0, used=4077637632, free=17285120, active=2219188224, inactive=2195013632, wired=1858449408)
2022-10-21 12:45:39,779:INFO:Physical Core: 2
2022-10-21 12:45:39,779:INFO:Logical Core: 4
2022-10-21 12:45:39,779:INFO:Checking libraries
2022-10-21 12:45:39,780:INFO:System:
2022-10-21 12:45:39,780:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 12:45:39,780:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 12:45:39,780:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 12:45:39,780:INFO:PyCaret required dependencies:
2022-10-21 12:45:39,780:INFO:                 pip: 21.2.4
2022-10-21 12:45:39,780:INFO:          setuptools: 58.0.4
2022-10-21 12:45:39,780:INFO:             pycaret: 3.0.0rc4
2022-10-21 12:45:39,780:INFO:             IPython: 7.29.0
2022-10-21 12:45:39,780:INFO:          ipywidgets: 7.6.5
2022-10-21 12:45:39,780:INFO:                tqdm: 4.62.3
2022-10-21 12:45:39,781:INFO:               numpy: 1.22.4
2022-10-21 12:45:39,781:INFO:              pandas: 1.4.4
2022-10-21 12:45:39,781:INFO:              jinja2: 3.1.2
2022-10-21 12:45:39,781:INFO:               scipy: 1.8.1
2022-10-21 12:45:39,781:INFO:              joblib: 1.1.0
2022-10-21 12:45:39,781:INFO:             sklearn: 1.0.2
2022-10-21 12:45:39,781:INFO:                pyod: 1.0.5
2022-10-21 12:45:39,781:INFO:            imblearn: 0.9.0
2022-10-21 12:45:39,782:INFO:   category_encoders: 2.5.1.post0
2022-10-21 12:45:39,782:INFO:            lightgbm: 3.3.2
2022-10-21 12:45:39,782:INFO:               numba: 0.55.2
2022-10-21 12:45:39,782:INFO:            requests: 2.28.1
2022-10-21 12:45:39,782:INFO:          matplotlib: 3.4.3
2022-10-21 12:45:39,782:INFO:          scikitplot: 0.3.7
2022-10-21 12:45:39,782:INFO:         yellowbrick: 1.4
2022-10-21 12:45:39,782:INFO:              plotly: 5.5.0
2022-10-21 12:45:39,782:INFO:             kaleido: 0.2.1
2022-10-21 12:45:39,782:INFO:         statsmodels: 0.13.2
2022-10-21 12:45:39,783:INFO:              sktime: 0.13.4
2022-10-21 12:45:39,783:INFO:               tbats: 1.1.1
2022-10-21 12:45:39,783:INFO:            pmdarima: 1.8.5
2022-10-21 12:45:39,783:INFO:              psutil: 5.9.2
2022-10-21 12:45:39,783:INFO:PyCaret optional dependencies:
2022-10-21 12:45:39,789:INFO:                shap: 0.41.0
2022-10-21 12:45:39,789:INFO:           interpret: Not installed
2022-10-21 12:45:39,789:INFO:                umap: Not installed
2022-10-21 12:45:39,789:INFO:    pandas_profiling: Not installed
2022-10-21 12:45:39,789:INFO:  explainerdashboard: Not installed
2022-10-21 12:45:39,789:INFO:             autoviz: Not installed
2022-10-21 12:45:39,790:INFO:           fairlearn: Not installed
2022-10-21 12:45:39,790:INFO:             xgboost: Not installed
2022-10-21 12:45:39,790:INFO:            catboost: 1.1
2022-10-21 12:45:39,790:INFO:              kmodes: Not installed
2022-10-21 12:45:39,790:INFO:             mlxtend: Not installed
2022-10-21 12:45:39,790:INFO:       statsforecast: 1.1.1
2022-10-21 12:45:39,790:INFO:        tune_sklearn: Not installed
2022-10-21 12:45:39,791:INFO:                 ray: Not installed
2022-10-21 12:45:39,791:INFO:            hyperopt: Not installed
2022-10-21 12:45:39,791:INFO:              optuna: Not installed
2022-10-21 12:45:39,791:INFO:               skopt: Not installed
2022-10-21 12:45:39,791:INFO:              mlflow: 1.29.0
2022-10-21 12:45:39,791:INFO:              gradio: Not installed
2022-10-21 12:45:39,791:INFO:             fastapi: Not installed
2022-10-21 12:45:39,791:INFO:             uvicorn: Not installed
2022-10-21 12:45:39,791:INFO:              m2cgen: Not installed
2022-10-21 12:45:39,791:INFO:           evidently: Not installed
2022-10-21 12:45:39,791:INFO:                nltk: 3.6.5
2022-10-21 12:45:39,791:INFO:            pyLDAvis: Not installed
2022-10-21 12:45:39,791:INFO:              gensim: Not installed
2022-10-21 12:45:39,791:INFO:               spacy: Not installed
2022-10-21 12:45:39,791:INFO:           wordcloud: Not installed
2022-10-21 12:45:39,791:INFO:            textblob: Not installed
2022-10-21 12:45:39,791:INFO:               fugue: Not installed
2022-10-21 12:45:39,791:INFO:           streamlit: Not installed
2022-10-21 12:45:39,791:INFO:             prophet: 1.1.1
2022-10-21 12:45:39,791:INFO:None
2022-10-21 12:45:39,792:INFO:Set up data.
2022-10-21 12:45:39,801:INFO:Set up train/test split.
2022-10-21 12:45:39,808:INFO:Set up index.
2022-10-21 12:45:39,808:INFO:Set up folding strategy.
2022-10-21 12:45:39,808:INFO:Assigning column types.
2022-10-21 12:45:39,813:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 12:45:39,813:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 12:45:39,823:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 12:45:39,830:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 12:45:39,932:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 12:45:40,293:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 12:45:40,294:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:38,218:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:09:38,219:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:09:38,220:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:09:38,220:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:09:41,243:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:09:41,885:INFO:PyCaret RegressionExperiment
2022-10-21 13:09:41,885:INFO:Logging name: reg-default-name
2022-10-21 13:09:41,885:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 13:09:41,886:INFO:version 3.0.0.rc4
2022-10-21 13:09:41,886:INFO:Initializing setup()
2022-10-21 13:09:41,886:INFO:self.USI: fca3
2022-10-21 13:09:41,886:INFO:self.variable_keys: {'display_container', '_ml_usecase', 'seed', 'USI', 'target_param', 'fold_groups_param', 'X_test', 'master_model_container', 'y_train', 'y_test', 'idx', 'pipeline', '_all_metrics', 'X', 'exp_name_log', '_available_plots', 'transform_target_method_param', 'transform_target_param', 'fold_generator', 'n_jobs_param', 'memory', 'variable_keys', '_gpu_n_jobs_param', '_all_models', 'html_param', 'gpu_param', 'X_train', 'fold_shuffle_param', 'exp_id', '_all_models_internal', 'log_plots_param', 'logging_param', 'data', 'y'}
2022-10-21 13:09:41,886:INFO:Checking environment
2022-10-21 13:09:41,886:INFO:python_version: 3.9.7
2022-10-21 13:09:41,886:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 13:09:41,886:INFO:machine: x86_64
2022-10-21 13:09:41,886:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 13:09:41,886:INFO:Memory: svmem(total=8589934592, available=2390249472, percent=72.2, used=4211392512, free=17530880, active=2374217728, inactive=2332016640, wired=1837174784)
2022-10-21 13:09:41,886:INFO:Physical Core: 2
2022-10-21 13:09:41,887:INFO:Logical Core: 4
2022-10-21 13:09:41,887:INFO:Checking libraries
2022-10-21 13:09:41,887:INFO:System:
2022-10-21 13:09:41,887:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 13:09:41,887:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 13:09:41,887:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 13:09:41,887:INFO:PyCaret required dependencies:
2022-10-21 13:09:41,887:INFO:                 pip: 21.2.4
2022-10-21 13:09:41,887:INFO:          setuptools: 58.0.4
2022-10-21 13:09:41,887:INFO:             pycaret: 3.0.0rc4
2022-10-21 13:09:41,887:INFO:             IPython: 7.29.0
2022-10-21 13:09:41,887:INFO:          ipywidgets: 7.6.5
2022-10-21 13:09:41,888:INFO:                tqdm: 4.62.3
2022-10-21 13:09:41,888:INFO:               numpy: 1.22.4
2022-10-21 13:09:41,888:INFO:              pandas: 1.4.4
2022-10-21 13:09:41,888:INFO:              jinja2: 3.1.2
2022-10-21 13:09:41,888:INFO:               scipy: 1.8.1
2022-10-21 13:09:41,888:INFO:              joblib: 1.1.0
2022-10-21 13:09:41,888:INFO:             sklearn: 1.0.2
2022-10-21 13:09:41,888:INFO:                pyod: 1.0.5
2022-10-21 13:09:41,888:INFO:            imblearn: 0.9.0
2022-10-21 13:09:41,888:INFO:   category_encoders: 2.5.1.post0
2022-10-21 13:09:41,888:INFO:            lightgbm: 3.3.2
2022-10-21 13:09:41,888:INFO:               numba: 0.55.2
2022-10-21 13:09:41,888:INFO:            requests: 2.28.1
2022-10-21 13:09:41,888:INFO:          matplotlib: 3.4.3
2022-10-21 13:09:41,888:INFO:          scikitplot: 0.3.7
2022-10-21 13:09:41,888:INFO:         yellowbrick: 1.4
2022-10-21 13:09:41,888:INFO:              plotly: 5.5.0
2022-10-21 13:09:41,888:INFO:             kaleido: 0.2.1
2022-10-21 13:09:41,888:INFO:         statsmodels: 0.13.2
2022-10-21 13:09:41,888:INFO:              sktime: 0.13.4
2022-10-21 13:09:41,889:INFO:               tbats: 1.1.1
2022-10-21 13:09:41,889:INFO:            pmdarima: 1.8.5
2022-10-21 13:09:41,889:INFO:              psutil: 5.9.2
2022-10-21 13:09:41,889:INFO:PyCaret optional dependencies:
2022-10-21 13:09:41,903:INFO:                shap: 0.41.0
2022-10-21 13:09:41,903:INFO:           interpret: Not installed
2022-10-21 13:09:41,903:INFO:                umap: Not installed
2022-10-21 13:09:41,903:INFO:    pandas_profiling: Not installed
2022-10-21 13:09:41,903:INFO:  explainerdashboard: Not installed
2022-10-21 13:09:41,904:INFO:             autoviz: Not installed
2022-10-21 13:09:41,904:INFO:           fairlearn: Not installed
2022-10-21 13:09:41,904:INFO:             xgboost: Not installed
2022-10-21 13:09:41,904:INFO:            catboost: 1.1
2022-10-21 13:09:41,904:INFO:              kmodes: Not installed
2022-10-21 13:09:41,904:INFO:             mlxtend: Not installed
2022-10-21 13:09:41,904:INFO:       statsforecast: 1.1.1
2022-10-21 13:09:41,904:INFO:        tune_sklearn: Not installed
2022-10-21 13:09:41,904:INFO:                 ray: Not installed
2022-10-21 13:09:41,904:INFO:            hyperopt: Not installed
2022-10-21 13:09:41,904:INFO:              optuna: Not installed
2022-10-21 13:09:41,904:INFO:               skopt: Not installed
2022-10-21 13:09:41,904:INFO:              mlflow: 1.29.0
2022-10-21 13:09:41,904:INFO:              gradio: Not installed
2022-10-21 13:09:41,904:INFO:             fastapi: Not installed
2022-10-21 13:09:41,904:INFO:             uvicorn: Not installed
2022-10-21 13:09:41,904:INFO:              m2cgen: Not installed
2022-10-21 13:09:41,904:INFO:           evidently: Not installed
2022-10-21 13:09:41,905:INFO:                nltk: 3.6.5
2022-10-21 13:09:41,905:INFO:            pyLDAvis: Not installed
2022-10-21 13:09:41,905:INFO:              gensim: Not installed
2022-10-21 13:09:41,905:INFO:               spacy: Not installed
2022-10-21 13:09:41,905:INFO:           wordcloud: Not installed
2022-10-21 13:09:41,905:INFO:            textblob: Not installed
2022-10-21 13:09:41,905:INFO:               fugue: Not installed
2022-10-21 13:09:41,905:INFO:           streamlit: Not installed
2022-10-21 13:09:41,905:INFO:             prophet: 1.1.1
2022-10-21 13:09:41,905:INFO:None
2022-10-21 13:09:41,905:INFO:Set up data.
2022-10-21 13:09:41,919:INFO:Set up train/test split.
2022-10-21 13:09:41,930:INFO:Set up index.
2022-10-21 13:09:41,931:INFO:Set up folding strategy.
2022-10-21 13:09:41,931:INFO:Assigning column types.
2022-10-21 13:09:41,937:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 13:09:41,938:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 13:09:41,954:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:09:41,964:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,159:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,338:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,343:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:42,343:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:42,476:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,484:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,492:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,617:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,698:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,699:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:42,699:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:42,701:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 13:09:42,710:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,718:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:09:42,839:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,004:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,005:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:43,006:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:43,018:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,035:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,160:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,250:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,251:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:43,251:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:43,253:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 13:09:43,270:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,416:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,533:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,534:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:43,534:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:43,563:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,699:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,804:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:43,805:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:43,805:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:43,806:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 13:09:44,038:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:44,137:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:44,138:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:44,138:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:44,296:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:44,404:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:09:44,405:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:44,405:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:44,409:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 13:09:44,553:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:44,678:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:44,679:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:44,869:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:09:44,982:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:44,983:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:44,984:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 13:09:45,265:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:45,266:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:45,588:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:45,589:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:45,594:INFO:Preparing preprocessing pipeline...
2022-10-21 13:09:45,595:INFO:Set up simple imputation.
2022-10-21 13:09:45,605:INFO:Set up encoding of ordinal features.
2022-10-21 13:09:45,611:INFO:Set up encoding of categorical features.
2022-10-21 13:09:45,612:INFO:Set up polynomial features.
2022-10-21 13:09:45,612:INFO:Set up variance threshold.
2022-10-21 13:09:45,612:INFO:Set up feature normalization.
2022-10-21 13:09:46,121:INFO:Finished creating preprocessing pipeline.
2022-10-21 13:09:46,157:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2022-10-21 13:09:46,157:INFO:Creating final display dataframe.
2022-10-21 13:09:47,600:INFO:Setup display_container:                  Description             Value
0                 Session id               122
1                     Target           charges
2                Target type        Regression
3                 Data shape        (1338, 49)
4           Train data shape         (936, 49)
5            Test data shape         (402, 49)
6           Ordinal features                 2
7           Numeric features                 3
8       Categorical features                 3
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15       Polynomial features              True
16         Polynomial degree                 2
17    Low variance threshold                 0
18                 Normalize              True
19          Normalize method            zscore
20            Fold Generator             KFold
21               Fold Number                10
22                  CPU Jobs                -1
23                   Use GPU             False
24            Log Experiment             False
25           Experiment Name  reg-default-name
26                       USI              fca3
2022-10-21 13:09:48,154:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:48,155:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:48,420:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:09:48,420:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:09:48,437:INFO:setup() successfully completed in 6.56s...............
2022-10-21 13:09:48,438:INFO:Initializing compare_models()
2022-10-21 13:09:48,438:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-21 13:09:48,438:INFO:Checking exceptions
2022-10-21 13:09:48,444:INFO:Preparing display monitor
2022-10-21 13:09:48,911:INFO:Initializing Linear Regression
2022-10-21 13:09:48,911:INFO:Total runtime is 1.0232130686442057e-05 minutes
2022-10-21 13:09:48,919:INFO:SubProcess create_model() called ==================================
2022-10-21 13:09:48,926:INFO:Initializing create_model()
2022-10-21 13:09:48,926:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:09:48,926:INFO:Checking exceptions
2022-10-21 13:09:48,949:INFO:Importing libraries
2022-10-21 13:09:48,950:INFO:Copying training dataset
2022-10-21 13:09:48,956:INFO:Defining folds
2022-10-21 13:09:48,957:INFO:Declaring metric variables
2022-10-21 13:09:48,978:INFO:Importing untrained model
2022-10-21 13:09:49,002:INFO:Linear Regression Imported successfully
2022-10-21 13:09:49,069:INFO:Starting cross validation
2022-10-21 13:09:49,110:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:10,350:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:10,353:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:10,356:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:10,430:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:11,971:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:12,039:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:12,047:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:12,104:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:14,092:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:14,169:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:14,202:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:14,220:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:15,711:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:15,788:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:15,837:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:15,870:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:17,227:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:17,307:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:18,310:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:18,372:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 13:10:18,893:INFO:Calculating mean and std
2022-10-21 13:10:18,901:INFO:Creating metrics dataframe
2022-10-21 13:10:18,913:INFO:Uploading results into container
2022-10-21 13:10:18,914:INFO:Uploading model into container now
2022-10-21 13:10:18,916:INFO:master_model_container: 1
2022-10-21 13:10:18,916:INFO:display_container: 2
2022-10-21 13:10:18,917:INFO:LinearRegression(n_jobs=-1)
2022-10-21 13:10:18,917:INFO:create_model() successfully completed......................................
2022-10-21 13:10:19,158:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:19,159:INFO:Creating metrics dataframe
2022-10-21 13:10:19,181:INFO:Initializing Lasso Regression
2022-10-21 13:10:19,181:INFO:Total runtime is 0.5045008818308512 minutes
2022-10-21 13:10:19,190:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:19,205:INFO:Initializing create_model()
2022-10-21 13:10:19,206:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:19,206:INFO:Checking exceptions
2022-10-21 13:10:19,210:INFO:Importing libraries
2022-10-21 13:10:19,210:INFO:Copying training dataset
2022-10-21 13:10:19,221:INFO:Defining folds
2022-10-21 13:10:19,222:INFO:Declaring metric variables
2022-10-21 13:10:19,316:INFO:Importing untrained model
2022-10-21 13:10:19,370:INFO:Lasso Regression Imported successfully
2022-10-21 13:10:19,472:INFO:Starting cross validation
2022-10-21 13:10:19,480:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:19,853:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.872e+09, tolerance: 1.200e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:19,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.938e+09, tolerance: 1.211e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:19,874:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+09, tolerance: 1.201e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:19,877:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+09, tolerance: 1.200e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:20,481:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.622e+09, tolerance: 1.201e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:20,598:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+09, tolerance: 1.242e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:20,600:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.006e+09, tolerance: 1.207e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:20,603:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.374e+09, tolerance: 1.231e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:21,062:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.154e+09, tolerance: 1.240e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:21,094:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+09, tolerance: 1.261e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:10:21,281:INFO:Calculating mean and std
2022-10-21 13:10:21,283:INFO:Creating metrics dataframe
2022-10-21 13:10:21,289:INFO:Uploading results into container
2022-10-21 13:10:21,290:INFO:Uploading model into container now
2022-10-21 13:10:21,291:INFO:master_model_container: 2
2022-10-21 13:10:21,292:INFO:display_container: 2
2022-10-21 13:10:21,292:INFO:Lasso(random_state=122)
2022-10-21 13:10:21,293:INFO:create_model() successfully completed......................................
2022-10-21 13:10:21,479:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:21,480:INFO:Creating metrics dataframe
2022-10-21 13:10:21,505:INFO:Initializing Ridge Regression
2022-10-21 13:10:21,506:INFO:Total runtime is 0.5432479302088419 minutes
2022-10-21 13:10:21,544:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:21,547:INFO:Initializing create_model()
2022-10-21 13:10:21,548:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:21,549:INFO:Checking exceptions
2022-10-21 13:10:21,555:INFO:Importing libraries
2022-10-21 13:10:21,555:INFO:Copying training dataset
2022-10-21 13:10:21,606:INFO:Defining folds
2022-10-21 13:10:21,606:INFO:Declaring metric variables
2022-10-21 13:10:21,633:INFO:Importing untrained model
2022-10-21 13:10:21,652:INFO:Ridge Regression Imported successfully
2022-10-21 13:10:21,683:INFO:Starting cross validation
2022-10-21 13:10:21,685:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:22,790:INFO:Calculating mean and std
2022-10-21 13:10:22,794:INFO:Creating metrics dataframe
2022-10-21 13:10:22,800:INFO:Uploading results into container
2022-10-21 13:10:22,802:INFO:Uploading model into container now
2022-10-21 13:10:22,803:INFO:master_model_container: 3
2022-10-21 13:10:22,803:INFO:display_container: 2
2022-10-21 13:10:22,804:INFO:Ridge(random_state=122)
2022-10-21 13:10:22,804:INFO:create_model() successfully completed......................................
2022-10-21 13:10:23,173:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:23,173:INFO:Creating metrics dataframe
2022-10-21 13:10:23,203:INFO:Initializing Elastic Net
2022-10-21 13:10:23,204:INFO:Total runtime is 0.5715490659077962 minutes
2022-10-21 13:10:23,241:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:23,242:INFO:Initializing create_model()
2022-10-21 13:10:23,247:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:23,247:INFO:Checking exceptions
2022-10-21 13:10:23,250:INFO:Importing libraries
2022-10-21 13:10:23,250:INFO:Copying training dataset
2022-10-21 13:10:23,307:INFO:Defining folds
2022-10-21 13:10:23,315:INFO:Declaring metric variables
2022-10-21 13:10:23,323:INFO:Importing untrained model
2022-10-21 13:10:23,361:INFO:Elastic Net Imported successfully
2022-10-21 13:10:23,382:INFO:Starting cross validation
2022-10-21 13:10:23,387:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:24,586:INFO:Calculating mean and std
2022-10-21 13:10:24,589:INFO:Creating metrics dataframe
2022-10-21 13:10:24,599:INFO:Uploading results into container
2022-10-21 13:10:24,600:INFO:Uploading model into container now
2022-10-21 13:10:24,600:INFO:master_model_container: 4
2022-10-21 13:10:24,601:INFO:display_container: 2
2022-10-21 13:10:24,601:INFO:ElasticNet(random_state=122)
2022-10-21 13:10:24,601:INFO:create_model() successfully completed......................................
2022-10-21 13:10:24,784:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:24,785:INFO:Creating metrics dataframe
2022-10-21 13:10:24,806:INFO:Initializing Least Angle Regression
2022-10-21 13:10:24,806:INFO:Total runtime is 0.5982555150985718 minutes
2022-10-21 13:10:24,853:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:24,867:INFO:Initializing create_model()
2022-10-21 13:10:24,867:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:24,868:INFO:Checking exceptions
2022-10-21 13:10:24,871:INFO:Importing libraries
2022-10-21 13:10:24,872:INFO:Copying training dataset
2022-10-21 13:10:24,887:INFO:Defining folds
2022-10-21 13:10:24,888:INFO:Declaring metric variables
2022-10-21 13:10:24,913:INFO:Importing untrained model
2022-10-21 13:10:24,932:INFO:Least Angle Regression Imported successfully
2022-10-21 13:10:24,964:INFO:Starting cross validation
2022-10-21 13:10:24,967:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:25,254:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,255:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,259:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,275:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.030e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,277:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.030e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,277:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.471e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,279:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.972e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,280:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.748e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,282:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.562e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,282:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.508e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,283:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.125e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,283:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.147e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,284:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.036e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,285:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.850e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,286:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.656e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,286:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.656e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,287:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.947e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,288:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.212e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,289:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.145e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,289:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.327e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,295:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.947e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,296:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.009e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,297:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.479e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,297:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.434e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,298:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.185e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,298:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.174e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,298:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.614e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,298:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.082e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,299:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.068e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,299:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.860e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,299:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.786e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,299:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.624e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,299:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.369e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,300:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.378e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,300:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.010e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,300:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.810e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,300:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.085e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.432e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.374e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.372e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.365e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,302:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.855e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,302:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.914e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,304:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.041e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,307:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.293e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,308:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.410e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,311:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.240e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,311:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.129e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,312:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.588e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,313:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.063e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,314:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.961e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,314:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.731e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,315:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.191e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,315:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.070e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,318:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.931e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,320:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.043e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 9.365e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,320:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.077e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,322:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.773e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,324:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.360e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,327:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.311e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,327:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.093e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,327:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.826e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,327:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.823e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,328:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.813e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,328:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.048e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,329:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.025e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,329:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.610e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.365e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,329:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.916e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,330:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.131e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,330:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.036e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,330:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.490e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,331:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.645e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,331:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.818e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,331:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.280e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,331:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.464e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.066e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.016e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.685e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.704e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,315:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.240e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.234e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.882e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.830e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.804e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.027e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.777e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.767e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=6.466e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.122e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.238e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.047e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.226e-05, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.803e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,336:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.358e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,336:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.797e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,352:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.613e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,353:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.457e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,354:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.230e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,355:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.201e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,356:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.097e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,358:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.900e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,358:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=9.487e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,359:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.190e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,369:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.083e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,372:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.947e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,373:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.916e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,373:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.242e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,374:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.885e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,375:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.199e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,387:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.452e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,389:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.315e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,389:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.193e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,389:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.107e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,390:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.009e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,390:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=9.216e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,390:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=7.358e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,392:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.358e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,392:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.531e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,392:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.110e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,393:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.848e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,393:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.691e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,394:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.652e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,396:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.078e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,396:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.859e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,397:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.687e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,397:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.232e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,397:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.232e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,738:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,747:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,749:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.858e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,752:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.049e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,754:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.988e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,756:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.607e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,756:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.607e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,757:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.266e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,760:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.106e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,760:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.106e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,761:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.211e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,762:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.655e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,763:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.345e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,764:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.008e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,765:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.855e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,765:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.806e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,765:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.526e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,766:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.473e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,766:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.326e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,766:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.145e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,767:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=9.114e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,768:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.143e+01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,769:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,770:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.846e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,770:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.432e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,771:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.170e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,771:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.130e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,771:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.100e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,772:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=6.807e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,775:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.280e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,778:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.156e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,779:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.225e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,779:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.825e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,779:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.330e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,780:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.183e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,781:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.940e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,781:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.193e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,781:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.571e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,781:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.398e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,782:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.862e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,782:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.091e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,783:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.979e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,785:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.665e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,787:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.175e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,788:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.478e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,788:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.199e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,788:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.388e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,789:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.169e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,789:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.714e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,789:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.624e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,790:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.577e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,790:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.163e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,790:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.850e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,794:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.139e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,794:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.566e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,794:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.557e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,794:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.643e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,795:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.618e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,795:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.436e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,795:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.192e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,795:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.199e-02, with an active set of 40 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,799:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.855e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.908e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,802:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.908e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,779:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.811e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,806:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.379e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,806:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.507e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,806:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.282e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.490e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.486e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,812:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.475e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,813:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.444e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,813:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.426e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.043e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,819:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:25,822:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.600e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.572e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.472e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.047e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,824:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.187e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,825:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.933e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,828:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.570e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,829:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.730e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,825:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.931e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,832:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.014e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,832:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.152e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,832:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.613e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.138e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.482e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.696e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,834:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.981e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,834:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.736e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,834:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.148e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,842:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.191e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,845:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.055e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.438e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.428e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.427e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.405e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.145e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,852:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.599e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,852:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.575e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,854:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.996e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,854:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.038e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,855:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.957e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,855:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.007e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,855:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.974e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,856:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.490e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,856:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.958e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,856:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.886e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,857:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.022e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,857:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.015e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,860:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.754e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,861:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.214e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.077e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.533e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.494e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.387e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.365e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.274e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.489e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.993e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:25,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.922e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,107:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:26,132:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:26,139:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.852e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,140:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.136e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,143:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.666e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,144:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.666e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,144:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.249e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,145:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.507e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,148:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.078e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,149:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.078e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,149:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.720e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,150:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.675e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,150:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.653e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,151:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.639e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,151:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.620e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,152:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.544e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,154:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.320e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,155:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.059e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,156:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.020e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,156:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.713e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,156:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.644e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,156:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.563e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,156:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.397e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,164:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.345e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,165:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.674e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,166:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.717e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,166:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.415e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,167:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.664e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,167:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.918e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,167:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.223e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,168:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.357e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,168:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.027e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,168:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.293e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,168:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.130e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,169:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.271e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,169:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.176e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,169:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.063e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,169:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.436e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,170:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.060e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,170:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.436e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,170:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.133e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,171:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.371e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,171:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.256e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,171:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.987e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,171:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.931e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,171:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.502e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,172:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.508e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,172:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.432e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,173:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.348e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,173:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.115e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,173:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.897e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,173:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.610e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,174:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.416e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,182:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.999e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,182:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.184e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,183:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.210e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,183:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.716e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,184:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.080e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,184:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.035e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,185:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.522e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,186:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,187:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,187:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,188:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.822e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,188:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.374e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,290:INFO:Calculating mean and std
2022-10-21 13:10:26,294:INFO:Creating metrics dataframe
2022-10-21 13:10:26,303:INFO:Uploading results into container
2022-10-21 13:10:26,305:INFO:Uploading model into container now
2022-10-21 13:10:26,305:INFO:master_model_container: 5
2022-10-21 13:10:26,306:INFO:display_container: 2
2022-10-21 13:10:26,308:INFO:Lars(random_state=122)
2022-10-21 13:10:26,308:INFO:create_model() successfully completed......................................
2022-10-21 13:10:26,483:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:26,483:INFO:Creating metrics dataframe
2022-10-21 13:10:26,505:INFO:Initializing Lasso Least Angle Regression
2022-10-21 13:10:26,505:INFO:Total runtime is 0.626574448744456 minutes
2022-10-21 13:10:26,534:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:26,535:INFO:Initializing create_model()
2022-10-21 13:10:26,535:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:26,535:INFO:Checking exceptions
2022-10-21 13:10:26,540:INFO:Importing libraries
2022-10-21 13:10:26,540:INFO:Copying training dataset
2022-10-21 13:10:26,564:INFO:Defining folds
2022-10-21 13:10:26,565:INFO:Declaring metric variables
2022-10-21 13:10:26,598:INFO:Importing untrained model
2022-10-21 13:10:26,615:INFO:Lasso Least Angle Regression Imported successfully
2022-10-21 13:10:26,663:INFO:Starting cross validation
2022-10-21 13:10:26,670:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:26,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:26,918:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.688e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,919:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.731e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,920:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.731e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,922:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.941e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,924:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.262e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,926:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.216e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,938:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:26,948:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.293e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,948:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.410e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,952:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.830e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,961:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.043e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,962:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.574e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,964:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.206e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,968:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:26,981:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:26,991:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.471e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,994:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.206e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,999:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.125e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:26,982:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.355e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,001:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.214e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,370:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:27,404:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.980e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,405:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.769e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,406:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.073e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,407:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.836e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,413:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.044e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,415:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.647e+00, previous alpha=1.480e+00, with an active set of 24 regressors.
  warnings.warn(

2022-10-21 13:10:27,449:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:27,450:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:27,454:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:27,455:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.570e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,463:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.348e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,463:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.225e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,464:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.348e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,465:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.323e+00, previous alpha=1.117e+00, with an active set of 27 regressors.
  warnings.warn(

2022-10-21 13:10:27,465:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.193e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,466:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.862e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,467:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.163e+00, previous alpha=4.862e+00, with an active set of 15 regressors.
  warnings.warn(

2022-10-21 13:10:27,491:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.992e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,494:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.992e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,495:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=2.933e+00, previous alpha=2.630e+00, with an active set of 18 regressors.
  warnings.warn(

2022-10-21 13:10:27,751:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:27,761:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.843e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,762:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.018e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,769:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.717e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,771:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.331e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,772:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.479e+00, previous alpha=1.249e+00, with an active set of 27 regressors.
  warnings.warn(

2022-10-21 13:10:27,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:10:27,835:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.759e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,839:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.371e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,852:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.529e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:10:27,853:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.413e+00, previous alpha=1.396e+00, with an active set of 24 regressors.
  warnings.warn(

2022-10-21 13:10:27,999:INFO:Calculating mean and std
2022-10-21 13:10:28,002:INFO:Creating metrics dataframe
2022-10-21 13:10:28,007:INFO:Uploading results into container
2022-10-21 13:10:28,010:INFO:Uploading model into container now
2022-10-21 13:10:28,012:INFO:master_model_container: 6
2022-10-21 13:10:28,012:INFO:display_container: 2
2022-10-21 13:10:28,014:INFO:LassoLars(random_state=122)
2022-10-21 13:10:28,014:INFO:create_model() successfully completed......................................
2022-10-21 13:10:28,182:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:28,183:INFO:Creating metrics dataframe
2022-10-21 13:10:28,214:INFO:Initializing Orthogonal Matching Pursuit
2022-10-21 13:10:28,218:INFO:Total runtime is 0.655124298731486 minutes
2022-10-21 13:10:28,270:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:28,270:INFO:Initializing create_model()
2022-10-21 13:10:28,271:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:28,271:INFO:Checking exceptions
2022-10-21 13:10:28,273:INFO:Importing libraries
2022-10-21 13:10:28,273:INFO:Copying training dataset
2022-10-21 13:10:28,307:INFO:Defining folds
2022-10-21 13:10:28,314:INFO:Declaring metric variables
2022-10-21 13:10:28,338:INFO:Importing untrained model
2022-10-21 13:10:28,352:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-21 13:10:28,400:INFO:Starting cross validation
2022-10-21 13:10:28,403:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:28,582:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:28,674:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:28,717:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:28,736:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:29,019:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:29,094:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:29,102:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:29,113:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:29,340:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:29,416:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:10:29,512:INFO:Calculating mean and std
2022-10-21 13:10:29,515:INFO:Creating metrics dataframe
2022-10-21 13:10:29,519:INFO:Uploading results into container
2022-10-21 13:10:29,520:INFO:Uploading model into container now
2022-10-21 13:10:29,522:INFO:master_model_container: 7
2022-10-21 13:10:29,522:INFO:display_container: 2
2022-10-21 13:10:29,523:INFO:OrthogonalMatchingPursuit()
2022-10-21 13:10:29,523:INFO:create_model() successfully completed......................................
2022-10-21 13:10:29,695:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:29,695:INFO:Creating metrics dataframe
2022-10-21 13:10:29,718:INFO:Initializing Bayesian Ridge
2022-10-21 13:10:29,718:INFO:Total runtime is 0.6801243662834168 minutes
2022-10-21 13:10:29,784:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:29,785:INFO:Initializing create_model()
2022-10-21 13:10:29,785:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:29,785:INFO:Checking exceptions
2022-10-21 13:10:29,790:INFO:Importing libraries
2022-10-21 13:10:29,790:INFO:Copying training dataset
2022-10-21 13:10:29,820:INFO:Defining folds
2022-10-21 13:10:29,821:INFO:Declaring metric variables
2022-10-21 13:10:29,839:INFO:Importing untrained model
2022-10-21 13:10:29,863:INFO:Bayesian Ridge Imported successfully
2022-10-21 13:10:29,903:INFO:Starting cross validation
2022-10-21 13:10:29,907:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:31,071:INFO:Calculating mean and std
2022-10-21 13:10:31,073:INFO:Creating metrics dataframe
2022-10-21 13:10:31,082:INFO:Uploading results into container
2022-10-21 13:10:31,083:INFO:Uploading model into container now
2022-10-21 13:10:31,084:INFO:master_model_container: 8
2022-10-21 13:10:31,084:INFO:display_container: 2
2022-10-21 13:10:31,085:INFO:BayesianRidge()
2022-10-21 13:10:31,085:INFO:create_model() successfully completed......................................
2022-10-21 13:10:31,255:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:31,256:INFO:Creating metrics dataframe
2022-10-21 13:10:31,291:INFO:Initializing Passive Aggressive Regressor
2022-10-21 13:10:31,294:INFO:Total runtime is 0.7063685178756715 minutes
2022-10-21 13:10:31,302:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:31,302:INFO:Initializing create_model()
2022-10-21 13:10:31,303:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:31,304:INFO:Checking exceptions
2022-10-21 13:10:31,307:INFO:Importing libraries
2022-10-21 13:10:31,307:INFO:Copying training dataset
2022-10-21 13:10:31,348:INFO:Defining folds
2022-10-21 13:10:31,350:INFO:Declaring metric variables
2022-10-21 13:10:31,373:INFO:Importing untrained model
2022-10-21 13:10:31,395:INFO:Passive Aggressive Regressor Imported successfully
2022-10-21 13:10:31,441:INFO:Starting cross validation
2022-10-21 13:10:31,449:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:33,713:INFO:Calculating mean and std
2022-10-21 13:10:33,716:INFO:Creating metrics dataframe
2022-10-21 13:10:33,720:INFO:Uploading results into container
2022-10-21 13:10:33,722:INFO:Uploading model into container now
2022-10-21 13:10:33,723:INFO:master_model_container: 9
2022-10-21 13:10:33,723:INFO:display_container: 2
2022-10-21 13:10:33,724:INFO:PassiveAggressiveRegressor(random_state=122)
2022-10-21 13:10:33,724:INFO:create_model() successfully completed......................................
2022-10-21 13:10:33,890:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:33,890:INFO:Creating metrics dataframe
2022-10-21 13:10:33,919:INFO:Initializing Huber Regressor
2022-10-21 13:10:33,919:INFO:Total runtime is 0.7501445968945821 minutes
2022-10-21 13:10:33,931:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:33,932:INFO:Initializing create_model()
2022-10-21 13:10:33,932:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:33,933:INFO:Checking exceptions
2022-10-21 13:10:33,937:INFO:Importing libraries
2022-10-21 13:10:33,937:INFO:Copying training dataset
2022-10-21 13:10:33,987:INFO:Defining folds
2022-10-21 13:10:33,987:INFO:Declaring metric variables
2022-10-21 13:10:34,017:INFO:Importing untrained model
2022-10-21 13:10:34,032:INFO:Huber Regressor Imported successfully
2022-10-21 13:10:34,084:INFO:Starting cross validation
2022-10-21 13:10:34,101:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:34,665:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:34,678:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:34,690:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:34,697:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:35,385:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:35,387:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:35,434:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:35,460:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:35,868:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:35,889:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:10:35,980:INFO:Calculating mean and std
2022-10-21 13:10:35,982:INFO:Creating metrics dataframe
2022-10-21 13:10:35,988:INFO:Uploading results into container
2022-10-21 13:10:35,989:INFO:Uploading model into container now
2022-10-21 13:10:35,990:INFO:master_model_container: 10
2022-10-21 13:10:35,991:INFO:display_container: 2
2022-10-21 13:10:35,991:INFO:HuberRegressor()
2022-10-21 13:10:35,991:INFO:create_model() successfully completed......................................
2022-10-21 13:10:36,162:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:36,162:INFO:Creating metrics dataframe
2022-10-21 13:10:36,185:INFO:Initializing K Neighbors Regressor
2022-10-21 13:10:36,185:INFO:Total runtime is 0.7879008293151856 minutes
2022-10-21 13:10:36,196:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:36,197:INFO:Initializing create_model()
2022-10-21 13:10:36,197:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:36,197:INFO:Checking exceptions
2022-10-21 13:10:36,202:INFO:Importing libraries
2022-10-21 13:10:36,202:INFO:Copying training dataset
2022-10-21 13:10:36,251:INFO:Defining folds
2022-10-21 13:10:36,251:INFO:Declaring metric variables
2022-10-21 13:10:36,273:INFO:Importing untrained model
2022-10-21 13:10:36,307:INFO:K Neighbors Regressor Imported successfully
2022-10-21 13:10:36,373:INFO:Starting cross validation
2022-10-21 13:10:36,380:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:37,587:INFO:Calculating mean and std
2022-10-21 13:10:37,589:INFO:Creating metrics dataframe
2022-10-21 13:10:37,596:INFO:Uploading results into container
2022-10-21 13:10:37,597:INFO:Uploading model into container now
2022-10-21 13:10:37,598:INFO:master_model_container: 11
2022-10-21 13:10:37,599:INFO:display_container: 2
2022-10-21 13:10:37,600:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-21 13:10:37,601:INFO:create_model() successfully completed......................................
2022-10-21 13:10:37,753:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:37,753:INFO:Creating metrics dataframe
2022-10-21 13:10:37,777:INFO:Initializing Decision Tree Regressor
2022-10-21 13:10:37,777:INFO:Total runtime is 0.814445161819458 minutes
2022-10-21 13:10:37,788:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:37,789:INFO:Initializing create_model()
2022-10-21 13:10:37,789:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:37,789:INFO:Checking exceptions
2022-10-21 13:10:37,823:INFO:Importing libraries
2022-10-21 13:10:37,824:INFO:Copying training dataset
2022-10-21 13:10:37,851:INFO:Defining folds
2022-10-21 13:10:37,851:INFO:Declaring metric variables
2022-10-21 13:10:37,871:INFO:Importing untrained model
2022-10-21 13:10:37,885:INFO:Decision Tree Regressor Imported successfully
2022-10-21 13:10:37,987:INFO:Starting cross validation
2022-10-21 13:10:37,990:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:39,267:INFO:Calculating mean and std
2022-10-21 13:10:39,269:INFO:Creating metrics dataframe
2022-10-21 13:10:39,277:INFO:Uploading results into container
2022-10-21 13:10:39,278:INFO:Uploading model into container now
2022-10-21 13:10:39,279:INFO:master_model_container: 12
2022-10-21 13:10:39,279:INFO:display_container: 2
2022-10-21 13:10:39,280:INFO:DecisionTreeRegressor(random_state=122)
2022-10-21 13:10:39,281:INFO:create_model() successfully completed......................................
2022-10-21 13:10:39,452:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:39,452:INFO:Creating metrics dataframe
2022-10-21 13:10:39,482:INFO:Initializing Random Forest Regressor
2022-10-21 13:10:39,482:INFO:Total runtime is 0.8428625663121542 minutes
2022-10-21 13:10:39,537:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:39,540:INFO:Initializing create_model()
2022-10-21 13:10:39,571:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:39,571:INFO:Checking exceptions
2022-10-21 13:10:39,583:INFO:Importing libraries
2022-10-21 13:10:39,583:INFO:Copying training dataset
2022-10-21 13:10:39,602:INFO:Defining folds
2022-10-21 13:10:39,605:INFO:Declaring metric variables
2022-10-21 13:10:39,623:INFO:Importing untrained model
2022-10-21 13:10:39,645:INFO:Random Forest Regressor Imported successfully
2022-10-21 13:10:39,778:INFO:Starting cross validation
2022-10-21 13:10:39,783:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:46,050:INFO:Calculating mean and std
2022-10-21 13:10:46,053:INFO:Creating metrics dataframe
2022-10-21 13:10:46,060:INFO:Uploading results into container
2022-10-21 13:10:46,062:INFO:Uploading model into container now
2022-10-21 13:10:46,064:INFO:master_model_container: 13
2022-10-21 13:10:46,064:INFO:display_container: 2
2022-10-21 13:10:46,065:INFO:RandomForestRegressor(n_jobs=-1, random_state=122)
2022-10-21 13:10:46,065:INFO:create_model() successfully completed......................................
2022-10-21 13:10:46,230:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:46,230:INFO:Creating metrics dataframe
2022-10-21 13:10:46,254:INFO:Initializing Extra Trees Regressor
2022-10-21 13:10:46,255:INFO:Total runtime is 0.9557352662086487 minutes
2022-10-21 13:10:46,267:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:46,367:INFO:Initializing create_model()
2022-10-21 13:10:46,367:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:46,367:INFO:Checking exceptions
2022-10-21 13:10:46,370:INFO:Importing libraries
2022-10-21 13:10:46,370:INFO:Copying training dataset
2022-10-21 13:10:46,420:INFO:Defining folds
2022-10-21 13:10:46,420:INFO:Declaring metric variables
2022-10-21 13:10:46,436:INFO:Importing untrained model
2022-10-21 13:10:46,452:INFO:Extra Trees Regressor Imported successfully
2022-10-21 13:10:46,501:INFO:Starting cross validation
2022-10-21 13:10:46,504:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:51,198:INFO:Calculating mean and std
2022-10-21 13:10:51,201:INFO:Creating metrics dataframe
2022-10-21 13:10:51,206:INFO:Uploading results into container
2022-10-21 13:10:51,207:INFO:Uploading model into container now
2022-10-21 13:10:51,208:INFO:master_model_container: 14
2022-10-21 13:10:51,209:INFO:display_container: 2
2022-10-21 13:10:51,209:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=122)
2022-10-21 13:10:51,210:INFO:create_model() successfully completed......................................
2022-10-21 13:10:51,401:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:51,402:INFO:Creating metrics dataframe
2022-10-21 13:10:51,435:INFO:Initializing AdaBoost Regressor
2022-10-21 13:10:51,436:INFO:Total runtime is 1.042082929611206 minutes
2022-10-21 13:10:51,506:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:51,507:INFO:Initializing create_model()
2022-10-21 13:10:51,507:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:51,507:INFO:Checking exceptions
2022-10-21 13:10:51,515:INFO:Importing libraries
2022-10-21 13:10:51,515:INFO:Copying training dataset
2022-10-21 13:10:51,548:INFO:Defining folds
2022-10-21 13:10:51,549:INFO:Declaring metric variables
2022-10-21 13:10:51,571:INFO:Importing untrained model
2022-10-21 13:10:51,586:INFO:AdaBoost Regressor Imported successfully
2022-10-21 13:10:51,615:INFO:Starting cross validation
2022-10-21 13:10:51,617:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:53,570:INFO:Calculating mean and std
2022-10-21 13:10:53,572:INFO:Creating metrics dataframe
2022-10-21 13:10:53,579:INFO:Uploading results into container
2022-10-21 13:10:53,580:INFO:Uploading model into container now
2022-10-21 13:10:53,581:INFO:master_model_container: 15
2022-10-21 13:10:53,581:INFO:display_container: 2
2022-10-21 13:10:53,582:INFO:AdaBoostRegressor(random_state=122)
2022-10-21 13:10:53,582:INFO:create_model() successfully completed......................................
2022-10-21 13:10:53,740:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:53,740:INFO:Creating metrics dataframe
2022-10-21 13:10:53,771:INFO:Initializing Gradient Boosting Regressor
2022-10-21 13:10:53,771:INFO:Total runtime is 1.0810027321179707 minutes
2022-10-21 13:10:53,800:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:53,802:INFO:Initializing create_model()
2022-10-21 13:10:53,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:53,808:INFO:Checking exceptions
2022-10-21 13:10:53,813:INFO:Importing libraries
2022-10-21 13:10:53,814:INFO:Copying training dataset
2022-10-21 13:10:53,839:INFO:Defining folds
2022-10-21 13:10:53,839:INFO:Declaring metric variables
2022-10-21 13:10:53,871:INFO:Importing untrained model
2022-10-21 13:10:53,899:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 13:10:53,962:INFO:Starting cross validation
2022-10-21 13:10:53,965:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:10:57,834:INFO:Calculating mean and std
2022-10-21 13:10:57,838:INFO:Creating metrics dataframe
2022-10-21 13:10:57,847:INFO:Uploading results into container
2022-10-21 13:10:57,848:INFO:Uploading model into container now
2022-10-21 13:10:57,852:INFO:master_model_container: 16
2022-10-21 13:10:57,852:INFO:display_container: 2
2022-10-21 13:10:57,855:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 13:10:57,855:INFO:create_model() successfully completed......................................
2022-10-21 13:10:58,064:INFO:SubProcess create_model() end ==================================
2022-10-21 13:10:58,064:INFO:Creating metrics dataframe
2022-10-21 13:10:58,091:INFO:Initializing Light Gradient Boosting Machine
2022-10-21 13:10:58,091:INFO:Total runtime is 1.1530125300089518 minutes
2022-10-21 13:10:58,154:INFO:SubProcess create_model() called ==================================
2022-10-21 13:10:58,156:INFO:Initializing create_model()
2022-10-21 13:10:58,173:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:10:58,173:INFO:Checking exceptions
2022-10-21 13:10:58,186:INFO:Importing libraries
2022-10-21 13:10:58,186:INFO:Copying training dataset
2022-10-21 13:10:58,202:INFO:Defining folds
2022-10-21 13:10:58,203:INFO:Declaring metric variables
2022-10-21 13:10:58,225:INFO:Importing untrained model
2022-10-21 13:10:58,260:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-21 13:10:58,299:INFO:Starting cross validation
2022-10-21 13:10:58,302:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:11:03,389:INFO:Calculating mean and std
2022-10-21 13:11:03,411:INFO:Creating metrics dataframe
2022-10-21 13:11:03,416:INFO:Uploading results into container
2022-10-21 13:11:03,417:INFO:Uploading model into container now
2022-10-21 13:11:03,417:INFO:master_model_container: 17
2022-10-21 13:11:03,417:INFO:display_container: 2
2022-10-21 13:11:03,418:INFO:LGBMRegressor(random_state=122)
2022-10-21 13:11:03,418:INFO:create_model() successfully completed......................................
2022-10-21 13:11:03,585:INFO:SubProcess create_model() end ==================================
2022-10-21 13:11:03,585:INFO:Creating metrics dataframe
2022-10-21 13:11:03,616:INFO:Initializing CatBoost Regressor
2022-10-21 13:11:03,616:INFO:Total runtime is 1.2450944662094117 minutes
2022-10-21 13:11:03,629:INFO:SubProcess create_model() called ==================================
2022-10-21 13:11:03,631:INFO:Initializing create_model()
2022-10-21 13:11:03,631:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:11:03,632:INFO:Checking exceptions
2022-10-21 13:11:03,634:INFO:Importing libraries
2022-10-21 13:11:03,635:INFO:Copying training dataset
2022-10-21 13:11:03,705:INFO:Defining folds
2022-10-21 13:11:03,705:INFO:Declaring metric variables
2022-10-21 13:11:03,725:INFO:Importing untrained model
2022-10-21 13:11:03,784:INFO:CatBoost Regressor Imported successfully
2022-10-21 13:11:03,803:INFO:Starting cross validation
2022-10-21 13:11:03,806:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:11:43,960:INFO:Calculating mean and std
2022-10-21 13:11:43,964:INFO:Creating metrics dataframe
2022-10-21 13:11:43,969:INFO:Uploading results into container
2022-10-21 13:11:43,970:INFO:Uploading model into container now
2022-10-21 13:11:43,971:INFO:master_model_container: 18
2022-10-21 13:11:43,971:INFO:display_container: 2
2022-10-21 13:11:43,971:INFO:<catboost.core.CatBoostRegressor object at 0x7f86435974f0>
2022-10-21 13:11:43,971:INFO:create_model() successfully completed......................................
2022-10-21 13:11:44,163:INFO:SubProcess create_model() end ==================================
2022-10-21 13:11:44,164:INFO:Creating metrics dataframe
2022-10-21 13:11:44,190:INFO:Initializing Dummy Regressor
2022-10-21 13:11:44,191:INFO:Total runtime is 1.921339813868205 minutes
2022-10-21 13:11:44,238:INFO:SubProcess create_model() called ==================================
2022-10-21 13:11:44,239:INFO:Initializing create_model()
2022-10-21 13:11:44,239:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f865e240df0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:11:44,239:INFO:Checking exceptions
2022-10-21 13:11:44,243:INFO:Importing libraries
2022-10-21 13:11:44,247:INFO:Copying training dataset
2022-10-21 13:11:44,263:INFO:Defining folds
2022-10-21 13:11:44,263:INFO:Declaring metric variables
2022-10-21 13:11:44,299:INFO:Importing untrained model
2022-10-21 13:11:44,348:INFO:Dummy Regressor Imported successfully
2022-10-21 13:11:44,390:INFO:Starting cross validation
2022-10-21 13:11:44,392:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:11:45,389:INFO:Calculating mean and std
2022-10-21 13:11:45,393:INFO:Creating metrics dataframe
2022-10-21 13:11:45,408:INFO:Uploading results into container
2022-10-21 13:11:45,409:INFO:Uploading model into container now
2022-10-21 13:11:45,414:INFO:master_model_container: 19
2022-10-21 13:11:45,414:INFO:display_container: 2
2022-10-21 13:11:45,415:INFO:DummyRegressor()
2022-10-21 13:11:45,415:INFO:create_model() successfully completed......................................
2022-10-21 13:11:45,781:INFO:SubProcess create_model() end ==================================
2022-10-21 13:11:45,781:INFO:Creating metrics dataframe
2022-10-21 13:11:45,973:INFO:Initializing create_model()
2022-10-21 13:11:45,973:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=GradientBoostingRegressor(random_state=122), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:11:45,973:INFO:Checking exceptions
2022-10-21 13:11:45,987:INFO:Importing libraries
2022-10-21 13:11:45,987:INFO:Copying training dataset
2022-10-21 13:11:46,003:INFO:Defining folds
2022-10-21 13:11:46,003:INFO:Declaring metric variables
2022-10-21 13:11:46,004:INFO:Importing untrained model
2022-10-21 13:11:46,004:INFO:Declaring custom model
2022-10-21 13:11:46,005:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 13:11:46,006:INFO:Cross validation set to False
2022-10-21 13:11:46,007:INFO:Fitting Model
2022-10-21 13:11:46,675:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 13:11:46,675:INFO:create_model() successfully completed......................................
2022-10-21 13:11:46,990:INFO:master_model_container: 19
2022-10-21 13:11:46,991:INFO:display_container: 2
2022-10-21 13:11:46,992:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 13:11:46,992:INFO:compare_models() successfully completed......................................
2022-10-21 13:11:47,002:INFO:Initializing evaluate_model()
2022-10-21 13:11:47,002:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=GradientBoostingRegressor(random_state=122), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-21 13:11:47,116:INFO:Initializing plot_model()
2022-10-21 13:11:47,117:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, system=True)
2022-10-21 13:11:47,119:INFO:Checking exceptions
2022-10-21 13:11:47,133:INFO:Preloading libraries
2022-10-21 13:11:47,202:INFO:Copying training dataset
2022-10-21 13:11:47,202:INFO:Plot type: pipeline
2022-10-21 13:11:47,786:INFO:Visual Rendered Successfully
2022-10-21 13:11:47,958:INFO:plot_model() successfully completed......................................
2022-10-21 13:11:48,032:INFO:Initializing predict_model()
2022-10-21 13:11:48,032:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f864317de50>)
2022-10-21 13:11:48,072:INFO:Checking exceptions
2022-10-21 13:11:48,073:INFO:Preloading libraries
2022-10-21 13:11:48,494:INFO:Initializing save_model()
2022-10-21 13:11:48,494:INFO:save_model(model=GradientBoostingRegressor(random_state=122), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 13:11:48,495:INFO:Adding model into prep_pipe
2022-10-21 13:11:48,524:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 13:11:48,553:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 13:11:48,553:INFO:save_model() successfully completed......................................
2022-10-21 13:12:36,880:INFO:Initializing plot_model()
2022-10-21 13:12:36,882:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, system=True)
2022-10-21 13:12:36,883:INFO:Checking exceptions
2022-10-21 13:12:36,886:INFO:Preloading libraries
2022-10-21 13:12:36,903:INFO:Copying training dataset
2022-10-21 13:12:36,903:INFO:Plot type: parameter
2022-10-21 13:12:36,909:INFO:Visual Rendered Successfully
2022-10-21 13:12:37,111:INFO:plot_model() successfully completed......................................
2022-10-21 13:12:38,309:INFO:Initializing plot_model()
2022-10-21 13:12:38,309:INFO:plot_model(plot=residuals, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, system=True)
2022-10-21 13:12:38,309:INFO:Checking exceptions
2022-10-21 13:12:38,314:INFO:Preloading libraries
2022-10-21 13:12:38,335:INFO:Copying training dataset
2022-10-21 13:12:38,335:INFO:Plot type: residuals
2022-10-21 13:12:38,662:INFO:Fitting Model
2022-10-21 13:12:38,721:INFO:Scoring test/hold-out set
2022-10-21 13:12:39,804:INFO:Visual Rendered Successfully
2022-10-21 13:12:40,027:INFO:plot_model() successfully completed......................................
2022-10-21 13:12:41,134:INFO:Initializing plot_model()
2022-10-21 13:12:41,134:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, system=True)
2022-10-21 13:12:41,134:INFO:Checking exceptions
2022-10-21 13:12:41,138:INFO:Preloading libraries
2022-10-21 13:12:41,154:INFO:Copying training dataset
2022-10-21 13:12:41,154:INFO:Plot type: error
2022-10-21 13:12:41,333:INFO:Fitting Model
2022-10-21 13:12:41,334:INFO:Scoring test/hold-out set
2022-10-21 13:12:41,728:INFO:Visual Rendered Successfully
2022-10-21 13:12:41,920:INFO:plot_model() successfully completed......................................
2022-10-21 13:12:42,681:INFO:Initializing plot_model()
2022-10-21 13:12:42,682:INFO:plot_model(plot=cooks, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, system=True)
2022-10-21 13:12:42,682:INFO:Checking exceptions
2022-10-21 13:12:42,686:INFO:Preloading libraries
2022-10-21 13:12:42,702:INFO:Copying training dataset
2022-10-21 13:12:42,703:INFO:Plot type: cooks
2022-10-21 13:12:42,904:INFO:Fitting Model
2022-10-21 13:12:43,267:INFO:Visual Rendered Successfully
2022-10-21 13:12:43,401:INFO:plot_model() successfully completed......................................
2022-10-21 13:12:44,572:INFO:Initializing plot_model()
2022-10-21 13:12:44,572:INFO:plot_model(plot=rfe, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f865e1ae910>, system=True)
2022-10-21 13:12:44,572:INFO:Checking exceptions
2022-10-21 13:12:44,576:INFO:Preloading libraries
2022-10-21 13:12:44,590:INFO:Copying training dataset
2022-10-21 13:12:44,590:INFO:Plot type: rfe
2022-10-21 13:12:44,775:INFO:Fitting Model
2022-10-21 13:22:55,309:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:22:55,328:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:22:55,328:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:22:55,328:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:22:58,800:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:25:32,098:INFO:PyCaret RegressionExperiment
2022-10-21 13:25:32,100:INFO:Logging name: reg-default-name
2022-10-21 13:25:32,100:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 13:25:32,100:INFO:version 3.0.0.rc4
2022-10-21 13:25:32,100:INFO:Initializing setup()
2022-10-21 13:25:32,100:INFO:self.USI: 6151
2022-10-21 13:25:32,100:INFO:self.variable_keys: {'exp_id', 'n_jobs_param', 'fold_generator', 'X_train', 'gpu_param', 'memory', '_gpu_n_jobs_param', 'display_container', 'log_plots_param', 'pipeline', 'logging_param', 'X_test', 'y', 'X', 'exp_name_log', '_all_metrics', 'y_train', 'transform_target_param', '_available_plots', 'transform_target_method_param', 'variable_keys', 'USI', '_all_models', 'fold_shuffle_param', 'idx', '_ml_usecase', '_all_models_internal', 'y_test', 'fold_groups_param', 'master_model_container', 'target_param', 'seed', 'data', 'html_param'}
2022-10-21 13:25:32,101:INFO:Checking environment
2022-10-21 13:25:32,101:INFO:python_version: 3.9.7
2022-10-21 13:25:32,101:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 13:25:32,101:INFO:machine: x86_64
2022-10-21 13:25:32,101:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 13:25:32,102:INFO:Memory: svmem(total=8589934592, available=2345857024, percent=72.7, used=4179558400, free=15724544, active=2334695424, inactive=2324074496, wired=1844862976)
2022-10-21 13:25:32,102:INFO:Physical Core: 2
2022-10-21 13:25:32,102:INFO:Logical Core: 4
2022-10-21 13:25:32,102:INFO:Checking libraries
2022-10-21 13:25:32,102:INFO:System:
2022-10-21 13:25:32,102:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 13:25:32,102:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 13:25:32,102:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 13:25:32,103:INFO:PyCaret required dependencies:
2022-10-21 13:25:32,105:INFO:                 pip: 21.2.4
2022-10-21 13:25:32,105:INFO:          setuptools: 58.0.4
2022-10-21 13:25:32,105:INFO:             pycaret: 3.0.0rc4
2022-10-21 13:25:32,106:INFO:             IPython: 7.29.0
2022-10-21 13:25:32,106:INFO:          ipywidgets: 7.6.5
2022-10-21 13:25:32,106:INFO:                tqdm: 4.62.3
2022-10-21 13:25:32,106:INFO:               numpy: 1.22.4
2022-10-21 13:25:32,106:INFO:              pandas: 1.4.4
2022-10-21 13:25:32,106:INFO:              jinja2: 3.1.2
2022-10-21 13:25:32,106:INFO:               scipy: 1.8.1
2022-10-21 13:25:32,106:INFO:              joblib: 1.1.0
2022-10-21 13:25:32,106:INFO:             sklearn: 1.0.2
2022-10-21 13:25:32,107:INFO:                pyod: 1.0.5
2022-10-21 13:25:32,107:INFO:            imblearn: 0.9.0
2022-10-21 13:25:32,107:INFO:   category_encoders: 2.5.1.post0
2022-10-21 13:25:32,107:INFO:            lightgbm: 3.3.2
2022-10-21 13:25:32,107:INFO:               numba: 0.55.2
2022-10-21 13:25:32,107:INFO:            requests: 2.28.1
2022-10-21 13:25:32,107:INFO:          matplotlib: 3.4.3
2022-10-21 13:25:32,107:INFO:          scikitplot: 0.3.7
2022-10-21 13:25:32,107:INFO:         yellowbrick: 1.4
2022-10-21 13:25:32,107:INFO:              plotly: 5.5.0
2022-10-21 13:25:32,107:INFO:             kaleido: 0.2.1
2022-10-21 13:25:32,107:INFO:         statsmodels: 0.13.2
2022-10-21 13:25:32,108:INFO:              sktime: 0.13.4
2022-10-21 13:25:32,108:INFO:               tbats: 1.1.1
2022-10-21 13:25:32,108:INFO:            pmdarima: 1.8.5
2022-10-21 13:25:32,108:INFO:              psutil: 5.9.2
2022-10-21 13:25:32,108:INFO:PyCaret optional dependencies:
2022-10-21 13:25:32,124:INFO:                shap: 0.41.0
2022-10-21 13:25:32,124:INFO:           interpret: Not installed
2022-10-21 13:25:32,125:INFO:                umap: Not installed
2022-10-21 13:25:32,125:INFO:    pandas_profiling: Not installed
2022-10-21 13:25:32,125:INFO:  explainerdashboard: Not installed
2022-10-21 13:25:32,125:INFO:             autoviz: Not installed
2022-10-21 13:25:32,125:INFO:           fairlearn: Not installed
2022-10-21 13:25:32,125:INFO:             xgboost: Not installed
2022-10-21 13:25:32,125:INFO:            catboost: 1.1
2022-10-21 13:25:32,125:INFO:              kmodes: Not installed
2022-10-21 13:25:32,125:INFO:             mlxtend: Not installed
2022-10-21 13:25:32,125:INFO:       statsforecast: 1.1.1
2022-10-21 13:25:32,125:INFO:        tune_sklearn: Not installed
2022-10-21 13:25:32,125:INFO:                 ray: Not installed
2022-10-21 13:25:32,125:INFO:            hyperopt: Not installed
2022-10-21 13:25:32,125:INFO:              optuna: Not installed
2022-10-21 13:25:32,125:INFO:               skopt: Not installed
2022-10-21 13:25:32,126:INFO:              mlflow: 1.29.0
2022-10-21 13:25:32,126:INFO:              gradio: Not installed
2022-10-21 13:25:32,126:INFO:             fastapi: Not installed
2022-10-21 13:25:32,126:INFO:             uvicorn: Not installed
2022-10-21 13:25:32,126:INFO:              m2cgen: Not installed
2022-10-21 13:25:32,126:INFO:           evidently: Not installed
2022-10-21 13:25:32,126:INFO:                nltk: 3.6.5
2022-10-21 13:25:32,126:INFO:            pyLDAvis: Not installed
2022-10-21 13:25:32,126:INFO:              gensim: Not installed
2022-10-21 13:25:32,126:INFO:               spacy: Not installed
2022-10-21 13:25:32,126:INFO:           wordcloud: Not installed
2022-10-21 13:25:32,126:INFO:            textblob: Not installed
2022-10-21 13:25:32,126:INFO:               fugue: Not installed
2022-10-21 13:25:32,126:INFO:           streamlit: Not installed
2022-10-21 13:25:32,126:INFO:             prophet: 1.1.1
2022-10-21 13:25:32,127:INFO:None
2022-10-21 13:25:32,127:INFO:Set up data.
2022-10-21 13:25:32,142:INFO:Set up train/test split.
2022-10-21 13:25:32,157:INFO:Set up index.
2022-10-21 13:25:32,158:INFO:Set up folding strategy.
2022-10-21 13:25:32,158:INFO:Assigning column types.
2022-10-21 13:25:32,177:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 13:25:32,177:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,194:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,210:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,424:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,564:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,565:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:32,565:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:32,663:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,676:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,684:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,773:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,843:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,844:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:32,844:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:32,845:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 13:25:32,854:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,862:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:25:32,951:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,055:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,056:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:33,059:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:33,073:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,083:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,228:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,313:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,314:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:33,315:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:33,316:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 13:25:33,335:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,541:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,792:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,796:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:33,806:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:33,844:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:25:33,993:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:34,125:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:34,127:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:34,128:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:34,129:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 13:25:34,360:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:34,484:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:34,485:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:34,486:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:34,682:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:34,774:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:25:34,775:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:34,775:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:34,776:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 13:25:34,916:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:35,016:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:35,016:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:35,243:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:25:35,475:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:35,475:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:35,476:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 13:25:35,699:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:35,699:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:35,883:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:25:35,883:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:25:35,884:INFO:Preparing preprocessing pipeline...
2022-10-21 13:25:35,887:INFO:Set up simple imputation.
2022-10-21 13:25:35,892:INFO:Set up encoding of ordinal features.
2022-10-21 13:25:35,895:INFO:Set up encoding of categorical features.
2022-10-21 13:25:35,895:INFO:Set up polynomial features.
2022-10-21 13:25:35,895:INFO:Set up variance threshold.
2022-10-21 13:25:35,895:INFO:Set up binning of numerical features.
2022-10-21 13:25:35,897:INFO:Set up feature normalization.
2022-10-21 13:26:19,761:INFO:PyCaret RegressionExperiment
2022-10-21 13:26:19,763:INFO:Logging name: reg-default-name
2022-10-21 13:26:19,763:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 13:26:19,763:INFO:version 3.0.0.rc4
2022-10-21 13:26:19,763:INFO:Initializing setup()
2022-10-21 13:26:19,763:INFO:self.USI: d49b
2022-10-21 13:26:19,764:INFO:self.variable_keys: {'exp_id', 'n_jobs_param', 'fold_generator', 'X_train', 'gpu_param', 'memory', '_gpu_n_jobs_param', 'display_container', 'log_plots_param', 'pipeline', 'logging_param', 'X_test', 'y', 'X', 'exp_name_log', '_all_metrics', 'y_train', 'transform_target_param', '_available_plots', 'transform_target_method_param', 'variable_keys', 'USI', '_all_models', 'fold_shuffle_param', 'idx', '_ml_usecase', '_all_models_internal', 'y_test', 'fold_groups_param', 'master_model_container', 'target_param', 'seed', 'data', 'html_param'}
2022-10-21 13:26:19,764:INFO:Checking environment
2022-10-21 13:26:19,764:INFO:python_version: 3.9.7
2022-10-21 13:26:19,764:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 13:26:19,764:INFO:machine: x86_64
2022-10-21 13:26:19,764:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 13:26:19,764:INFO:Memory: svmem(total=8589934592, available=2355232768, percent=72.6, used=4205633536, free=17555456, active=2338848768, inactive=2336817152, wired=1866784768)
2022-10-21 13:26:19,768:INFO:Physical Core: 2
2022-10-21 13:26:19,769:INFO:Logical Core: 4
2022-10-21 13:26:19,769:INFO:Checking libraries
2022-10-21 13:26:19,769:INFO:System:
2022-10-21 13:26:19,769:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 13:26:19,769:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 13:26:19,769:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 13:26:19,769:INFO:PyCaret required dependencies:
2022-10-21 13:26:19,769:INFO:                 pip: 21.2.4
2022-10-21 13:26:19,769:INFO:          setuptools: 58.0.4
2022-10-21 13:26:19,769:INFO:             pycaret: 3.0.0rc4
2022-10-21 13:26:19,769:INFO:             IPython: 7.29.0
2022-10-21 13:26:19,770:INFO:          ipywidgets: 7.6.5
2022-10-21 13:26:19,770:INFO:                tqdm: 4.62.3
2022-10-21 13:26:19,770:INFO:               numpy: 1.22.4
2022-10-21 13:26:19,770:INFO:              pandas: 1.4.4
2022-10-21 13:26:19,770:INFO:              jinja2: 3.1.2
2022-10-21 13:26:19,770:INFO:               scipy: 1.8.1
2022-10-21 13:26:19,770:INFO:              joblib: 1.1.0
2022-10-21 13:26:19,770:INFO:             sklearn: 1.0.2
2022-10-21 13:26:19,777:INFO:                pyod: 1.0.5
2022-10-21 13:26:19,778:INFO:            imblearn: 0.9.0
2022-10-21 13:26:19,778:INFO:   category_encoders: 2.5.1.post0
2022-10-21 13:26:19,778:INFO:            lightgbm: 3.3.2
2022-10-21 13:26:19,778:INFO:               numba: 0.55.2
2022-10-21 13:26:19,778:INFO:            requests: 2.28.1
2022-10-21 13:26:19,778:INFO:          matplotlib: 3.4.3
2022-10-21 13:26:19,778:INFO:          scikitplot: 0.3.7
2022-10-21 13:26:19,779:INFO:         yellowbrick: 1.4
2022-10-21 13:26:19,779:INFO:              plotly: 5.5.0
2022-10-21 13:26:19,779:INFO:             kaleido: 0.2.1
2022-10-21 13:26:19,779:INFO:         statsmodels: 0.13.2
2022-10-21 13:26:19,779:INFO:              sktime: 0.13.4
2022-10-21 13:26:19,779:INFO:               tbats: 1.1.1
2022-10-21 13:26:19,779:INFO:            pmdarima: 1.8.5
2022-10-21 13:26:19,779:INFO:              psutil: 5.9.2
2022-10-21 13:26:19,779:INFO:PyCaret optional dependencies:
2022-10-21 13:26:19,779:INFO:                shap: 0.41.0
2022-10-21 13:26:19,779:INFO:           interpret: Not installed
2022-10-21 13:26:19,779:INFO:                umap: Not installed
2022-10-21 13:26:19,779:INFO:    pandas_profiling: Not installed
2022-10-21 13:26:19,779:INFO:  explainerdashboard: Not installed
2022-10-21 13:26:19,779:INFO:             autoviz: Not installed
2022-10-21 13:26:19,780:INFO:           fairlearn: Not installed
2022-10-21 13:26:19,780:INFO:             xgboost: Not installed
2022-10-21 13:26:19,780:INFO:            catboost: 1.1
2022-10-21 13:26:19,780:INFO:              kmodes: Not installed
2022-10-21 13:26:19,780:INFO:             mlxtend: Not installed
2022-10-21 13:26:19,780:INFO:       statsforecast: 1.1.1
2022-10-21 13:26:19,780:INFO:        tune_sklearn: Not installed
2022-10-21 13:26:19,781:INFO:                 ray: Not installed
2022-10-21 13:26:19,781:INFO:            hyperopt: Not installed
2022-10-21 13:26:19,782:INFO:              optuna: Not installed
2022-10-21 13:26:19,782:INFO:               skopt: Not installed
2022-10-21 13:26:19,782:INFO:              mlflow: 1.29.0
2022-10-21 13:26:19,782:INFO:              gradio: Not installed
2022-10-21 13:26:19,782:INFO:             fastapi: Not installed
2022-10-21 13:26:19,782:INFO:             uvicorn: Not installed
2022-10-21 13:26:19,782:INFO:              m2cgen: Not installed
2022-10-21 13:26:19,782:INFO:           evidently: Not installed
2022-10-21 13:26:19,783:INFO:                nltk: 3.6.5
2022-10-21 13:26:19,783:INFO:            pyLDAvis: Not installed
2022-10-21 13:26:19,783:INFO:              gensim: Not installed
2022-10-21 13:26:19,783:INFO:               spacy: Not installed
2022-10-21 13:26:19,783:INFO:           wordcloud: Not installed
2022-10-21 13:26:19,783:INFO:            textblob: Not installed
2022-10-21 13:26:19,783:INFO:               fugue: Not installed
2022-10-21 13:26:19,783:INFO:           streamlit: Not installed
2022-10-21 13:26:19,783:INFO:             prophet: 1.1.1
2022-10-21 13:26:19,784:INFO:None
2022-10-21 13:26:19,784:INFO:Set up data.
2022-10-21 13:26:19,814:INFO:Set up train/test split.
2022-10-21 13:26:19,886:INFO:Set up index.
2022-10-21 13:26:19,887:INFO:Set up folding strategy.
2022-10-21 13:26:19,887:INFO:Assigning column types.
2022-10-21 13:26:19,895:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 13:26:19,895:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 13:26:19,908:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:26:19,941:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,251:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,374:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,375:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:20,375:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:20,378:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,391:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,404:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,534:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,634:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,636:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:20,636:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:20,638:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 13:26:20,650:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,668:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,814:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,895:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,895:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:20,896:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:20,906:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 13:26:20,914:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,001:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,071:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,073:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:21,073:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:21,074:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 13:26:21,086:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,177:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,265:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,267:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:21,267:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:21,289:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,527:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,696:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,698:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:21,702:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:21,710:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 13:26:21,886:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,977:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:21,978:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:21,978:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:22,163:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:22,244:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 13:26:22,245:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:22,245:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:22,246:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 13:26:22,442:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:22,537:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:22,545:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:22,676:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 13:26:22,843:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:22,844:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:22,845:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 13:26:23,201:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:23,201:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:23,422:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:23,423:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:23,426:INFO:Preparing preprocessing pipeline...
2022-10-21 13:26:23,427:INFO:Set up simple imputation.
2022-10-21 13:26:23,433:INFO:Set up encoding of ordinal features.
2022-10-21 13:26:23,440:INFO:Set up encoding of categorical features.
2022-10-21 13:26:23,440:INFO:Set up polynomial features.
2022-10-21 13:26:23,441:INFO:Set up variance threshold.
2022-10-21 13:26:23,441:INFO:Set up feature normalization.
2022-10-21 13:26:23,563:INFO:Finished creating preprocessing pipeline.
2022-10-21 13:26:23,590:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2022-10-21 13:26:23,590:INFO:Creating final display dataframe.
2022-10-21 13:26:24,086:INFO:Setup display_container:                  Description             Value
0                 Session id               122
1                     Target           charges
2                Target type        Regression
3                 Data shape        (1338, 49)
4           Train data shape         (936, 49)
5            Test data shape         (402, 49)
6           Ordinal features                 2
7           Numeric features                 3
8       Categorical features                 3
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15       Polynomial features              True
16         Polynomial degree                 2
17    Low variance threshold                 0
18                 Normalize              True
19          Normalize method            zscore
20            Fold Generator             KFold
21               Fold Number                10
22                  CPU Jobs                -1
23                   Use GPU             False
24            Log Experiment             False
25           Experiment Name  reg-default-name
26                       USI              d49b
2022-10-21 13:26:24,359:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:24,359:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:24,533:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 13:26:24,534:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 13:26:24,547:INFO:setup() successfully completed in 4.79s...............
2022-10-21 13:26:24,547:INFO:Initializing compare_models()
2022-10-21 13:26:24,548:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-21 13:26:24,548:INFO:Checking exceptions
2022-10-21 13:26:24,550:INFO:Preparing display monitor
2022-10-21 13:26:24,786:INFO:Initializing Linear Regression
2022-10-21 13:26:24,788:INFO:Total runtime is 4.89195187886556e-05 minutes
2022-10-21 13:26:24,801:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:24,802:INFO:Initializing create_model()
2022-10-21 13:26:24,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:24,803:INFO:Checking exceptions
2022-10-21 13:26:24,811:INFO:Importing libraries
2022-10-21 13:26:24,811:INFO:Copying training dataset
2022-10-21 13:26:24,820:INFO:Defining folds
2022-10-21 13:26:24,821:INFO:Declaring metric variables
2022-10-21 13:26:24,877:INFO:Importing untrained model
2022-10-21 13:26:24,891:INFO:Linear Regression Imported successfully
2022-10-21 13:26:24,913:INFO:Starting cross validation
2022-10-21 13:26:24,932:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:40,972:INFO:Calculating mean and std
2022-10-21 13:26:40,978:INFO:Creating metrics dataframe
2022-10-21 13:26:40,989:INFO:Uploading results into container
2022-10-21 13:26:40,992:INFO:Uploading model into container now
2022-10-21 13:26:40,993:INFO:master_model_container: 1
2022-10-21 13:26:40,994:INFO:display_container: 2
2022-10-21 13:26:40,995:INFO:LinearRegression(n_jobs=-1)
2022-10-21 13:26:40,995:INFO:create_model() successfully completed......................................
2022-10-21 13:26:41,461:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:41,461:INFO:Creating metrics dataframe
2022-10-21 13:26:41,488:INFO:Initializing Lasso Regression
2022-10-21 13:26:41,488:INFO:Total runtime is 0.2783764322598775 minutes
2022-10-21 13:26:41,497:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:41,498:INFO:Initializing create_model()
2022-10-21 13:26:41,500:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:41,500:INFO:Checking exceptions
2022-10-21 13:26:41,503:INFO:Importing libraries
2022-10-21 13:26:41,504:INFO:Copying training dataset
2022-10-21 13:26:41,524:INFO:Defining folds
2022-10-21 13:26:41,524:INFO:Declaring metric variables
2022-10-21 13:26:41,557:INFO:Importing untrained model
2022-10-21 13:26:41,602:INFO:Lasso Regression Imported successfully
2022-10-21 13:26:41,643:INFO:Starting cross validation
2022-10-21 13:26:41,648:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:42,074:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+09, tolerance: 1.200e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,075:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.872e+09, tolerance: 1.200e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,076:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+09, tolerance: 1.201e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,076:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.938e+09, tolerance: 1.211e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,512:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+09, tolerance: 1.242e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,514:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.622e+09, tolerance: 1.201e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,520:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.374e+09, tolerance: 1.231e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.006e+09, tolerance: 1.207e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,747:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.154e+09, tolerance: 1.240e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,753:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+09, tolerance: 1.261e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 13:26:42,813:INFO:Calculating mean and std
2022-10-21 13:26:42,815:INFO:Creating metrics dataframe
2022-10-21 13:26:42,820:INFO:Uploading results into container
2022-10-21 13:26:42,821:INFO:Uploading model into container now
2022-10-21 13:26:42,823:INFO:master_model_container: 2
2022-10-21 13:26:42,823:INFO:display_container: 2
2022-10-21 13:26:42,824:INFO:Lasso(random_state=122)
2022-10-21 13:26:42,825:INFO:create_model() successfully completed......................................
2022-10-21 13:26:42,972:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:42,972:INFO:Creating metrics dataframe
2022-10-21 13:26:42,989:INFO:Initializing Ridge Regression
2022-10-21 13:26:42,989:INFO:Total runtime is 0.3033881346384684 minutes
2022-10-21 13:26:42,996:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:42,996:INFO:Initializing create_model()
2022-10-21 13:26:42,997:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:42,997:INFO:Checking exceptions
2022-10-21 13:26:43,001:INFO:Importing libraries
2022-10-21 13:26:43,001:INFO:Copying training dataset
2022-10-21 13:26:43,009:INFO:Defining folds
2022-10-21 13:26:43,009:INFO:Declaring metric variables
2022-10-21 13:26:43,040:INFO:Importing untrained model
2022-10-21 13:26:43,052:INFO:Ridge Regression Imported successfully
2022-10-21 13:26:43,115:INFO:Starting cross validation
2022-10-21 13:26:43,119:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:43,900:INFO:Calculating mean and std
2022-10-21 13:26:43,903:INFO:Creating metrics dataframe
2022-10-21 13:26:43,909:INFO:Uploading results into container
2022-10-21 13:26:43,910:INFO:Uploading model into container now
2022-10-21 13:26:43,911:INFO:master_model_container: 3
2022-10-21 13:26:43,911:INFO:display_container: 2
2022-10-21 13:26:43,912:INFO:Ridge(random_state=122)
2022-10-21 13:26:43,912:INFO:create_model() successfully completed......................................
2022-10-21 13:26:44,056:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:44,057:INFO:Creating metrics dataframe
2022-10-21 13:26:44,075:INFO:Initializing Elastic Net
2022-10-21 13:26:44,075:INFO:Total runtime is 0.3214994470278422 minutes
2022-10-21 13:26:44,081:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:44,082:INFO:Initializing create_model()
2022-10-21 13:26:44,082:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:44,082:INFO:Checking exceptions
2022-10-21 13:26:44,086:INFO:Importing libraries
2022-10-21 13:26:44,086:INFO:Copying training dataset
2022-10-21 13:26:44,111:INFO:Defining folds
2022-10-21 13:26:44,111:INFO:Declaring metric variables
2022-10-21 13:26:44,126:INFO:Importing untrained model
2022-10-21 13:26:44,135:INFO:Elastic Net Imported successfully
2022-10-21 13:26:44,153:INFO:Starting cross validation
2022-10-21 13:26:44,158:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:45,127:INFO:Calculating mean and std
2022-10-21 13:26:45,130:INFO:Creating metrics dataframe
2022-10-21 13:26:45,135:INFO:Uploading results into container
2022-10-21 13:26:45,135:INFO:Uploading model into container now
2022-10-21 13:26:45,136:INFO:master_model_container: 4
2022-10-21 13:26:45,137:INFO:display_container: 2
2022-10-21 13:26:45,138:INFO:ElasticNet(random_state=122)
2022-10-21 13:26:45,138:INFO:create_model() successfully completed......................................
2022-10-21 13:26:45,314:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:45,314:INFO:Creating metrics dataframe
2022-10-21 13:26:45,349:INFO:Initializing Least Angle Regression
2022-10-21 13:26:45,349:INFO:Total runtime is 0.3427326520284017 minutes
2022-10-21 13:26:45,360:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:45,361:INFO:Initializing create_model()
2022-10-21 13:26:45,361:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:45,361:INFO:Checking exceptions
2022-10-21 13:26:45,366:INFO:Importing libraries
2022-10-21 13:26:45,367:INFO:Copying training dataset
2022-10-21 13:26:45,599:INFO:Defining folds
2022-10-21 13:26:45,602:INFO:Declaring metric variables
2022-10-21 13:26:45,616:INFO:Importing untrained model
2022-10-21 13:26:45,630:INFO:Least Angle Regression Imported successfully
2022-10-21 13:26:45,651:INFO:Starting cross validation
2022-10-21 13:26:45,656:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:46,069:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,098:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.030e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,098:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.030e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.972e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,102:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.748e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,127:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.562e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,127:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.508e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,128:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.147e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,165:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,185:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.009e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,189:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.614e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,191:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.041e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,198:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,209:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.471e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,219:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.036e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,221:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.656e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,224:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.656e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,224:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.125e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,225:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.947e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,226:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.947e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,230:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.850e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,231:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.212e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,231:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.479e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,232:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.145e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,232:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.434e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,232:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.327e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,232:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.931e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,234:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.185e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,234:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.174e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,234:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.043e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 9.365e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,235:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.077e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,237:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.082e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,237:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.068e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,242:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.773e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,243:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.360e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,244:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.311e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,245:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.093e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,238:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.860e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,246:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.826e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,246:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.786e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,247:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.823e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,247:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.624e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,247:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.813e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,247:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.369e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,248:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.610e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.365e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,248:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.131e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,248:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.378e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,248:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.036e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,248:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.010e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,249:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.490e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,249:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.645e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,249:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.818e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,250:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.810e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,251:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.464e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,254:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.704e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,254:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.085e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,254:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.234e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,254:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.432e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,256:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.374e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,257:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.372e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,258:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.365e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,259:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.855e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,263:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.240e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,263:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.129e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,264:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.914e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,265:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.063e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,265:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.961e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,266:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.731e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,268:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.191e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,269:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.070e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,270:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.048e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,270:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.025e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.882e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.916e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.830e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,302:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.804e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,302:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.777e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,302:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.767e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,303:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.280e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,303:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.122e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,303:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.066e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,303:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.226e-05, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,312:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.016e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,312:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.685e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,316:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=6.466e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,316:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.238e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,317:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.047e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,318:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.803e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,318:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.358e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,318:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.797e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,326:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,347:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.293e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,349:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.410e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,357:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.588e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,360:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.240e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,364:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.027e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,365:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.613e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,366:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.457e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,379:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.230e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,379:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.201e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,380:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.097e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,381:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.900e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,382:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=9.487e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,382:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.190e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,383:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.083e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,384:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.947e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,385:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.916e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,385:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.242e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,402:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.885e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,402:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.199e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,403:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.452e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,477:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.315e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,478:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.193e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,478:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.107e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,479:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.009e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,479:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=9.216e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,480:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=7.358e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,480:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.358e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,481:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.531e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,481:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.110e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,481:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.848e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,482:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.691e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,482:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.652e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,483:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.078e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,489:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.859e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,490:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.687e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,490:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.232e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,490:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.232e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,764:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,774:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,776:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,780:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.266e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,782:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.858e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,782:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.143e+01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,785:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.049e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,786:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.225e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,787:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.988e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,791:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.607e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,791:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.607e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,792:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.106e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,793:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.106e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,795:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.211e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,798:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.655e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,798:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.345e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,799:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.280e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,799:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.193e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,800:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.008e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,800:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.855e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,800:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.862e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.806e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.526e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,802:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.156e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,802:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.473e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,802:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.326e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,802:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.145e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,802:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.979e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,803:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=9.114e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,803:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.846e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,804:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.432e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.170e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,808:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.130e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,808:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.100e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,809:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=6.807e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,809:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.825e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.330e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,819:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.183e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.388e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,824:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.163e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,841:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.855e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,844:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.940e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,845:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.571e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,845:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.398e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,846:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.908e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,847:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.908e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,847:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.091e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,848:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.665e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.175e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.478e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.199e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.169e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,851:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.714e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,851:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.624e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,852:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.577e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,852:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.850e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,853:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.139e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,882:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.566e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,882:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.811e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,882:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.557e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,883:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.643e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,883:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.600e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,883:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.618e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,883:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.379e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,883:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.436e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,883:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.507e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,884:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.192e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,884:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.282e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,884:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.490e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,884:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.199e-02, with an active set of 40 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,884:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.572e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,885:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.486e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,885:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.475e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,886:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.444e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,890:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.426e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,891:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.043e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,897:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.472e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,898:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.047e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,898:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.187e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,899:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.933e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,899:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.931e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,899:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.014e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,914:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.152e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,914:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.613e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,915:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.138e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,920:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.482e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,920:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.696e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,923:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.981e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,923:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.736e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,930:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.148e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,986:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:46,994:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.570e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,996:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.730e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,998:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.191e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:46,999:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.055e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,004:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.438e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,004:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.428e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,006:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.427e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,006:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.405e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,006:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.145e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,010:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.599e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,010:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.575e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,011:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.996e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,012:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.038e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,012:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.957e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,013:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.007e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,013:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.974e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,013:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.490e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,013:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.958e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,014:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.886e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,014:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.022e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,014:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.015e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,015:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.754e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,016:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.214e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,016:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.077e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,016:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.533e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,017:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.494e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,017:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.387e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,018:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.365e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,019:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.274e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,019:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.489e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,020:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.993e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,020:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.922e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,192:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:47,197:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:47,199:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.852e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,203:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.666e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,204:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.666e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,206:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.249e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,208:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.507e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,209:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.717e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,209:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.136e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,210:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.918e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,211:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.357e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,211:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.027e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,212:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.078e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,212:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.078e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,212:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.720e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,212:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.436e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,213:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.436e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,213:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.371e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,214:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.931e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,214:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.508e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,215:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.675e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,215:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.432e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,215:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.653e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,215:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.348e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,215:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.115e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,216:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.897e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,216:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.610e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,216:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.416e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,216:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.639e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,217:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.620e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,217:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.999e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,217:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.544e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,218:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.184e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,218:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.210e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,219:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.716e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,219:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.080e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,219:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.320e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,219:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.035e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,220:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.522e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,220:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,220:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.059e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,222:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,223:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.020e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,223:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,224:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.713e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,224:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.822e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,224:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.644e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,224:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.374e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,224:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.563e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,225:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.397e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,225:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.345e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,226:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.674e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,226:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.415e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,226:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.664e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,227:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.223e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,230:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.293e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,230:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.130e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,230:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.271e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,231:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.176e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,231:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.063e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,232:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.060e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,232:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.133e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,233:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.256e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,233:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.987e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,234:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.502e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,345:INFO:Calculating mean and std
2022-10-21 13:26:47,349:INFO:Creating metrics dataframe
2022-10-21 13:26:47,356:INFO:Uploading results into container
2022-10-21 13:26:47,358:INFO:Uploading model into container now
2022-10-21 13:26:47,359:INFO:master_model_container: 5
2022-10-21 13:26:47,359:INFO:display_container: 2
2022-10-21 13:26:47,360:INFO:Lars(random_state=122)
2022-10-21 13:26:47,361:INFO:create_model() successfully completed......................................
2022-10-21 13:26:47,552:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:47,552:INFO:Creating metrics dataframe
2022-10-21 13:26:47,574:INFO:Initializing Lasso Least Angle Regression
2022-10-21 13:26:47,575:INFO:Total runtime is 0.37981905142466227 minutes
2022-10-21 13:26:47,580:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:47,581:INFO:Initializing create_model()
2022-10-21 13:26:47,581:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:47,581:INFO:Checking exceptions
2022-10-21 13:26:47,585:INFO:Importing libraries
2022-10-21 13:26:47,585:INFO:Copying training dataset
2022-10-21 13:26:47,620:INFO:Defining folds
2022-10-21 13:26:47,621:INFO:Declaring metric variables
2022-10-21 13:26:47,646:INFO:Importing untrained model
2022-10-21 13:26:47,681:INFO:Lasso Least Angle Regression Imported successfully
2022-10-21 13:26:47,738:INFO:Starting cross validation
2022-10-21 13:26:47,746:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:47,951:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:47,967:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.688e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,969:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.731e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,969:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.731e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,973:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.941e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,974:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.262e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,975:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.216e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,976:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:47,985:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:47,985:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.293e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,986:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.410e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,990:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:47,991:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.830e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,996:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.471e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:47,999:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.355e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,000:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.043e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,001:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.574e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,002:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.125e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,004:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.206e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,006:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.206e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,007:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.214e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,340:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:48,347:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.980e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,348:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.769e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,348:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.073e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,350:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.836e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,353:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.044e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,358:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.647e+00, previous alpha=1.480e+00, with an active set of 24 regressors.
  warnings.warn(

2022-10-21 13:26:48,364:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:48,378:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.225e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,379:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:48,380:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.193e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,380:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:48,381:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.862e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,382:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.163e+00, previous alpha=4.862e+00, with an active set of 15 regressors.
  warnings.warn(

2022-10-21 13:26:48,387:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.570e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,393:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.992e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,393:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.992e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,394:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=2.933e+00, previous alpha=2.630e+00, with an active set of 18 regressors.
  warnings.warn(

2022-10-21 13:26:48,395:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.348e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,396:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.348e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,397:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.323e+00, previous alpha=1.117e+00, with an active set of 27 regressors.
  warnings.warn(

2022-10-21 13:26:48,599:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:48,607:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.843e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,608:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.018e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,616:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.717e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,616:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 13:26:48,617:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.331e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,618:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.479e+00, previous alpha=1.249e+00, with an active set of 27 regressors.
  warnings.warn(

2022-10-21 13:26:48,625:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.759e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,627:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.371e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,630:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.529e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 13:26:48,631:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.413e+00, previous alpha=1.396e+00, with an active set of 24 regressors.
  warnings.warn(

2022-10-21 13:26:48,716:INFO:Calculating mean and std
2022-10-21 13:26:48,721:INFO:Creating metrics dataframe
2022-10-21 13:26:48,729:INFO:Uploading results into container
2022-10-21 13:26:48,732:INFO:Uploading model into container now
2022-10-21 13:26:48,733:INFO:master_model_container: 6
2022-10-21 13:26:48,733:INFO:display_container: 2
2022-10-21 13:26:48,734:INFO:LassoLars(random_state=122)
2022-10-21 13:26:48,734:INFO:create_model() successfully completed......................................
2022-10-21 13:26:48,952:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:48,953:INFO:Creating metrics dataframe
2022-10-21 13:26:48,992:INFO:Initializing Orthogonal Matching Pursuit
2022-10-21 13:26:48,992:INFO:Total runtime is 0.40344908634821575 minutes
2022-10-21 13:26:49,000:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:49,001:INFO:Initializing create_model()
2022-10-21 13:26:49,001:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:49,002:INFO:Checking exceptions
2022-10-21 13:26:49,040:INFO:Importing libraries
2022-10-21 13:26:49,040:INFO:Copying training dataset
2022-10-21 13:26:49,065:INFO:Defining folds
2022-10-21 13:26:49,066:INFO:Declaring metric variables
2022-10-21 13:26:49,098:INFO:Importing untrained model
2022-10-21 13:26:49,131:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-21 13:26:49,181:INFO:Starting cross validation
2022-10-21 13:26:49,188:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:49,365:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,386:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,404:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,407:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,635:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,650:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,665:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,674:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,808:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,819:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 13:26:49,866:INFO:Calculating mean and std
2022-10-21 13:26:49,870:INFO:Creating metrics dataframe
2022-10-21 13:26:49,876:INFO:Uploading results into container
2022-10-21 13:26:49,877:INFO:Uploading model into container now
2022-10-21 13:26:49,877:INFO:master_model_container: 7
2022-10-21 13:26:49,877:INFO:display_container: 2
2022-10-21 13:26:49,877:INFO:OrthogonalMatchingPursuit()
2022-10-21 13:26:49,878:INFO:create_model() successfully completed......................................
2022-10-21 13:26:50,009:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:50,010:INFO:Creating metrics dataframe
2022-10-21 13:26:50,030:INFO:Initializing Bayesian Ridge
2022-10-21 13:26:50,030:INFO:Total runtime is 0.42074170112609866 minutes
2022-10-21 13:26:50,037:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:50,038:INFO:Initializing create_model()
2022-10-21 13:26:50,038:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:50,038:INFO:Checking exceptions
2022-10-21 13:26:50,043:INFO:Importing libraries
2022-10-21 13:26:50,043:INFO:Copying training dataset
2022-10-21 13:26:50,052:INFO:Defining folds
2022-10-21 13:26:50,052:INFO:Declaring metric variables
2022-10-21 13:26:50,082:INFO:Importing untrained model
2022-10-21 13:26:50,108:INFO:Bayesian Ridge Imported successfully
2022-10-21 13:26:50,146:INFO:Starting cross validation
2022-10-21 13:26:50,149:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:50,870:INFO:Calculating mean and std
2022-10-21 13:26:50,873:INFO:Creating metrics dataframe
2022-10-21 13:26:50,878:INFO:Uploading results into container
2022-10-21 13:26:50,879:INFO:Uploading model into container now
2022-10-21 13:26:50,879:INFO:master_model_container: 8
2022-10-21 13:26:50,880:INFO:display_container: 2
2022-10-21 13:26:50,880:INFO:BayesianRidge()
2022-10-21 13:26:50,880:INFO:create_model() successfully completed......................................
2022-10-21 13:26:51,023:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:51,024:INFO:Creating metrics dataframe
2022-10-21 13:26:51,040:INFO:Initializing Passive Aggressive Regressor
2022-10-21 13:26:51,041:INFO:Total runtime is 0.43758473396301273 minutes
2022-10-21 13:26:51,049:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:51,049:INFO:Initializing create_model()
2022-10-21 13:26:51,049:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:51,049:INFO:Checking exceptions
2022-10-21 13:26:51,053:INFO:Importing libraries
2022-10-21 13:26:51,053:INFO:Copying training dataset
2022-10-21 13:26:51,074:INFO:Defining folds
2022-10-21 13:26:51,074:INFO:Declaring metric variables
2022-10-21 13:26:51,111:INFO:Importing untrained model
2022-10-21 13:26:51,132:INFO:Passive Aggressive Regressor Imported successfully
2022-10-21 13:26:51,199:INFO:Starting cross validation
2022-10-21 13:26:51,241:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:52,368:INFO:Calculating mean and std
2022-10-21 13:26:52,374:INFO:Creating metrics dataframe
2022-10-21 13:26:52,380:INFO:Uploading results into container
2022-10-21 13:26:52,381:INFO:Uploading model into container now
2022-10-21 13:26:52,381:INFO:master_model_container: 9
2022-10-21 13:26:52,382:INFO:display_container: 2
2022-10-21 13:26:52,382:INFO:PassiveAggressiveRegressor(random_state=122)
2022-10-21 13:26:52,383:INFO:create_model() successfully completed......................................
2022-10-21 13:26:52,527:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:52,527:INFO:Creating metrics dataframe
2022-10-21 13:26:52,544:INFO:Initializing Huber Regressor
2022-10-21 13:26:52,544:INFO:Total runtime is 0.46263968547185264 minutes
2022-10-21 13:26:52,550:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:52,551:INFO:Initializing create_model()
2022-10-21 13:26:52,551:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:52,551:INFO:Checking exceptions
2022-10-21 13:26:52,556:INFO:Importing libraries
2022-10-21 13:26:52,557:INFO:Copying training dataset
2022-10-21 13:26:52,564:INFO:Defining folds
2022-10-21 13:26:52,564:INFO:Declaring metric variables
2022-10-21 13:26:52,595:INFO:Importing untrained model
2022-10-21 13:26:52,643:INFO:Huber Regressor Imported successfully
2022-10-21 13:26:52,669:INFO:Starting cross validation
2022-10-21 13:26:52,677:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:53,094:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,097:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,110:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,122:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,580:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,603:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,653:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,752:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,976:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:53,985:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 13:26:54,031:INFO:Calculating mean and std
2022-10-21 13:26:54,033:INFO:Creating metrics dataframe
2022-10-21 13:26:54,039:INFO:Uploading results into container
2022-10-21 13:26:54,041:INFO:Uploading model into container now
2022-10-21 13:26:54,042:INFO:master_model_container: 10
2022-10-21 13:26:54,042:INFO:display_container: 2
2022-10-21 13:26:54,043:INFO:HuberRegressor()
2022-10-21 13:26:54,043:INFO:create_model() successfully completed......................................
2022-10-21 13:26:54,179:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:54,179:INFO:Creating metrics dataframe
2022-10-21 13:26:54,196:INFO:Initializing K Neighbors Regressor
2022-10-21 13:26:54,197:INFO:Total runtime is 0.4901867667833964 minutes
2022-10-21 13:26:54,203:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:54,204:INFO:Initializing create_model()
2022-10-21 13:26:54,204:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:54,204:INFO:Checking exceptions
2022-10-21 13:26:54,209:INFO:Importing libraries
2022-10-21 13:26:54,209:INFO:Copying training dataset
2022-10-21 13:26:54,218:INFO:Defining folds
2022-10-21 13:26:54,218:INFO:Declaring metric variables
2022-10-21 13:26:54,246:INFO:Importing untrained model
2022-10-21 13:26:54,278:INFO:K Neighbors Regressor Imported successfully
2022-10-21 13:26:54,296:INFO:Starting cross validation
2022-10-21 13:26:54,300:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:55,036:INFO:Calculating mean and std
2022-10-21 13:26:55,040:INFO:Creating metrics dataframe
2022-10-21 13:26:55,045:INFO:Uploading results into container
2022-10-21 13:26:55,046:INFO:Uploading model into container now
2022-10-21 13:26:55,047:INFO:master_model_container: 11
2022-10-21 13:26:55,047:INFO:display_container: 2
2022-10-21 13:26:55,047:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-21 13:26:55,048:INFO:create_model() successfully completed......................................
2022-10-21 13:26:55,204:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:55,204:INFO:Creating metrics dataframe
2022-10-21 13:26:55,278:INFO:Initializing Decision Tree Regressor
2022-10-21 13:26:55,279:INFO:Total runtime is 0.5082203030586243 minutes
2022-10-21 13:26:55,294:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:55,295:INFO:Initializing create_model()
2022-10-21 13:26:55,296:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:55,297:INFO:Checking exceptions
2022-10-21 13:26:55,301:INFO:Importing libraries
2022-10-21 13:26:55,303:INFO:Copying training dataset
2022-10-21 13:26:55,370:INFO:Defining folds
2022-10-21 13:26:55,370:INFO:Declaring metric variables
2022-10-21 13:26:55,381:INFO:Importing untrained model
2022-10-21 13:26:55,415:INFO:Decision Tree Regressor Imported successfully
2022-10-21 13:26:55,442:INFO:Starting cross validation
2022-10-21 13:26:55,446:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:26:56,473:INFO:Calculating mean and std
2022-10-21 13:26:56,475:INFO:Creating metrics dataframe
2022-10-21 13:26:56,479:INFO:Uploading results into container
2022-10-21 13:26:56,480:INFO:Uploading model into container now
2022-10-21 13:26:56,481:INFO:master_model_container: 12
2022-10-21 13:26:56,481:INFO:display_container: 2
2022-10-21 13:26:56,481:INFO:DecisionTreeRegressor(random_state=122)
2022-10-21 13:26:56,482:INFO:create_model() successfully completed......................................
2022-10-21 13:26:56,616:INFO:SubProcess create_model() end ==================================
2022-10-21 13:26:56,616:INFO:Creating metrics dataframe
2022-10-21 13:26:56,638:INFO:Initializing Random Forest Regressor
2022-10-21 13:26:56,638:INFO:Total runtime is 0.530874780813853 minutes
2022-10-21 13:26:56,646:INFO:SubProcess create_model() called ==================================
2022-10-21 13:26:56,647:INFO:Initializing create_model()
2022-10-21 13:26:56,647:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:26:56,647:INFO:Checking exceptions
2022-10-21 13:26:56,650:INFO:Importing libraries
2022-10-21 13:26:56,651:INFO:Copying training dataset
2022-10-21 13:26:56,660:INFO:Defining folds
2022-10-21 13:26:56,660:INFO:Declaring metric variables
2022-10-21 13:26:56,670:INFO:Importing untrained model
2022-10-21 13:26:56,679:INFO:Random Forest Regressor Imported successfully
2022-10-21 13:26:56,695:INFO:Starting cross validation
2022-10-21 13:26:56,697:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:27:01,792:INFO:Calculating mean and std
2022-10-21 13:27:01,794:INFO:Creating metrics dataframe
2022-10-21 13:27:01,798:INFO:Uploading results into container
2022-10-21 13:27:01,799:INFO:Uploading model into container now
2022-10-21 13:27:01,800:INFO:master_model_container: 13
2022-10-21 13:27:01,800:INFO:display_container: 2
2022-10-21 13:27:01,801:INFO:RandomForestRegressor(n_jobs=-1, random_state=122)
2022-10-21 13:27:01,801:INFO:create_model() successfully completed......................................
2022-10-21 13:27:01,946:INFO:SubProcess create_model() end ==================================
2022-10-21 13:27:01,946:INFO:Creating metrics dataframe
2022-10-21 13:27:01,963:INFO:Initializing Extra Trees Regressor
2022-10-21 13:27:01,964:INFO:Total runtime is 0.6196384191513062 minutes
2022-10-21 13:27:01,968:INFO:SubProcess create_model() called ==================================
2022-10-21 13:27:01,969:INFO:Initializing create_model()
2022-10-21 13:27:01,969:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:27:01,969:INFO:Checking exceptions
2022-10-21 13:27:01,973:INFO:Importing libraries
2022-10-21 13:27:01,974:INFO:Copying training dataset
2022-10-21 13:27:01,981:INFO:Defining folds
2022-10-21 13:27:01,982:INFO:Declaring metric variables
2022-10-21 13:27:02,018:INFO:Importing untrained model
2022-10-21 13:27:02,043:INFO:Extra Trees Regressor Imported successfully
2022-10-21 13:27:02,108:INFO:Starting cross validation
2022-10-21 13:27:02,114:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:27:05,907:INFO:Calculating mean and std
2022-10-21 13:27:05,909:INFO:Creating metrics dataframe
2022-10-21 13:27:05,914:INFO:Uploading results into container
2022-10-21 13:27:05,915:INFO:Uploading model into container now
2022-10-21 13:27:05,916:INFO:master_model_container: 14
2022-10-21 13:27:05,916:INFO:display_container: 2
2022-10-21 13:27:05,916:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=122)
2022-10-21 13:27:05,917:INFO:create_model() successfully completed......................................
2022-10-21 13:27:06,065:INFO:SubProcess create_model() end ==================================
2022-10-21 13:27:06,065:INFO:Creating metrics dataframe
2022-10-21 13:27:06,084:INFO:Initializing AdaBoost Regressor
2022-10-21 13:27:06,085:INFO:Total runtime is 0.6883297840754192 minutes
2022-10-21 13:27:06,096:INFO:SubProcess create_model() called ==================================
2022-10-21 13:27:06,097:INFO:Initializing create_model()
2022-10-21 13:27:06,097:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:27:06,097:INFO:Checking exceptions
2022-10-21 13:27:06,100:INFO:Importing libraries
2022-10-21 13:27:06,101:INFO:Copying training dataset
2022-10-21 13:27:06,114:INFO:Defining folds
2022-10-21 13:27:06,114:INFO:Declaring metric variables
2022-10-21 13:27:06,154:INFO:Importing untrained model
2022-10-21 13:27:06,171:INFO:AdaBoost Regressor Imported successfully
2022-10-21 13:27:06,210:INFO:Starting cross validation
2022-10-21 13:27:06,215:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:27:07,387:INFO:Calculating mean and std
2022-10-21 13:27:07,391:INFO:Creating metrics dataframe
2022-10-21 13:27:07,396:INFO:Uploading results into container
2022-10-21 13:27:07,397:INFO:Uploading model into container now
2022-10-21 13:27:07,397:INFO:master_model_container: 15
2022-10-21 13:27:07,398:INFO:display_container: 2
2022-10-21 13:27:07,398:INFO:AdaBoostRegressor(random_state=122)
2022-10-21 13:27:07,399:INFO:create_model() successfully completed......................................
2022-10-21 13:27:07,541:INFO:SubProcess create_model() end ==================================
2022-10-21 13:27:07,541:INFO:Creating metrics dataframe
2022-10-21 13:27:07,565:INFO:Initializing Gradient Boosting Regressor
2022-10-21 13:27:07,565:INFO:Total runtime is 0.7129913806915285 minutes
2022-10-21 13:27:07,575:INFO:SubProcess create_model() called ==================================
2022-10-21 13:27:07,575:INFO:Initializing create_model()
2022-10-21 13:27:07,575:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:27:07,575:INFO:Checking exceptions
2022-10-21 13:27:07,578:INFO:Importing libraries
2022-10-21 13:27:07,578:INFO:Copying training dataset
2022-10-21 13:27:07,591:INFO:Defining folds
2022-10-21 13:27:07,593:INFO:Declaring metric variables
2022-10-21 13:27:07,606:INFO:Importing untrained model
2022-10-21 13:27:07,623:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 13:27:07,647:INFO:Starting cross validation
2022-10-21 13:27:07,651:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:27:09,905:INFO:Calculating mean and std
2022-10-21 13:27:09,908:INFO:Creating metrics dataframe
2022-10-21 13:27:09,913:INFO:Uploading results into container
2022-10-21 13:27:09,913:INFO:Uploading model into container now
2022-10-21 13:27:09,914:INFO:master_model_container: 16
2022-10-21 13:27:09,914:INFO:display_container: 2
2022-10-21 13:27:09,915:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 13:27:09,915:INFO:create_model() successfully completed......................................
2022-10-21 13:27:10,052:INFO:SubProcess create_model() end ==================================
2022-10-21 13:27:10,053:INFO:Creating metrics dataframe
2022-10-21 13:27:10,081:INFO:Initializing Light Gradient Boosting Machine
2022-10-21 13:27:10,081:INFO:Total runtime is 0.7549317836761475 minutes
2022-10-21 13:27:10,086:INFO:SubProcess create_model() called ==================================
2022-10-21 13:27:10,088:INFO:Initializing create_model()
2022-10-21 13:27:10,092:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:27:10,092:INFO:Checking exceptions
2022-10-21 13:27:10,095:INFO:Importing libraries
2022-10-21 13:27:10,096:INFO:Copying training dataset
2022-10-21 13:27:10,113:INFO:Defining folds
2022-10-21 13:27:10,113:INFO:Declaring metric variables
2022-10-21 13:27:10,148:INFO:Importing untrained model
2022-10-21 13:27:10,154:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-21 13:27:10,195:INFO:Starting cross validation
2022-10-21 13:27:10,216:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:27:13,819:INFO:Calculating mean and std
2022-10-21 13:27:13,825:INFO:Creating metrics dataframe
2022-10-21 13:27:13,829:INFO:Uploading results into container
2022-10-21 13:27:13,830:INFO:Uploading model into container now
2022-10-21 13:27:13,831:INFO:master_model_container: 17
2022-10-21 13:27:13,831:INFO:display_container: 2
2022-10-21 13:27:13,832:INFO:LGBMRegressor(random_state=122)
2022-10-21 13:27:13,832:INFO:create_model() successfully completed......................................
2022-10-21 13:27:13,965:INFO:SubProcess create_model() end ==================================
2022-10-21 13:27:13,966:INFO:Creating metrics dataframe
2022-10-21 13:27:13,984:INFO:Initializing CatBoost Regressor
2022-10-21 13:27:13,985:INFO:Total runtime is 0.8199852863947551 minutes
2022-10-21 13:27:13,992:INFO:SubProcess create_model() called ==================================
2022-10-21 13:27:13,993:INFO:Initializing create_model()
2022-10-21 13:27:13,993:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:27:13,994:INFO:Checking exceptions
2022-10-21 13:27:13,996:INFO:Importing libraries
2022-10-21 13:27:13,996:INFO:Copying training dataset
2022-10-21 13:27:14,006:INFO:Defining folds
2022-10-21 13:27:14,012:INFO:Declaring metric variables
2022-10-21 13:27:14,031:INFO:Importing untrained model
2022-10-21 13:27:14,064:INFO:CatBoost Regressor Imported successfully
2022-10-21 13:27:14,101:INFO:Starting cross validation
2022-10-21 13:27:14,104:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:27:41,298:INFO:Calculating mean and std
2022-10-21 13:27:41,302:INFO:Creating metrics dataframe
2022-10-21 13:27:41,309:INFO:Uploading results into container
2022-10-21 13:27:41,311:INFO:Uploading model into container now
2022-10-21 13:27:41,312:INFO:master_model_container: 18
2022-10-21 13:27:41,312:INFO:display_container: 2
2022-10-21 13:27:41,312:INFO:<catboost.core.CatBoostRegressor object at 0x7f91c59e96a0>
2022-10-21 13:27:41,312:INFO:create_model() successfully completed......................................
2022-10-21 13:27:41,514:INFO:SubProcess create_model() end ==================================
2022-10-21 13:27:41,515:INFO:Creating metrics dataframe
2022-10-21 13:27:41,538:INFO:Initializing Dummy Regressor
2022-10-21 13:27:41,538:INFO:Total runtime is 1.2792093833287557 minutes
2022-10-21 13:27:41,554:INFO:SubProcess create_model() called ==================================
2022-10-21 13:27:41,554:INFO:Initializing create_model()
2022-10-21 13:27:41,554:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f91c6c45f70>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:27:41,554:INFO:Checking exceptions
2022-10-21 13:27:41,562:INFO:Importing libraries
2022-10-21 13:27:41,562:INFO:Copying training dataset
2022-10-21 13:27:41,599:INFO:Defining folds
2022-10-21 13:27:41,600:INFO:Declaring metric variables
2022-10-21 13:27:41,627:INFO:Importing untrained model
2022-10-21 13:27:41,658:INFO:Dummy Regressor Imported successfully
2022-10-21 13:27:41,727:INFO:Starting cross validation
2022-10-21 13:27:41,732:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 13:27:42,825:INFO:Calculating mean and std
2022-10-21 13:27:42,829:INFO:Creating metrics dataframe
2022-10-21 13:27:42,834:INFO:Uploading results into container
2022-10-21 13:27:42,835:INFO:Uploading model into container now
2022-10-21 13:27:42,837:INFO:master_model_container: 19
2022-10-21 13:27:42,837:INFO:display_container: 2
2022-10-21 13:27:42,837:INFO:DummyRegressor()
2022-10-21 13:27:42,837:INFO:create_model() successfully completed......................................
2022-10-21 13:27:43,026:INFO:SubProcess create_model() end ==================================
2022-10-21 13:27:43,026:INFO:Creating metrics dataframe
2022-10-21 13:27:43,084:INFO:Initializing create_model()
2022-10-21 13:27:43,086:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=GradientBoostingRegressor(random_state=122), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 13:27:43,086:INFO:Checking exceptions
2022-10-21 13:27:43,097:INFO:Importing libraries
2022-10-21 13:27:43,097:INFO:Copying training dataset
2022-10-21 13:27:43,103:INFO:Defining folds
2022-10-21 13:27:43,103:INFO:Declaring metric variables
2022-10-21 13:27:43,104:INFO:Importing untrained model
2022-10-21 13:27:43,104:INFO:Declaring custom model
2022-10-21 13:27:43,105:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 13:27:43,109:INFO:Cross validation set to False
2022-10-21 13:27:43,109:INFO:Fitting Model
2022-10-21 13:27:43,689:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 13:27:43,689:INFO:create_model() successfully completed......................................
2022-10-21 13:27:44,001:INFO:master_model_container: 19
2022-10-21 13:27:44,001:INFO:display_container: 2
2022-10-21 13:27:44,002:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 13:27:44,002:INFO:compare_models() successfully completed......................................
2022-10-21 13:27:44,004:INFO:Initializing evaluate_model()
2022-10-21 13:27:44,004:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=GradientBoostingRegressor(random_state=122), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-21 13:27:44,087:INFO:Initializing plot_model()
2022-10-21 13:27:44,088:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, system=True)
2022-10-21 13:27:44,088:INFO:Checking exceptions
2022-10-21 13:27:44,099:INFO:Preloading libraries
2022-10-21 13:27:44,181:INFO:Copying training dataset
2022-10-21 13:27:44,181:INFO:Plot type: pipeline
2022-10-21 13:27:44,871:INFO:Visual Rendered Successfully
2022-10-21 13:27:45,022:INFO:plot_model() successfully completed......................................
2022-10-21 13:27:45,028:INFO:Initializing predict_model()
2022-10-21 13:27:45,028:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f91c6650790>)
2022-10-21 13:27:45,029:INFO:Checking exceptions
2022-10-21 13:27:45,029:INFO:Preloading libraries
2022-10-21 13:27:45,495:INFO:Initializing save_model()
2022-10-21 13:27:45,496:INFO:save_model(model=GradientBoostingRegressor(random_state=122), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 13:27:45,496:INFO:Adding model into prep_pipe
2022-10-21 13:27:45,583:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 13:27:45,661:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 13:27:45,662:INFO:save_model() successfully completed......................................
2022-10-21 13:29:47,186:INFO:Initializing plot_model()
2022-10-21 13:29:47,189:INFO:plot_model(plot=residuals, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f91dc7db340>, system=True)
2022-10-21 13:29:47,189:INFO:Checking exceptions
2022-10-21 13:29:47,226:INFO:Preloading libraries
2022-10-21 13:29:47,245:INFO:Copying training dataset
2022-10-21 13:29:47,245:INFO:Plot type: residuals
2022-10-21 13:29:47,674:INFO:Fitting Model
2022-10-21 13:29:47,727:INFO:Scoring test/hold-out set
2022-10-21 13:29:48,920:INFO:Visual Rendered Successfully
2022-10-21 13:29:49,292:INFO:plot_model() successfully completed......................................
2022-10-21 13:29:51,507:INFO:Initializing load_model()
2022-10-21 13:29:51,507:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 13:35:02,515:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:02,515:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:02,516:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:02,516:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:05,580:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:35:16,913:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:16,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:16,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:16,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:35:19,194:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:37:25,628:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:37:25,628:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:37:25,629:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:37:25,629:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:37:27,879:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:43:31,946:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:43:31,947:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:43:31,947:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:43:31,947:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:43:35,078:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:45:28,732:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:45:28,734:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:45:28,734:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:45:28,734:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:45:31,500:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:45:32,726:INFO:Initializing load_model()
2022-10-21 13:45:32,726:INFO:load_model(model_name=deployment_28042020, platform=None, authentication=None, verbose=True)
2022-10-21 13:46:38,313:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:46:38,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:46:38,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:46:38,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 13:46:41,259:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 13:46:42,536:INFO:Initializing load_model()
2022-10-21 13:46:42,536:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 13:51:27,287:INFO:Initializing predict_model()
2022-10-21 13:51:27,293:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fba017e7c70>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fba017ab1f0>)
2022-10-21 13:51:27,293:INFO:Checking exceptions
2022-10-21 13:51:27,293:INFO:Preloading libraries
2022-10-21 13:51:27,297:INFO:Set up data.
2022-10-21 13:51:27,377:INFO:Set up index.
2022-10-21 13:51:27,650:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 14:25:27,954:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:25:27,956:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:25:27,956:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:25:27,956:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:25:30,966:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 14:25:32,286:INFO:Initializing load_model()
2022-10-21 14:25:32,287:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 14:25:49,317:INFO:Initializing predict_model()
2022-10-21 14:25:49,318:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9c357e7c40>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f9c357a91f0>)
2022-10-21 14:25:49,318:INFO:Checking exceptions
2022-10-21 14:25:49,318:INFO:Preloading libraries
2022-10-21 14:25:49,319:INFO:Set up data.
2022-10-21 14:25:49,333:INFO:Set up index.
2022-10-21 14:25:49,585:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 14:36:22,584:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:36:22,586:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:36:22,586:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:36:22,586:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 14:36:26,577:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 14:37:34,760:INFO:PyCaret RegressionExperiment
2022-10-21 14:37:34,761:INFO:Logging name: reg-default-name
2022-10-21 14:37:34,761:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 14:37:34,761:INFO:version 3.0.0.rc4
2022-10-21 14:37:34,761:INFO:Initializing setup()
2022-10-21 14:37:34,762:INFO:self.USI: c95e
2022-10-21 14:37:34,762:INFO:self.variable_keys: {'_available_plots', '_all_metrics', '_all_models_internal', 'X_train', 'display_container', 'gpu_param', 'idx', 'transform_target_method_param', 'html_param', 'exp_id', 'exp_name_log', 'USI', 'log_plots_param', 'variable_keys', 'seed', 'n_jobs_param', 'y_test', 'y_train', '_all_models', 'transform_target_param', '_ml_usecase', 'logging_param', 'data', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'X', 'fold_generator', 'y', 'master_model_container', 'X_test', 'target_param', '_gpu_n_jobs_param', 'pipeline'}
2022-10-21 14:37:34,762:INFO:Checking environment
2022-10-21 14:37:34,762:INFO:python_version: 3.9.7
2022-10-21 14:37:34,762:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 14:37:34,762:INFO:machine: x86_64
2022-10-21 14:37:34,762:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 14:37:34,762:INFO:Memory: svmem(total=8589934592, available=2328457216, percent=72.9, used=4164542464, free=45465600, active=2284879872, inactive=2270371840, wired=1879662592)
2022-10-21 14:37:34,762:INFO:Physical Core: 2
2022-10-21 14:37:34,762:INFO:Logical Core: 4
2022-10-21 14:37:34,762:INFO:Checking libraries
2022-10-21 14:37:34,762:INFO:System:
2022-10-21 14:37:34,763:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 14:37:34,763:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 14:37:34,763:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 14:37:34,763:INFO:PyCaret required dependencies:
2022-10-21 14:37:34,763:INFO:                 pip: 21.2.4
2022-10-21 14:37:34,763:INFO:          setuptools: 58.0.4
2022-10-21 14:37:34,763:INFO:             pycaret: 3.0.0rc4
2022-10-21 14:37:34,763:INFO:             IPython: 7.29.0
2022-10-21 14:37:34,763:INFO:          ipywidgets: 7.6.5
2022-10-21 14:37:34,764:INFO:                tqdm: 4.62.3
2022-10-21 14:37:34,764:INFO:               numpy: 1.22.4
2022-10-21 14:37:34,764:INFO:              pandas: 1.4.4
2022-10-21 14:37:34,764:INFO:              jinja2: 3.1.2
2022-10-21 14:37:34,764:INFO:               scipy: 1.8.1
2022-10-21 14:37:34,764:INFO:              joblib: 1.1.0
2022-10-21 14:37:34,764:INFO:             sklearn: 1.0.2
2022-10-21 14:37:34,764:INFO:                pyod: 1.0.5
2022-10-21 14:37:34,764:INFO:            imblearn: 0.9.0
2022-10-21 14:37:34,764:INFO:   category_encoders: 2.5.1.post0
2022-10-21 14:37:34,765:INFO:            lightgbm: 3.3.2
2022-10-21 14:37:34,765:INFO:               numba: 0.55.2
2022-10-21 14:37:34,765:INFO:            requests: 2.28.1
2022-10-21 14:37:34,765:INFO:          matplotlib: 3.4.3
2022-10-21 14:37:34,765:INFO:          scikitplot: 0.3.7
2022-10-21 14:37:34,765:INFO:         yellowbrick: 1.4
2022-10-21 14:37:34,765:INFO:              plotly: 5.5.0
2022-10-21 14:37:34,765:INFO:             kaleido: 0.2.1
2022-10-21 14:37:34,765:INFO:         statsmodels: 0.13.2
2022-10-21 14:37:34,765:INFO:              sktime: 0.13.4
2022-10-21 14:37:34,765:INFO:               tbats: 1.1.1
2022-10-21 14:37:34,765:INFO:            pmdarima: 1.8.5
2022-10-21 14:37:34,765:INFO:              psutil: 5.9.2
2022-10-21 14:37:34,765:INFO:PyCaret optional dependencies:
2022-10-21 14:37:34,775:INFO:                shap: 0.41.0
2022-10-21 14:37:34,775:INFO:           interpret: Not installed
2022-10-21 14:37:34,775:INFO:                umap: Not installed
2022-10-21 14:37:34,776:INFO:    pandas_profiling: Not installed
2022-10-21 14:37:34,776:INFO:  explainerdashboard: Not installed
2022-10-21 14:37:34,776:INFO:             autoviz: Not installed
2022-10-21 14:37:34,776:INFO:           fairlearn: Not installed
2022-10-21 14:37:34,776:INFO:             xgboost: Not installed
2022-10-21 14:37:34,776:INFO:            catboost: 1.1
2022-10-21 14:37:34,776:INFO:              kmodes: Not installed
2022-10-21 14:37:34,777:INFO:             mlxtend: Not installed
2022-10-21 14:37:34,777:INFO:       statsforecast: 1.1.1
2022-10-21 14:37:34,777:INFO:        tune_sklearn: Not installed
2022-10-21 14:37:34,777:INFO:                 ray: Not installed
2022-10-21 14:37:34,777:INFO:            hyperopt: Not installed
2022-10-21 14:37:34,777:INFO:              optuna: Not installed
2022-10-21 14:37:34,777:INFO:               skopt: Not installed
2022-10-21 14:37:34,777:INFO:              mlflow: 1.29.0
2022-10-21 14:37:34,777:INFO:              gradio: Not installed
2022-10-21 14:37:34,777:INFO:             fastapi: Not installed
2022-10-21 14:37:34,777:INFO:             uvicorn: Not installed
2022-10-21 14:37:34,778:INFO:              m2cgen: Not installed
2022-10-21 14:37:34,778:INFO:           evidently: Not installed
2022-10-21 14:37:34,778:INFO:                nltk: 3.6.5
2022-10-21 14:37:34,778:INFO:            pyLDAvis: Not installed
2022-10-21 14:37:34,778:INFO:              gensim: Not installed
2022-10-21 14:37:34,778:INFO:               spacy: Not installed
2022-10-21 14:37:34,778:INFO:           wordcloud: Not installed
2022-10-21 14:37:34,778:INFO:            textblob: Not installed
2022-10-21 14:37:34,778:INFO:               fugue: Not installed
2022-10-21 14:37:34,778:INFO:           streamlit: Not installed
2022-10-21 14:37:34,778:INFO:             prophet: 1.1.1
2022-10-21 14:37:34,778:INFO:None
2022-10-21 14:37:34,778:INFO:Set up data.
2022-10-21 14:37:34,789:INFO:Set up train/test split.
2022-10-21 14:37:34,801:INFO:Set up index.
2022-10-21 14:37:34,802:INFO:Set up folding strategy.
2022-10-21 14:37:34,802:INFO:Assigning column types.
2022-10-21 14:37:34,809:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 14:37:34,809:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 14:37:34,820:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 14:37:34,833:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,034:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,141:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,144:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:35,144:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:35,278:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,291:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,300:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,426:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,541:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,541:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:35,542:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:35,543:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 14:37:35,556:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,580:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,676:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,747:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,748:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:35,748:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:35,759:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,769:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,868:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,975:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:35,976:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:35,976:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:35,978:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 14:37:35,996:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,115:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,202:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:36,202:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:36,219:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,314:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,388:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,390:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:36,391:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:36,391:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 14:37:36,496:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,570:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,571:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:36,571:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:36,692:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,762:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,762:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:36,763:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:36,763:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 14:37:36,868:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:36,939:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:36,939:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:37,044:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 14:37:37,116:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:37,117:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:37,117:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 14:37:37,314:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:37,314:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:37,511:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:37,511:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:37,512:INFO:Preparing preprocessing pipeline...
2022-10-21 14:37:37,513:INFO:Set up simple imputation.
2022-10-21 14:37:37,523:INFO:Set up encoding of ordinal features.
2022-10-21 14:37:37,526:INFO:Set up encoding of categorical features.
2022-10-21 14:37:37,526:INFO:Set up variance threshold.
2022-10-21 14:37:37,825:INFO:Finished creating preprocessing pipeline.
2022-10-21 14:37:37,843:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-21 14:37:37,844:INFO:Creating final display dataframe.
2022-10-21 14:37:38,651:INFO:Setup display_container:                  Description             Value
0                 Session id               123
1                     Target           charges
2                Target type        Regression
3                 Data shape        (1338, 10)
4           Train data shape         (936, 10)
5            Test data shape         (402, 10)
6           Ordinal features                 2
7           Numeric features                 3
8       Categorical features                 3
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15    Low variance threshold                 0
16            Fold Generator             KFold
17               Fold Number                10
18                  CPU Jobs                -1
19                   Use GPU             False
20            Log Experiment             False
21           Experiment Name  reg-default-name
22                       USI              c95e
2022-10-21 14:37:38,954:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:38,954:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:39,171:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 14:37:39,173:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 14:37:39,183:INFO:setup() successfully completed in 4.43s...............
2022-10-21 16:31:40,693:INFO:PyCaret RegressionExperiment
2022-10-21 16:31:40,695:INFO:Logging name: reg-default-name
2022-10-21 16:31:40,696:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 16:31:40,696:INFO:version 3.0.0.rc4
2022-10-21 16:31:40,696:INFO:Initializing setup()
2022-10-21 16:31:40,696:INFO:self.USI: e72c
2022-10-21 16:31:40,696:INFO:self.variable_keys: {'_available_plots', '_all_metrics', '_all_models_internal', 'X_train', 'display_container', 'gpu_param', 'idx', 'transform_target_method_param', 'html_param', 'exp_id', 'exp_name_log', 'USI', 'log_plots_param', 'variable_keys', 'seed', 'n_jobs_param', 'y_test', 'y_train', '_all_models', 'transform_target_param', '_ml_usecase', 'logging_param', 'data', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'X', 'fold_generator', 'y', 'master_model_container', 'X_test', 'target_param', '_gpu_n_jobs_param', 'pipeline'}
2022-10-21 16:31:40,696:INFO:Checking environment
2022-10-21 16:31:40,696:INFO:python_version: 3.9.7
2022-10-21 16:31:40,696:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 16:31:40,700:INFO:machine: x86_64
2022-10-21 16:31:40,700:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:31:40,700:INFO:Memory: svmem(total=8589934592, available=2324418560, percent=72.9, used=4171108352, free=14606336, active=2311770112, inactive=2309754880, wired=1859338240)
2022-10-21 16:31:40,700:INFO:Physical Core: 2
2022-10-21 16:31:40,701:INFO:Logical Core: 4
2022-10-21 16:31:40,701:INFO:Checking libraries
2022-10-21 16:31:40,701:INFO:System:
2022-10-21 16:31:40,701:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 16:31:40,701:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 16:31:40,701:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:31:40,701:INFO:PyCaret required dependencies:
2022-10-21 16:31:40,703:INFO:                 pip: 21.2.4
2022-10-21 16:31:40,703:INFO:          setuptools: 58.0.4
2022-10-21 16:31:40,703:INFO:             pycaret: 3.0.0rc4
2022-10-21 16:31:40,703:INFO:             IPython: 7.29.0
2022-10-21 16:31:40,704:INFO:          ipywidgets: 7.6.5
2022-10-21 16:31:40,704:INFO:                tqdm: 4.62.3
2022-10-21 16:31:40,704:INFO:               numpy: 1.22.4
2022-10-21 16:31:40,704:INFO:              pandas: 1.4.4
2022-10-21 16:31:40,704:INFO:              jinja2: 3.1.2
2022-10-21 16:31:40,704:INFO:               scipy: 1.8.1
2022-10-21 16:31:40,704:INFO:              joblib: 1.1.0
2022-10-21 16:31:40,704:INFO:             sklearn: 1.0.2
2022-10-21 16:31:40,704:INFO:                pyod: 1.0.5
2022-10-21 16:31:40,704:INFO:            imblearn: 0.9.0
2022-10-21 16:31:40,704:INFO:   category_encoders: 2.5.1.post0
2022-10-21 16:31:40,704:INFO:            lightgbm: 3.3.2
2022-10-21 16:31:40,704:INFO:               numba: 0.55.2
2022-10-21 16:31:40,704:INFO:            requests: 2.28.1
2022-10-21 16:31:40,704:INFO:          matplotlib: 3.4.3
2022-10-21 16:31:40,704:INFO:          scikitplot: 0.3.7
2022-10-21 16:31:40,704:INFO:         yellowbrick: 1.4
2022-10-21 16:31:40,704:INFO:              plotly: 5.5.0
2022-10-21 16:31:40,704:INFO:             kaleido: 0.2.1
2022-10-21 16:31:40,704:INFO:         statsmodels: 0.13.2
2022-10-21 16:31:40,705:INFO:              sktime: 0.13.4
2022-10-21 16:31:40,705:INFO:               tbats: 1.1.1
2022-10-21 16:31:40,705:INFO:            pmdarima: 1.8.5
2022-10-21 16:31:40,705:INFO:              psutil: 5.9.2
2022-10-21 16:31:40,705:INFO:PyCaret optional dependencies:
2022-10-21 16:31:40,705:INFO:                shap: 0.41.0
2022-10-21 16:31:40,705:INFO:           interpret: Not installed
2022-10-21 16:31:40,705:INFO:                umap: Not installed
2022-10-21 16:31:40,705:INFO:    pandas_profiling: Not installed
2022-10-21 16:31:40,705:INFO:  explainerdashboard: Not installed
2022-10-21 16:31:40,705:INFO:             autoviz: Not installed
2022-10-21 16:31:40,705:INFO:           fairlearn: Not installed
2022-10-21 16:31:40,705:INFO:             xgboost: Not installed
2022-10-21 16:31:40,705:INFO:            catboost: 1.1
2022-10-21 16:31:40,705:INFO:              kmodes: Not installed
2022-10-21 16:31:40,705:INFO:             mlxtend: Not installed
2022-10-21 16:31:40,705:INFO:       statsforecast: 1.1.1
2022-10-21 16:31:40,705:INFO:        tune_sklearn: Not installed
2022-10-21 16:31:40,706:INFO:                 ray: Not installed
2022-10-21 16:31:40,706:INFO:            hyperopt: Not installed
2022-10-21 16:31:40,706:INFO:              optuna: Not installed
2022-10-21 16:31:40,706:INFO:               skopt: Not installed
2022-10-21 16:31:40,706:INFO:              mlflow: 1.29.0
2022-10-21 16:31:40,706:INFO:              gradio: Not installed
2022-10-21 16:31:40,706:INFO:             fastapi: Not installed
2022-10-21 16:31:40,706:INFO:             uvicorn: Not installed
2022-10-21 16:31:40,706:INFO:              m2cgen: Not installed
2022-10-21 16:31:40,706:INFO:           evidently: Not installed
2022-10-21 16:31:40,706:INFO:                nltk: 3.6.5
2022-10-21 16:31:40,706:INFO:            pyLDAvis: Not installed
2022-10-21 16:31:40,706:INFO:              gensim: Not installed
2022-10-21 16:31:40,706:INFO:               spacy: Not installed
2022-10-21 16:31:40,706:INFO:           wordcloud: Not installed
2022-10-21 16:31:40,706:INFO:            textblob: Not installed
2022-10-21 16:31:40,706:INFO:               fugue: Not installed
2022-10-21 16:31:40,706:INFO:           streamlit: Not installed
2022-10-21 16:31:40,706:INFO:             prophet: 1.1.1
2022-10-21 16:31:40,707:INFO:None
2022-10-21 16:31:40,707:INFO:Set up data.
2022-10-21 16:31:50,905:INFO:PyCaret RegressionExperiment
2022-10-21 16:31:50,905:INFO:Logging name: reg-default-name
2022-10-21 16:31:50,905:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 16:31:50,905:INFO:version 3.0.0.rc4
2022-10-21 16:31:50,905:INFO:Initializing setup()
2022-10-21 16:31:50,905:INFO:self.USI: 5a4b
2022-10-21 16:31:50,905:INFO:self.variable_keys: {'_available_plots', '_all_metrics', '_all_models_internal', 'X_train', 'display_container', 'gpu_param', 'idx', 'transform_target_method_param', 'html_param', 'exp_id', 'exp_name_log', 'USI', 'log_plots_param', 'variable_keys', 'seed', 'n_jobs_param', 'y_test', 'y_train', '_all_models', 'transform_target_param', '_ml_usecase', 'logging_param', 'data', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'X', 'fold_generator', 'y', 'master_model_container', 'X_test', 'target_param', '_gpu_n_jobs_param', 'pipeline'}
2022-10-21 16:31:50,905:INFO:Checking environment
2022-10-21 16:31:50,906:INFO:python_version: 3.9.7
2022-10-21 16:31:50,906:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 16:31:50,906:INFO:machine: x86_64
2022-10-21 16:31:50,906:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:31:50,906:INFO:Memory: svmem(total=8589934592, available=2352816128, percent=72.6, used=4175859712, free=33456128, active=2324963328, inactive=2317840384, wired=1850896384)
2022-10-21 16:31:50,906:INFO:Physical Core: 2
2022-10-21 16:31:50,906:INFO:Logical Core: 4
2022-10-21 16:31:50,906:INFO:Checking libraries
2022-10-21 16:31:50,906:INFO:System:
2022-10-21 16:31:50,906:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 16:31:50,906:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 16:31:50,906:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:31:50,907:INFO:PyCaret required dependencies:
2022-10-21 16:31:50,907:INFO:                 pip: 21.2.4
2022-10-21 16:31:50,907:INFO:          setuptools: 58.0.4
2022-10-21 16:31:50,907:INFO:             pycaret: 3.0.0rc4
2022-10-21 16:31:50,907:INFO:             IPython: 7.29.0
2022-10-21 16:31:50,907:INFO:          ipywidgets: 7.6.5
2022-10-21 16:31:50,907:INFO:                tqdm: 4.62.3
2022-10-21 16:31:50,907:INFO:               numpy: 1.22.4
2022-10-21 16:31:50,907:INFO:              pandas: 1.4.4
2022-10-21 16:31:50,907:INFO:              jinja2: 3.1.2
2022-10-21 16:31:50,907:INFO:               scipy: 1.8.1
2022-10-21 16:31:50,907:INFO:              joblib: 1.1.0
2022-10-21 16:31:50,907:INFO:             sklearn: 1.0.2
2022-10-21 16:31:50,907:INFO:                pyod: 1.0.5
2022-10-21 16:31:50,908:INFO:            imblearn: 0.9.0
2022-10-21 16:31:50,908:INFO:   category_encoders: 2.5.1.post0
2022-10-21 16:31:50,908:INFO:            lightgbm: 3.3.2
2022-10-21 16:31:50,908:INFO:               numba: 0.55.2
2022-10-21 16:31:50,908:INFO:            requests: 2.28.1
2022-10-21 16:31:50,908:INFO:          matplotlib: 3.4.3
2022-10-21 16:31:50,908:INFO:          scikitplot: 0.3.7
2022-10-21 16:31:50,908:INFO:         yellowbrick: 1.4
2022-10-21 16:31:50,908:INFO:              plotly: 5.5.0
2022-10-21 16:31:50,908:INFO:             kaleido: 0.2.1
2022-10-21 16:31:50,908:INFO:         statsmodels: 0.13.2
2022-10-21 16:31:50,908:INFO:              sktime: 0.13.4
2022-10-21 16:31:50,908:INFO:               tbats: 1.1.1
2022-10-21 16:31:50,908:INFO:            pmdarima: 1.8.5
2022-10-21 16:31:50,909:INFO:              psutil: 5.9.2
2022-10-21 16:31:50,909:INFO:PyCaret optional dependencies:
2022-10-21 16:31:50,909:INFO:                shap: 0.41.0
2022-10-21 16:31:50,910:INFO:           interpret: Not installed
2022-10-21 16:31:50,910:INFO:                umap: Not installed
2022-10-21 16:31:50,910:INFO:    pandas_profiling: Not installed
2022-10-21 16:31:50,911:INFO:  explainerdashboard: Not installed
2022-10-21 16:31:50,911:INFO:             autoviz: Not installed
2022-10-21 16:31:50,911:INFO:           fairlearn: Not installed
2022-10-21 16:31:50,911:INFO:             xgboost: Not installed
2022-10-21 16:31:50,911:INFO:            catboost: 1.1
2022-10-21 16:31:50,911:INFO:              kmodes: Not installed
2022-10-21 16:31:50,911:INFO:             mlxtend: Not installed
2022-10-21 16:31:50,912:INFO:       statsforecast: 1.1.1
2022-10-21 16:31:50,912:INFO:        tune_sklearn: Not installed
2022-10-21 16:31:50,912:INFO:                 ray: Not installed
2022-10-21 16:31:50,912:INFO:            hyperopt: Not installed
2022-10-21 16:31:50,912:INFO:              optuna: Not installed
2022-10-21 16:31:50,912:INFO:               skopt: Not installed
2022-10-21 16:31:50,912:INFO:              mlflow: 1.29.0
2022-10-21 16:31:50,912:INFO:              gradio: Not installed
2022-10-21 16:31:50,912:INFO:             fastapi: Not installed
2022-10-21 16:31:50,912:INFO:             uvicorn: Not installed
2022-10-21 16:31:50,912:INFO:              m2cgen: Not installed
2022-10-21 16:31:50,912:INFO:           evidently: Not installed
2022-10-21 16:31:50,912:INFO:                nltk: 3.6.5
2022-10-21 16:31:50,912:INFO:            pyLDAvis: Not installed
2022-10-21 16:31:50,912:INFO:              gensim: Not installed
2022-10-21 16:31:50,912:INFO:               spacy: Not installed
2022-10-21 16:31:50,912:INFO:           wordcloud: Not installed
2022-10-21 16:31:50,913:INFO:            textblob: Not installed
2022-10-21 16:31:50,913:INFO:               fugue: Not installed
2022-10-21 16:31:50,913:INFO:           streamlit: Not installed
2022-10-21 16:31:50,913:INFO:             prophet: 1.1.1
2022-10-21 16:31:50,913:INFO:None
2022-10-21 16:31:50,913:INFO:Set up data.
2022-10-21 16:31:50,934:INFO:Set up train/test split.
2022-10-21 16:31:50,945:INFO:Set up index.
2022-10-21 16:31:50,945:INFO:Set up folding strategy.
2022-10-21 16:31:50,946:INFO:Assigning column types.
2022-10-21 16:31:50,954:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 16:31:50,954:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:31:50,972:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:31:50,987:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:31:51,529:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:51,903:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:51,905:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:51,906:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:51,907:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:31:51,917:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:31:51,929:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,445:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,539:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,540:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:52,541:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:52,542:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 16:31:52,552:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,559:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,675:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,758:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,759:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:52,759:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:52,770:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,777:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,874:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,951:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:52,952:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:52,953:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:52,954:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 16:31:52,969:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,068:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,142:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,143:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:53,143:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:53,160:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,258:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,336:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,337:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:53,337:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:53,338:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 16:31:53,452:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,528:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,529:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:53,529:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:53,642:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,717:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,718:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:53,719:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:53,720:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 16:31:53,837:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:53,911:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:53,912:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:54,026:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:31:54,102:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:54,102:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:54,103:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 16:31:54,318:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:54,319:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:54,513:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:54,516:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:54,518:INFO:Preparing preprocessing pipeline...
2022-10-21 16:31:54,519:INFO:Set up simple imputation.
2022-10-21 16:31:54,525:INFO:Set up encoding of ordinal features.
2022-10-21 16:31:54,528:INFO:Set up encoding of categorical features.
2022-10-21 16:31:54,529:INFO:Set up variance threshold.
2022-10-21 16:31:54,786:INFO:Finished creating preprocessing pipeline.
2022-10-21 16:31:54,805:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-21 16:31:54,805:INFO:Creating final display dataframe.
2022-10-21 16:31:55,634:INFO:Setup display_container:                  Description             Value
0                 Session id               123
1                     Target             Label
2                Target type        Regression
3                 Data shape        (1338, 10)
4           Train data shape         (936, 10)
5            Test data shape         (402, 10)
6           Ordinal features                 2
7           Numeric features                 3
8       Categorical features                 3
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15    Low variance threshold                 0
16            Fold Generator             KFold
17               Fold Number                10
18                  CPU Jobs                -1
19                   Use GPU             False
20            Log Experiment             False
21           Experiment Name  reg-default-name
22                       USI              5a4b
2022-10-21 16:31:55,922:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:55,922:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:56,119:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:31:56,119:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:31:56,129:INFO:setup() successfully completed in 5.23s...............
2022-10-21 16:32:30,734:INFO:Initializing compare_models()
2022-10-21 16:32:30,736:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-21 16:32:30,737:INFO:Checking exceptions
2022-10-21 16:32:30,741:INFO:Preparing display monitor
2022-10-21 16:32:30,974:INFO:Initializing Linear Regression
2022-10-21 16:32:30,974:INFO:Total runtime is 1.3120969136555989e-05 minutes
2022-10-21 16:32:30,983:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:30,984:INFO:Initializing create_model()
2022-10-21 16:32:30,984:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:30,984:INFO:Checking exceptions
2022-10-21 16:32:30,990:INFO:Importing libraries
2022-10-21 16:32:30,995:INFO:Copying training dataset
2022-10-21 16:32:31,007:INFO:Defining folds
2022-10-21 16:32:31,007:INFO:Declaring metric variables
2022-10-21 16:32:31,018:INFO:Importing untrained model
2022-10-21 16:32:31,030:INFO:Linear Regression Imported successfully
2022-10-21 16:32:31,058:INFO:Starting cross validation
2022-10-21 16:32:31,165:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:52,675:INFO:Calculating mean and std
2022-10-21 16:32:52,682:INFO:Creating metrics dataframe
2022-10-21 16:32:52,692:INFO:Uploading results into container
2022-10-21 16:32:52,693:INFO:Uploading model into container now
2022-10-21 16:32:52,694:INFO:master_model_container: 1
2022-10-21 16:32:52,695:INFO:display_container: 2
2022-10-21 16:32:52,696:INFO:LinearRegression(n_jobs=-1)
2022-10-21 16:32:52,696:INFO:create_model() successfully completed......................................
2022-10-21 16:32:53,302:INFO:SubProcess create_model() end ==================================
2022-10-21 16:32:53,303:INFO:Creating metrics dataframe
2022-10-21 16:32:53,320:INFO:Initializing Lasso Regression
2022-10-21 16:32:53,320:INFO:Total runtime is 0.37244437138239544 minutes
2022-10-21 16:32:53,328:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:53,329:INFO:Initializing create_model()
2022-10-21 16:32:53,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:53,329:INFO:Checking exceptions
2022-10-21 16:32:53,334:INFO:Importing libraries
2022-10-21 16:32:53,335:INFO:Copying training dataset
2022-10-21 16:32:53,346:INFO:Defining folds
2022-10-21 16:32:53,346:INFO:Declaring metric variables
2022-10-21 16:32:53,381:INFO:Importing untrained model
2022-10-21 16:32:53,395:INFO:Lasso Regression Imported successfully
2022-10-21 16:32:53,458:INFO:Starting cross validation
2022-10-21 16:32:53,461:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:54,295:INFO:Calculating mean and std
2022-10-21 16:32:54,298:INFO:Creating metrics dataframe
2022-10-21 16:32:54,305:INFO:Uploading results into container
2022-10-21 16:32:54,306:INFO:Uploading model into container now
2022-10-21 16:32:54,307:INFO:master_model_container: 2
2022-10-21 16:32:54,307:INFO:display_container: 2
2022-10-21 16:32:54,307:INFO:Lasso(random_state=123)
2022-10-21 16:32:54,308:INFO:create_model() successfully completed......................................
2022-10-21 16:32:54,439:INFO:SubProcess create_model() end ==================================
2022-10-21 16:32:54,440:INFO:Creating metrics dataframe
2022-10-21 16:32:54,455:INFO:Initializing Ridge Regression
2022-10-21 16:32:54,455:INFO:Total runtime is 0.39135775168736775 minutes
2022-10-21 16:32:54,461:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:54,462:INFO:Initializing create_model()
2022-10-21 16:32:54,462:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:54,462:INFO:Checking exceptions
2022-10-21 16:32:54,465:INFO:Importing libraries
2022-10-21 16:32:54,465:INFO:Copying training dataset
2022-10-21 16:32:54,478:INFO:Defining folds
2022-10-21 16:32:54,478:INFO:Declaring metric variables
2022-10-21 16:32:54,495:INFO:Importing untrained model
2022-10-21 16:32:54,514:INFO:Ridge Regression Imported successfully
2022-10-21 16:32:54,570:INFO:Starting cross validation
2022-10-21 16:32:54,573:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:55,147:INFO:Calculating mean and std
2022-10-21 16:32:55,149:INFO:Creating metrics dataframe
2022-10-21 16:32:55,156:INFO:Uploading results into container
2022-10-21 16:32:55,157:INFO:Uploading model into container now
2022-10-21 16:32:55,158:INFO:master_model_container: 3
2022-10-21 16:32:55,159:INFO:display_container: 2
2022-10-21 16:32:55,159:INFO:Ridge(random_state=123)
2022-10-21 16:32:55,159:INFO:create_model() successfully completed......................................
2022-10-21 16:32:55,296:INFO:SubProcess create_model() end ==================================
2022-10-21 16:32:55,296:INFO:Creating metrics dataframe
2022-10-21 16:32:55,311:INFO:Initializing Elastic Net
2022-10-21 16:32:55,311:INFO:Total runtime is 0.40563098589579266 minutes
2022-10-21 16:32:55,319:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:55,320:INFO:Initializing create_model()
2022-10-21 16:32:55,320:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:55,320:INFO:Checking exceptions
2022-10-21 16:32:55,324:INFO:Importing libraries
2022-10-21 16:32:55,325:INFO:Copying training dataset
2022-10-21 16:32:55,342:INFO:Defining folds
2022-10-21 16:32:55,342:INFO:Declaring metric variables
2022-10-21 16:32:55,377:INFO:Importing untrained model
2022-10-21 16:32:55,396:INFO:Elastic Net Imported successfully
2022-10-21 16:32:55,424:INFO:Starting cross validation
2022-10-21 16:32:55,427:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:56,036:INFO:Calculating mean and std
2022-10-21 16:32:56,039:INFO:Creating metrics dataframe
2022-10-21 16:32:56,044:INFO:Uploading results into container
2022-10-21 16:32:56,045:INFO:Uploading model into container now
2022-10-21 16:32:56,046:INFO:master_model_container: 4
2022-10-21 16:32:56,046:INFO:display_container: 2
2022-10-21 16:32:56,046:INFO:ElasticNet(random_state=123)
2022-10-21 16:32:56,046:INFO:create_model() successfully completed......................................
2022-10-21 16:32:56,184:INFO:SubProcess create_model() end ==================================
2022-10-21 16:32:56,184:INFO:Creating metrics dataframe
2022-10-21 16:32:56,204:INFO:Initializing Least Angle Regression
2022-10-21 16:32:56,205:INFO:Total runtime is 0.4205233216285706 minutes
2022-10-21 16:32:56,214:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:56,215:INFO:Initializing create_model()
2022-10-21 16:32:56,215:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:56,217:INFO:Checking exceptions
2022-10-21 16:32:56,220:INFO:Importing libraries
2022-10-21 16:32:56,220:INFO:Copying training dataset
2022-10-21 16:32:56,230:INFO:Defining folds
2022-10-21 16:32:56,230:INFO:Declaring metric variables
2022-10-21 16:32:56,258:INFO:Importing untrained model
2022-10-21 16:32:56,275:INFO:Least Angle Regression Imported successfully
2022-10-21 16:32:56,313:INFO:Starting cross validation
2022-10-21 16:32:56,315:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:56,509:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,520:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,521:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,538:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,713:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,728:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,729:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,743:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,871:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,874:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:56,913:INFO:Calculating mean and std
2022-10-21 16:32:56,916:INFO:Creating metrics dataframe
2022-10-21 16:32:56,923:INFO:Uploading results into container
2022-10-21 16:32:56,924:INFO:Uploading model into container now
2022-10-21 16:32:56,925:INFO:master_model_container: 5
2022-10-21 16:32:56,925:INFO:display_container: 2
2022-10-21 16:32:56,928:INFO:Lars(random_state=123)
2022-10-21 16:32:56,928:INFO:create_model() successfully completed......................................
2022-10-21 16:32:57,102:INFO:SubProcess create_model() end ==================================
2022-10-21 16:32:57,103:INFO:Creating metrics dataframe
2022-10-21 16:32:57,128:INFO:Initializing Lasso Least Angle Regression
2022-10-21 16:32:57,128:INFO:Total runtime is 0.43590978781382245 minutes
2022-10-21 16:32:57,144:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:57,144:INFO:Initializing create_model()
2022-10-21 16:32:57,145:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:57,145:INFO:Checking exceptions
2022-10-21 16:32:57,148:INFO:Importing libraries
2022-10-21 16:32:57,149:INFO:Copying training dataset
2022-10-21 16:32:57,157:INFO:Defining folds
2022-10-21 16:32:57,157:INFO:Declaring metric variables
2022-10-21 16:32:57,171:INFO:Importing untrained model
2022-10-21 16:32:57,193:INFO:Lasso Least Angle Regression Imported successfully
2022-10-21 16:32:57,241:INFO:Starting cross validation
2022-10-21 16:32:57,270:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:57,513:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,513:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,539:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,541:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,745:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,746:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,769:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,772:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,886:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,887:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:32:57,926:INFO:Calculating mean and std
2022-10-21 16:32:57,929:INFO:Creating metrics dataframe
2022-10-21 16:32:57,934:INFO:Uploading results into container
2022-10-21 16:32:57,935:INFO:Uploading model into container now
2022-10-21 16:32:57,936:INFO:master_model_container: 6
2022-10-21 16:32:57,936:INFO:display_container: 2
2022-10-21 16:32:57,937:INFO:LassoLars(random_state=123)
2022-10-21 16:32:57,937:INFO:create_model() successfully completed......................................
2022-10-21 16:32:58,093:INFO:SubProcess create_model() end ==================================
2022-10-21 16:32:58,093:INFO:Creating metrics dataframe
2022-10-21 16:32:58,131:INFO:Initializing Orthogonal Matching Pursuit
2022-10-21 16:32:58,132:INFO:Total runtime is 0.4526366869608561 minutes
2022-10-21 16:32:58,161:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:58,162:INFO:Initializing create_model()
2022-10-21 16:32:58,162:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:58,162:INFO:Checking exceptions
2022-10-21 16:32:58,166:INFO:Importing libraries
2022-10-21 16:32:58,166:INFO:Copying training dataset
2022-10-21 16:32:58,175:INFO:Defining folds
2022-10-21 16:32:58,176:INFO:Declaring metric variables
2022-10-21 16:32:58,208:INFO:Importing untrained model
2022-10-21 16:32:58,232:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-21 16:32:58,293:INFO:Starting cross validation
2022-10-21 16:32:58,305:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:58,509:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,539:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,555:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,558:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,754:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,755:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,759:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,760:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,929:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,929:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:32:58,988:INFO:Calculating mean and std
2022-10-21 16:32:58,991:INFO:Creating metrics dataframe
2022-10-21 16:32:58,996:INFO:Uploading results into container
2022-10-21 16:32:58,997:INFO:Uploading model into container now
2022-10-21 16:32:58,998:INFO:master_model_container: 7
2022-10-21 16:32:58,998:INFO:display_container: 2
2022-10-21 16:32:58,999:INFO:OrthogonalMatchingPursuit()
2022-10-21 16:32:59,002:INFO:create_model() successfully completed......................................
2022-10-21 16:32:59,138:INFO:SubProcess create_model() end ==================================
2022-10-21 16:32:59,140:INFO:Creating metrics dataframe
2022-10-21 16:32:59,158:INFO:Initializing Bayesian Ridge
2022-10-21 16:32:59,158:INFO:Total runtime is 0.46974048614501956 minutes
2022-10-21 16:32:59,166:INFO:SubProcess create_model() called ==================================
2022-10-21 16:32:59,168:INFO:Initializing create_model()
2022-10-21 16:32:59,168:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:32:59,169:INFO:Checking exceptions
2022-10-21 16:32:59,172:INFO:Importing libraries
2022-10-21 16:32:59,173:INFO:Copying training dataset
2022-10-21 16:32:59,207:INFO:Defining folds
2022-10-21 16:32:59,207:INFO:Declaring metric variables
2022-10-21 16:32:59,239:INFO:Importing untrained model
2022-10-21 16:32:59,246:INFO:Bayesian Ridge Imported successfully
2022-10-21 16:32:59,281:INFO:Starting cross validation
2022-10-21 16:32:59,286:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:32:59,996:INFO:Calculating mean and std
2022-10-21 16:33:00,004:INFO:Creating metrics dataframe
2022-10-21 16:33:00,013:INFO:Uploading results into container
2022-10-21 16:33:00,014:INFO:Uploading model into container now
2022-10-21 16:33:00,014:INFO:master_model_container: 8
2022-10-21 16:33:00,015:INFO:display_container: 2
2022-10-21 16:33:00,015:INFO:BayesianRidge()
2022-10-21 16:33:00,016:INFO:create_model() successfully completed......................................
2022-10-21 16:33:00,148:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:00,149:INFO:Creating metrics dataframe
2022-10-21 16:33:00,170:INFO:Initializing Passive Aggressive Regressor
2022-10-21 16:33:00,171:INFO:Total runtime is 0.48661875327428183 minutes
2022-10-21 16:33:00,178:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:00,179:INFO:Initializing create_model()
2022-10-21 16:33:00,179:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:00,180:INFO:Checking exceptions
2022-10-21 16:33:00,183:INFO:Importing libraries
2022-10-21 16:33:00,183:INFO:Copying training dataset
2022-10-21 16:33:00,196:INFO:Defining folds
2022-10-21 16:33:00,197:INFO:Declaring metric variables
2022-10-21 16:33:00,242:INFO:Importing untrained model
2022-10-21 16:33:00,257:INFO:Passive Aggressive Regressor Imported successfully
2022-10-21 16:33:00,272:INFO:Starting cross validation
2022-10-21 16:33:00,274:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:01,091:INFO:Calculating mean and std
2022-10-21 16:33:01,095:INFO:Creating metrics dataframe
2022-10-21 16:33:01,101:INFO:Uploading results into container
2022-10-21 16:33:01,102:INFO:Uploading model into container now
2022-10-21 16:33:01,103:INFO:master_model_container: 9
2022-10-21 16:33:01,104:INFO:display_container: 2
2022-10-21 16:33:01,104:INFO:PassiveAggressiveRegressor(random_state=123)
2022-10-21 16:33:01,105:INFO:create_model() successfully completed......................................
2022-10-21 16:33:01,252:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:01,252:INFO:Creating metrics dataframe
2022-10-21 16:33:01,272:INFO:Initializing Huber Regressor
2022-10-21 16:33:01,272:INFO:Total runtime is 0.5049835681915283 minutes
2022-10-21 16:33:01,278:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:01,278:INFO:Initializing create_model()
2022-10-21 16:33:01,278:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:01,278:INFO:Checking exceptions
2022-10-21 16:33:01,285:INFO:Importing libraries
2022-10-21 16:33:01,285:INFO:Copying training dataset
2022-10-21 16:33:01,290:INFO:Defining folds
2022-10-21 16:33:01,291:INFO:Declaring metric variables
2022-10-21 16:33:01,330:INFO:Importing untrained model
2022-10-21 16:33:01,348:INFO:Huber Regressor Imported successfully
2022-10-21 16:33:01,396:INFO:Starting cross validation
2022-10-21 16:33:01,398:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:01,724:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:01,731:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:01,755:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:01,764:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:02,042:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:02,069:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:02,090:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:02,093:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:02,307:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:02,328:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:33:02,364:INFO:Calculating mean and std
2022-10-21 16:33:02,369:INFO:Creating metrics dataframe
2022-10-21 16:33:02,374:INFO:Uploading results into container
2022-10-21 16:33:02,375:INFO:Uploading model into container now
2022-10-21 16:33:02,376:INFO:master_model_container: 10
2022-10-21 16:33:02,376:INFO:display_container: 2
2022-10-21 16:33:02,377:INFO:HuberRegressor()
2022-10-21 16:33:02,377:INFO:create_model() successfully completed......................................
2022-10-21 16:33:02,520:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:02,520:INFO:Creating metrics dataframe
2022-10-21 16:33:02,542:INFO:Initializing K Neighbors Regressor
2022-10-21 16:33:02,542:INFO:Total runtime is 0.5261426369349163 minutes
2022-10-21 16:33:02,558:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:02,558:INFO:Initializing create_model()
2022-10-21 16:33:02,558:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:02,559:INFO:Checking exceptions
2022-10-21 16:33:02,562:INFO:Importing libraries
2022-10-21 16:33:02,562:INFO:Copying training dataset
2022-10-21 16:33:02,582:INFO:Defining folds
2022-10-21 16:33:02,583:INFO:Declaring metric variables
2022-10-21 16:33:02,605:INFO:Importing untrained model
2022-10-21 16:33:02,616:INFO:K Neighbors Regressor Imported successfully
2022-10-21 16:33:02,637:INFO:Starting cross validation
2022-10-21 16:33:02,639:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:03,285:INFO:Calculating mean and std
2022-10-21 16:33:03,288:INFO:Creating metrics dataframe
2022-10-21 16:33:03,294:INFO:Uploading results into container
2022-10-21 16:33:03,296:INFO:Uploading model into container now
2022-10-21 16:33:03,297:INFO:master_model_container: 11
2022-10-21 16:33:03,297:INFO:display_container: 2
2022-10-21 16:33:03,297:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-21 16:33:03,297:INFO:create_model() successfully completed......................................
2022-10-21 16:33:03,429:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:03,430:INFO:Creating metrics dataframe
2022-10-21 16:33:03,446:INFO:Initializing Decision Tree Regressor
2022-10-21 16:33:03,447:INFO:Total runtime is 0.541219182809194 minutes
2022-10-21 16:33:03,455:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:03,456:INFO:Initializing create_model()
2022-10-21 16:33:03,456:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:03,456:INFO:Checking exceptions
2022-10-21 16:33:03,460:INFO:Importing libraries
2022-10-21 16:33:03,461:INFO:Copying training dataset
2022-10-21 16:33:03,479:INFO:Defining folds
2022-10-21 16:33:03,479:INFO:Declaring metric variables
2022-10-21 16:33:03,488:INFO:Importing untrained model
2022-10-21 16:33:03,497:INFO:Decision Tree Regressor Imported successfully
2022-10-21 16:33:03,513:INFO:Starting cross validation
2022-10-21 16:33:03,516:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:04,137:INFO:Calculating mean and std
2022-10-21 16:33:04,139:INFO:Creating metrics dataframe
2022-10-21 16:33:04,143:INFO:Uploading results into container
2022-10-21 16:33:04,144:INFO:Uploading model into container now
2022-10-21 16:33:04,146:INFO:master_model_container: 12
2022-10-21 16:33:04,146:INFO:display_container: 2
2022-10-21 16:33:04,146:INFO:DecisionTreeRegressor(random_state=123)
2022-10-21 16:33:04,146:INFO:create_model() successfully completed......................................
2022-10-21 16:33:04,287:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:04,287:INFO:Creating metrics dataframe
2022-10-21 16:33:04,304:INFO:Initializing Random Forest Regressor
2022-10-21 16:33:04,304:INFO:Total runtime is 0.5555153052012126 minutes
2022-10-21 16:33:04,312:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:04,312:INFO:Initializing create_model()
2022-10-21 16:33:04,312:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:04,312:INFO:Checking exceptions
2022-10-21 16:33:04,316:INFO:Importing libraries
2022-10-21 16:33:04,316:INFO:Copying training dataset
2022-10-21 16:33:04,325:INFO:Defining folds
2022-10-21 16:33:04,325:INFO:Declaring metric variables
2022-10-21 16:33:04,357:INFO:Importing untrained model
2022-10-21 16:33:04,375:INFO:Random Forest Regressor Imported successfully
2022-10-21 16:33:04,442:INFO:Starting cross validation
2022-10-21 16:33:04,455:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:07,475:INFO:Calculating mean and std
2022-10-21 16:33:07,479:INFO:Creating metrics dataframe
2022-10-21 16:33:07,485:INFO:Uploading results into container
2022-10-21 16:33:07,486:INFO:Uploading model into container now
2022-10-21 16:33:07,487:INFO:master_model_container: 13
2022-10-21 16:33:07,487:INFO:display_container: 2
2022-10-21 16:33:07,487:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2022-10-21 16:33:07,487:INFO:create_model() successfully completed......................................
2022-10-21 16:33:07,629:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:07,629:INFO:Creating metrics dataframe
2022-10-21 16:33:07,647:INFO:Initializing Extra Trees Regressor
2022-10-21 16:33:07,648:INFO:Total runtime is 0.6112358371416728 minutes
2022-10-21 16:33:07,656:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:07,657:INFO:Initializing create_model()
2022-10-21 16:33:07,657:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:07,657:INFO:Checking exceptions
2022-10-21 16:33:07,660:INFO:Importing libraries
2022-10-21 16:33:07,661:INFO:Copying training dataset
2022-10-21 16:33:07,680:INFO:Defining folds
2022-10-21 16:33:07,680:INFO:Declaring metric variables
2022-10-21 16:33:07,713:INFO:Importing untrained model
2022-10-21 16:33:07,730:INFO:Extra Trees Regressor Imported successfully
2022-10-21 16:33:07,763:INFO:Starting cross validation
2022-10-21 16:33:07,770:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:09,922:INFO:Calculating mean and std
2022-10-21 16:33:09,925:INFO:Creating metrics dataframe
2022-10-21 16:33:09,930:INFO:Uploading results into container
2022-10-21 16:33:09,931:INFO:Uploading model into container now
2022-10-21 16:33:09,932:INFO:master_model_container: 14
2022-10-21 16:33:09,932:INFO:display_container: 2
2022-10-21 16:33:09,933:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2022-10-21 16:33:09,934:INFO:create_model() successfully completed......................................
2022-10-21 16:33:10,080:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:10,080:INFO:Creating metrics dataframe
2022-10-21 16:33:10,098:INFO:Initializing AdaBoost Regressor
2022-10-21 16:33:10,098:INFO:Total runtime is 0.652080500125885 minutes
2022-10-21 16:33:10,107:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:10,107:INFO:Initializing create_model()
2022-10-21 16:33:10,107:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:10,108:INFO:Checking exceptions
2022-10-21 16:33:10,111:INFO:Importing libraries
2022-10-21 16:33:10,111:INFO:Copying training dataset
2022-10-21 16:33:10,131:INFO:Defining folds
2022-10-21 16:33:10,131:INFO:Declaring metric variables
2022-10-21 16:33:10,169:INFO:Importing untrained model
2022-10-21 16:33:10,196:INFO:AdaBoost Regressor Imported successfully
2022-10-21 16:33:10,259:INFO:Starting cross validation
2022-10-21 16:33:10,262:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:11,120:INFO:Calculating mean and std
2022-10-21 16:33:11,122:INFO:Creating metrics dataframe
2022-10-21 16:33:11,126:INFO:Uploading results into container
2022-10-21 16:33:11,128:INFO:Uploading model into container now
2022-10-21 16:33:11,129:INFO:master_model_container: 15
2022-10-21 16:33:11,129:INFO:display_container: 2
2022-10-21 16:33:11,129:INFO:AdaBoostRegressor(random_state=123)
2022-10-21 16:33:11,129:INFO:create_model() successfully completed......................................
2022-10-21 16:33:11,271:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:11,271:INFO:Creating metrics dataframe
2022-10-21 16:33:11,292:INFO:Initializing Gradient Boosting Regressor
2022-10-21 16:33:11,292:INFO:Total runtime is 0.6719829519589742 minutes
2022-10-21 16:33:11,298:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:11,299:INFO:Initializing create_model()
2022-10-21 16:33:11,299:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:11,299:INFO:Checking exceptions
2022-10-21 16:33:11,305:INFO:Importing libraries
2022-10-21 16:33:11,305:INFO:Copying training dataset
2022-10-21 16:33:11,316:INFO:Defining folds
2022-10-21 16:33:11,317:INFO:Declaring metric variables
2022-10-21 16:33:11,350:INFO:Importing untrained model
2022-10-21 16:33:11,373:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:33:11,389:INFO:Starting cross validation
2022-10-21 16:33:11,391:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:12,426:INFO:Calculating mean and std
2022-10-21 16:33:12,429:INFO:Creating metrics dataframe
2022-10-21 16:33:12,436:INFO:Uploading results into container
2022-10-21 16:33:12,437:INFO:Uploading model into container now
2022-10-21 16:33:12,437:INFO:master_model_container: 16
2022-10-21 16:33:12,437:INFO:display_container: 2
2022-10-21 16:33:12,438:INFO:GradientBoostingRegressor(random_state=123)
2022-10-21 16:33:12,438:INFO:create_model() successfully completed......................................
2022-10-21 16:33:12,578:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:12,578:INFO:Creating metrics dataframe
2022-10-21 16:33:12,604:INFO:Initializing Light Gradient Boosting Machine
2022-10-21 16:33:12,604:INFO:Total runtime is 0.6938509702682495 minutes
2022-10-21 16:33:12,615:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:12,616:INFO:Initializing create_model()
2022-10-21 16:33:12,616:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:12,616:INFO:Checking exceptions
2022-10-21 16:33:12,620:INFO:Importing libraries
2022-10-21 16:33:12,620:INFO:Copying training dataset
2022-10-21 16:33:12,630:INFO:Defining folds
2022-10-21 16:33:12,630:INFO:Declaring metric variables
2022-10-21 16:33:12,664:INFO:Importing untrained model
2022-10-21 16:33:12,693:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-21 16:33:12,729:INFO:Starting cross validation
2022-10-21 16:33:12,731:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:15,825:INFO:Calculating mean and std
2022-10-21 16:33:15,829:INFO:Creating metrics dataframe
2022-10-21 16:33:15,835:INFO:Uploading results into container
2022-10-21 16:33:15,836:INFO:Uploading model into container now
2022-10-21 16:33:15,837:INFO:master_model_container: 17
2022-10-21 16:33:15,837:INFO:display_container: 2
2022-10-21 16:33:15,838:INFO:LGBMRegressor(random_state=123)
2022-10-21 16:33:15,839:INFO:create_model() successfully completed......................................
2022-10-21 16:33:15,979:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:15,979:INFO:Creating metrics dataframe
2022-10-21 16:33:16,003:INFO:Initializing CatBoost Regressor
2022-10-21 16:33:16,004:INFO:Total runtime is 0.7505033175150553 minutes
2022-10-21 16:33:16,012:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:16,012:INFO:Initializing create_model()
2022-10-21 16:33:16,013:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:16,013:INFO:Checking exceptions
2022-10-21 16:33:16,017:INFO:Importing libraries
2022-10-21 16:33:16,018:INFO:Copying training dataset
2022-10-21 16:33:16,025:INFO:Defining folds
2022-10-21 16:33:16,026:INFO:Declaring metric variables
2022-10-21 16:33:16,054:INFO:Importing untrained model
2022-10-21 16:33:16,078:INFO:CatBoost Regressor Imported successfully
2022-10-21 16:33:16,113:INFO:Starting cross validation
2022-10-21 16:33:16,115:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:21,689:INFO:Calculating mean and std
2022-10-21 16:33:21,691:INFO:Creating metrics dataframe
2022-10-21 16:33:21,696:INFO:Uploading results into container
2022-10-21 16:33:21,696:INFO:Uploading model into container now
2022-10-21 16:33:21,697:INFO:master_model_container: 18
2022-10-21 16:33:21,697:INFO:display_container: 2
2022-10-21 16:33:21,698:INFO:<catboost.core.CatBoostRegressor object at 0x7facbfb4b280>
2022-10-21 16:33:21,698:INFO:create_model() successfully completed......................................
2022-10-21 16:33:21,880:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:21,880:INFO:Creating metrics dataframe
2022-10-21 16:33:21,904:INFO:Initializing Dummy Regressor
2022-10-21 16:33:21,904:INFO:Total runtime is 0.84884401957194 minutes
2022-10-21 16:33:21,912:INFO:SubProcess create_model() called ==================================
2022-10-21 16:33:21,913:INFO:Initializing create_model()
2022-10-21 16:33:21,913:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca0574820>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:21,913:INFO:Checking exceptions
2022-10-21 16:33:21,920:INFO:Importing libraries
2022-10-21 16:33:21,920:INFO:Copying training dataset
2022-10-21 16:33:21,947:INFO:Defining folds
2022-10-21 16:33:21,947:INFO:Declaring metric variables
2022-10-21 16:33:21,963:INFO:Importing untrained model
2022-10-21 16:33:21,998:INFO:Dummy Regressor Imported successfully
2022-10-21 16:33:22,044:INFO:Starting cross validation
2022-10-21 16:33:22,054:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:33:22,645:INFO:Calculating mean and std
2022-10-21 16:33:22,647:INFO:Creating metrics dataframe
2022-10-21 16:33:22,659:INFO:Uploading results into container
2022-10-21 16:33:22,660:INFO:Uploading model into container now
2022-10-21 16:33:22,660:INFO:master_model_container: 19
2022-10-21 16:33:22,660:INFO:display_container: 2
2022-10-21 16:33:22,661:INFO:DummyRegressor()
2022-10-21 16:33:22,661:INFO:create_model() successfully completed......................................
2022-10-21 16:33:22,809:INFO:SubProcess create_model() end ==================================
2022-10-21 16:33:22,809:INFO:Creating metrics dataframe
2022-10-21 16:33:22,858:INFO:Initializing create_model()
2022-10-21 16:33:22,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:33:22,858:INFO:Checking exceptions
2022-10-21 16:33:22,916:INFO:Importing libraries
2022-10-21 16:33:22,916:INFO:Copying training dataset
2022-10-21 16:33:22,927:INFO:Defining folds
2022-10-21 16:33:22,927:INFO:Declaring metric variables
2022-10-21 16:33:22,927:INFO:Importing untrained model
2022-10-21 16:33:22,927:INFO:Declaring custom model
2022-10-21 16:33:22,929:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:33:22,930:INFO:Cross validation set to False
2022-10-21 16:33:22,931:INFO:Fitting Model
2022-10-21 16:33:23,222:INFO:GradientBoostingRegressor(random_state=123)
2022-10-21 16:33:23,222:INFO:create_model() successfully completed......................................
2022-10-21 16:33:23,527:INFO:master_model_container: 19
2022-10-21 16:33:23,527:INFO:display_container: 2
2022-10-21 16:33:23,527:INFO:GradientBoostingRegressor(random_state=123)
2022-10-21 16:33:23,527:INFO:compare_models() successfully completed......................................
2022-10-21 16:33:23,542:INFO:Initializing evaluate_model()
2022-10-21 16:33:23,543:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=GradientBoostingRegressor(random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-21 16:33:23,639:INFO:Initializing plot_model()
2022-10-21 16:33:23,639:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, system=True)
2022-10-21 16:33:23,639:INFO:Checking exceptions
2022-10-21 16:33:23,646:INFO:Preloading libraries
2022-10-21 16:33:23,677:INFO:Copying training dataset
2022-10-21 16:33:23,677:INFO:Plot type: pipeline
2022-10-21 16:33:24,153:INFO:Visual Rendered Successfully
2022-10-21 16:33:24,308:INFO:plot_model() successfully completed......................................
2022-10-21 16:33:24,311:INFO:Initializing predict_model()
2022-10-21 16:33:24,311:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fac9fff68b0>)
2022-10-21 16:33:24,311:INFO:Checking exceptions
2022-10-21 16:33:24,312:INFO:Preloading libraries
2022-10-21 16:33:24,642:INFO:Initializing save_model()
2022-10-21 16:33:24,642:INFO:save_model(model=GradientBoostingRegressor(random_state=123), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 16:33:24,642:INFO:Adding model into prep_pipe
2022-10-21 16:33:24,660:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 16:33:24,729:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=123))])
2022-10-21 16:33:24,730:INFO:save_model() successfully completed......................................
2022-10-21 16:33:52,589:INFO:Initializing plot_model()
2022-10-21 16:33:52,591:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca007a760>, system=True)
2022-10-21 16:33:52,591:INFO:Checking exceptions
2022-10-21 16:33:52,595:INFO:Preloading libraries
2022-10-21 16:33:52,610:INFO:Copying training dataset
2022-10-21 16:33:52,611:INFO:Plot type: parameter
2022-10-21 16:33:52,618:INFO:Visual Rendered Successfully
2022-10-21 16:33:52,781:INFO:plot_model() successfully completed......................................
2022-10-21 16:34:59,165:INFO:PyCaret RegressionExperiment
2022-10-21 16:34:59,165:INFO:Logging name: reg-default-name
2022-10-21 16:34:59,166:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 16:34:59,166:INFO:version 3.0.0.rc4
2022-10-21 16:34:59,166:INFO:Initializing setup()
2022-10-21 16:34:59,166:INFO:self.USI: 22f0
2022-10-21 16:34:59,166:INFO:self.variable_keys: {'_available_plots', '_all_metrics', '_all_models_internal', 'X_train', 'display_container', 'gpu_param', 'idx', 'transform_target_method_param', 'html_param', 'exp_id', 'exp_name_log', 'USI', 'log_plots_param', 'variable_keys', 'seed', 'n_jobs_param', 'y_test', 'y_train', '_all_models', 'transform_target_param', '_ml_usecase', 'logging_param', 'data', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'X', 'fold_generator', 'y', 'master_model_container', 'X_test', 'target_param', '_gpu_n_jobs_param', 'pipeline'}
2022-10-21 16:34:59,166:INFO:Checking environment
2022-10-21 16:34:59,167:INFO:python_version: 3.9.7
2022-10-21 16:34:59,167:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 16:34:59,167:INFO:machine: x86_64
2022-10-21 16:34:59,167:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:34:59,167:INFO:Memory: svmem(total=8589934592, available=2411278336, percent=71.9, used=4251987968, free=30793728, active=2382958592, inactive=2376474624, wired=1869029376)
2022-10-21 16:34:59,167:INFO:Physical Core: 2
2022-10-21 16:34:59,167:INFO:Logical Core: 4
2022-10-21 16:34:59,167:INFO:Checking libraries
2022-10-21 16:34:59,167:INFO:System:
2022-10-21 16:34:59,167:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 16:34:59,167:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 16:34:59,168:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:34:59,168:INFO:PyCaret required dependencies:
2022-10-21 16:34:59,168:INFO:                 pip: 21.2.4
2022-10-21 16:34:59,168:INFO:          setuptools: 58.0.4
2022-10-21 16:34:59,168:INFO:             pycaret: 3.0.0rc4
2022-10-21 16:34:59,168:INFO:             IPython: 7.29.0
2022-10-21 16:34:59,168:INFO:          ipywidgets: 7.6.5
2022-10-21 16:34:59,168:INFO:                tqdm: 4.62.3
2022-10-21 16:34:59,168:INFO:               numpy: 1.22.4
2022-10-21 16:34:59,169:INFO:              pandas: 1.4.4
2022-10-21 16:34:59,169:INFO:              jinja2: 3.1.2
2022-10-21 16:34:59,169:INFO:               scipy: 1.8.1
2022-10-21 16:34:59,169:INFO:              joblib: 1.1.0
2022-10-21 16:34:59,169:INFO:             sklearn: 1.0.2
2022-10-21 16:34:59,169:INFO:                pyod: 1.0.5
2022-10-21 16:34:59,169:INFO:            imblearn: 0.9.0
2022-10-21 16:34:59,169:INFO:   category_encoders: 2.5.1.post0
2022-10-21 16:34:59,169:INFO:            lightgbm: 3.3.2
2022-10-21 16:34:59,169:INFO:               numba: 0.55.2
2022-10-21 16:34:59,169:INFO:            requests: 2.28.1
2022-10-21 16:34:59,169:INFO:          matplotlib: 3.4.3
2022-10-21 16:34:59,170:INFO:          scikitplot: 0.3.7
2022-10-21 16:34:59,170:INFO:         yellowbrick: 1.4
2022-10-21 16:34:59,170:INFO:              plotly: 5.5.0
2022-10-21 16:34:59,170:INFO:             kaleido: 0.2.1
2022-10-21 16:34:59,170:INFO:         statsmodels: 0.13.2
2022-10-21 16:34:59,170:INFO:              sktime: 0.13.4
2022-10-21 16:34:59,170:INFO:               tbats: 1.1.1
2022-10-21 16:34:59,170:INFO:            pmdarima: 1.8.5
2022-10-21 16:34:59,170:INFO:              psutil: 5.9.2
2022-10-21 16:34:59,170:INFO:PyCaret optional dependencies:
2022-10-21 16:34:59,171:INFO:                shap: 0.41.0
2022-10-21 16:34:59,171:INFO:           interpret: Not installed
2022-10-21 16:34:59,172:INFO:                umap: Not installed
2022-10-21 16:34:59,172:INFO:    pandas_profiling: Not installed
2022-10-21 16:34:59,172:INFO:  explainerdashboard: Not installed
2022-10-21 16:34:59,173:INFO:             autoviz: Not installed
2022-10-21 16:34:59,173:INFO:           fairlearn: Not installed
2022-10-21 16:34:59,173:INFO:             xgboost: Not installed
2022-10-21 16:34:59,173:INFO:            catboost: 1.1
2022-10-21 16:34:59,173:INFO:              kmodes: Not installed
2022-10-21 16:34:59,173:INFO:             mlxtend: Not installed
2022-10-21 16:34:59,173:INFO:       statsforecast: 1.1.1
2022-10-21 16:34:59,173:INFO:        tune_sklearn: Not installed
2022-10-21 16:34:59,173:INFO:                 ray: Not installed
2022-10-21 16:34:59,173:INFO:            hyperopt: Not installed
2022-10-21 16:34:59,173:INFO:              optuna: Not installed
2022-10-21 16:34:59,173:INFO:               skopt: Not installed
2022-10-21 16:34:59,173:INFO:              mlflow: 1.29.0
2022-10-21 16:34:59,173:INFO:              gradio: Not installed
2022-10-21 16:34:59,173:INFO:             fastapi: Not installed
2022-10-21 16:34:59,174:INFO:             uvicorn: Not installed
2022-10-21 16:34:59,174:INFO:              m2cgen: Not installed
2022-10-21 16:34:59,174:INFO:           evidently: Not installed
2022-10-21 16:34:59,174:INFO:                nltk: 3.6.5
2022-10-21 16:34:59,174:INFO:            pyLDAvis: Not installed
2022-10-21 16:34:59,174:INFO:              gensim: Not installed
2022-10-21 16:34:59,174:INFO:               spacy: Not installed
2022-10-21 16:34:59,174:INFO:           wordcloud: Not installed
2022-10-21 16:34:59,174:INFO:            textblob: Not installed
2022-10-21 16:34:59,174:INFO:               fugue: Not installed
2022-10-21 16:34:59,174:INFO:           streamlit: Not installed
2022-10-21 16:34:59,174:INFO:             prophet: 1.1.1
2022-10-21 16:34:59,174:INFO:None
2022-10-21 16:34:59,174:INFO:Set up data.
2022-10-21 16:34:59,183:INFO:Set up train/test split.
2022-10-21 16:34:59,195:INFO:Set up index.
2022-10-21 16:34:59,195:INFO:Set up folding strategy.
2022-10-21 16:34:59,196:INFO:Assigning column types.
2022-10-21 16:34:59,210:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 16:34:59,211:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,221:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,237:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,417:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,615:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,616:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:34:59,616:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:34:59,618:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,628:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,639:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,763:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,840:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,841:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:34:59,842:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:34:59,842:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 16:34:59,850:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,859:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:34:59,961:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,042:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,043:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:00,043:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:00,051:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,060:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,158:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,299:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,300:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:00,301:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:00,301:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 16:35:00,340:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,511:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,614:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,615:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:00,615:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:00,636:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,858:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,951:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:35:00,951:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:00,954:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:00,955:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 16:35:01,079:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:01,158:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:35:01,158:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:01,159:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:01,314:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:01,462:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:35:01,463:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:01,463:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:01,464:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 16:35:01,584:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:01,716:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:01,717:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:01,862:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:35:01,936:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:01,937:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:01,939:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 16:35:02,142:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:02,142:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:02,337:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:02,338:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:02,339:INFO:Preparing preprocessing pipeline...
2022-10-21 16:35:02,341:INFO:Set up simple imputation.
2022-10-21 16:35:02,346:INFO:Set up encoding of ordinal features.
2022-10-21 16:35:02,349:INFO:Set up encoding of categorical features.
2022-10-21 16:35:02,350:INFO:Set up polynomial features.
2022-10-21 16:35:02,350:INFO:Set up variance threshold.
2022-10-21 16:35:02,350:INFO:Set up feature normalization.
2022-10-21 16:35:02,518:INFO:Finished creating preprocessing pipeline.
2022-10-21 16:35:02,540:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2022-10-21 16:35:02,540:INFO:Creating final display dataframe.
2022-10-21 16:35:03,236:INFO:Setup display_container:                  Description             Value
0                 Session id               123
1                     Target             Label
2                Target type        Regression
3                 Data shape        (1338, 49)
4           Train data shape         (936, 49)
5            Test data shape         (402, 49)
6           Ordinal features                 2
7           Numeric features                 3
8       Categorical features                 3
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15       Polynomial features              True
16         Polynomial degree                 2
17    Low variance threshold                 0
18                 Normalize              True
19          Normalize method            zscore
20            Fold Generator             KFold
21               Fold Number                10
22                  CPU Jobs                -1
23                   Use GPU             False
24            Log Experiment             False
25           Experiment Name  reg-default-name
26                       USI              22f0
2022-10-21 16:35:03,502:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:03,502:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:03,729:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:35:03,729:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:35:03,742:INFO:setup() successfully completed in 4.58s...............
2022-10-21 16:35:03,743:INFO:Initializing compare_models()
2022-10-21 16:35:03,743:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-21 16:35:03,743:INFO:Checking exceptions
2022-10-21 16:35:03,747:INFO:Preparing display monitor
2022-10-21 16:35:03,901:INFO:Initializing Linear Regression
2022-10-21 16:35:03,901:INFO:Total runtime is 5.968411763509115e-06 minutes
2022-10-21 16:35:03,924:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:03,926:INFO:Initializing create_model()
2022-10-21 16:35:03,927:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:03,927:INFO:Checking exceptions
2022-10-21 16:35:03,930:INFO:Importing libraries
2022-10-21 16:35:03,930:INFO:Copying training dataset
2022-10-21 16:35:03,940:INFO:Defining folds
2022-10-21 16:35:03,940:INFO:Declaring metric variables
2022-10-21 16:35:03,950:INFO:Importing untrained model
2022-10-21 16:35:03,961:INFO:Linear Regression Imported successfully
2022-10-21 16:35:03,993:INFO:Starting cross validation
2022-10-21 16:35:03,997:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:04,229:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:04,314:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:04,327:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:04,344:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:05,346:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:05,394:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:05,397:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:05,553:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:06,213:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:06,300:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:06,309:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:06,395:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:06,934:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:06,958:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:06,988:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:07,083:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:07,618:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:07,622:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:07,978:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:07,979:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:35:08,278:INFO:Calculating mean and std
2022-10-21 16:35:08,280:INFO:Creating metrics dataframe
2022-10-21 16:35:08,285:INFO:Uploading results into container
2022-10-21 16:35:08,286:INFO:Uploading model into container now
2022-10-21 16:35:08,287:INFO:master_model_container: 1
2022-10-21 16:35:08,287:INFO:display_container: 2
2022-10-21 16:35:08,288:INFO:LinearRegression(n_jobs=-1)
2022-10-21 16:35:08,288:INFO:create_model() successfully completed......................................
2022-10-21 16:35:08,429:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:08,429:INFO:Creating metrics dataframe
2022-10-21 16:35:08,456:INFO:Initializing Lasso Regression
2022-10-21 16:35:08,456:INFO:Total runtime is 0.07591731548309326 minutes
2022-10-21 16:35:08,463:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:08,464:INFO:Initializing create_model()
2022-10-21 16:35:08,464:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:08,464:INFO:Checking exceptions
2022-10-21 16:35:08,468:INFO:Importing libraries
2022-10-21 16:35:08,468:INFO:Copying training dataset
2022-10-21 16:35:08,499:INFO:Defining folds
2022-10-21 16:35:08,499:INFO:Declaring metric variables
2022-10-21 16:35:08,518:INFO:Importing untrained model
2022-10-21 16:35:08,533:INFO:Lasso Regression Imported successfully
2022-10-21 16:35:08,564:INFO:Starting cross validation
2022-10-21 16:35:08,581:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:08,923:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+09, tolerance: 1.224e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:08,928:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.203e+09, tolerance: 1.228e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:08,933:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.475e+09, tolerance: 1.271e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:08,940:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.409e+09, tolerance: 1.305e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:09,249:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.186e+09, tolerance: 1.260e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:09,261:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.251e+09, tolerance: 1.240e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:09,269:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.193e+09, tolerance: 1.215e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:09,273:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.336e+09, tolerance: 1.250e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:09,476:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.351e+09, tolerance: 1.242e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:09,477:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+09, tolerance: 1.242e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:35:09,529:INFO:Calculating mean and std
2022-10-21 16:35:09,532:INFO:Creating metrics dataframe
2022-10-21 16:35:09,537:INFO:Uploading results into container
2022-10-21 16:35:09,539:INFO:Uploading model into container now
2022-10-21 16:35:09,540:INFO:master_model_container: 2
2022-10-21 16:35:09,540:INFO:display_container: 2
2022-10-21 16:35:09,541:INFO:Lasso(random_state=123)
2022-10-21 16:35:09,541:INFO:create_model() successfully completed......................................
2022-10-21 16:35:09,707:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:09,707:INFO:Creating metrics dataframe
2022-10-21 16:35:09,731:INFO:Initializing Ridge Regression
2022-10-21 16:35:09,731:INFO:Total runtime is 0.09716409842173258 minutes
2022-10-21 16:35:09,740:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:09,740:INFO:Initializing create_model()
2022-10-21 16:35:09,740:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:09,742:INFO:Checking exceptions
2022-10-21 16:35:09,748:INFO:Importing libraries
2022-10-21 16:35:09,748:INFO:Copying training dataset
2022-10-21 16:35:09,779:INFO:Defining folds
2022-10-21 16:35:09,780:INFO:Declaring metric variables
2022-10-21 16:35:09,801:INFO:Importing untrained model
2022-10-21 16:35:09,813:INFO:Ridge Regression Imported successfully
2022-10-21 16:35:09,834:INFO:Starting cross validation
2022-10-21 16:35:09,848:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:10,631:INFO:Calculating mean and std
2022-10-21 16:35:10,633:INFO:Creating metrics dataframe
2022-10-21 16:35:10,641:INFO:Uploading results into container
2022-10-21 16:35:10,642:INFO:Uploading model into container now
2022-10-21 16:35:10,643:INFO:master_model_container: 3
2022-10-21 16:35:10,644:INFO:display_container: 2
2022-10-21 16:35:10,644:INFO:Ridge(random_state=123)
2022-10-21 16:35:10,644:INFO:create_model() successfully completed......................................
2022-10-21 16:35:10,782:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:10,782:INFO:Creating metrics dataframe
2022-10-21 16:35:10,797:INFO:Initializing Elastic Net
2022-10-21 16:35:10,797:INFO:Total runtime is 0.11493680079778035 minutes
2022-10-21 16:35:10,802:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:10,803:INFO:Initializing create_model()
2022-10-21 16:35:10,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:10,803:INFO:Checking exceptions
2022-10-21 16:35:10,808:INFO:Importing libraries
2022-10-21 16:35:10,808:INFO:Copying training dataset
2022-10-21 16:35:10,818:INFO:Defining folds
2022-10-21 16:35:10,818:INFO:Declaring metric variables
2022-10-21 16:35:10,835:INFO:Importing untrained model
2022-10-21 16:35:10,880:INFO:Elastic Net Imported successfully
2022-10-21 16:35:10,933:INFO:Starting cross validation
2022-10-21 16:35:10,937:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:11,962:INFO:Calculating mean and std
2022-10-21 16:35:11,964:INFO:Creating metrics dataframe
2022-10-21 16:35:11,969:INFO:Uploading results into container
2022-10-21 16:35:11,970:INFO:Uploading model into container now
2022-10-21 16:35:11,971:INFO:master_model_container: 4
2022-10-21 16:35:11,971:INFO:display_container: 2
2022-10-21 16:35:11,972:INFO:ElasticNet(random_state=123)
2022-10-21 16:35:11,972:INFO:create_model() successfully completed......................................
2022-10-21 16:35:12,118:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:12,119:INFO:Creating metrics dataframe
2022-10-21 16:35:12,144:INFO:Initializing Least Angle Regression
2022-10-21 16:35:12,144:INFO:Total runtime is 0.13738580147425333 minutes
2022-10-21 16:35:12,150:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:12,151:INFO:Initializing create_model()
2022-10-21 16:35:12,151:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:12,151:INFO:Checking exceptions
2022-10-21 16:35:12,160:INFO:Importing libraries
2022-10-21 16:35:12,160:INFO:Copying training dataset
2022-10-21 16:35:12,184:INFO:Defining folds
2022-10-21 16:35:12,185:INFO:Declaring metric variables
2022-10-21 16:35:12,210:INFO:Importing untrained model
2022-10-21 16:35:12,225:INFO:Least Angle Regression Imported successfully
2022-10-21 16:35:12,264:INFO:Starting cross validation
2022-10-21 16:35:12,268:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:12,462:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,475:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.609e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,477:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.088e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,478:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.361e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,479:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.949e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,480:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.815e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,482:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.607e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,484:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.301e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,484:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.283e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,485:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.283e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,485:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.251e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,489:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,490:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.205e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,492:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.074e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,493:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.912e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,493:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.809e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,493:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,494:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.575e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,497:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=9.494e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,498:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.340e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,498:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.106e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,499:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,499:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=6.524e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,499:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=6.093e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,499:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.543e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,500:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.250e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,501:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.338e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,502:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.128e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,500:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.680e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,505:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.977e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,505:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.962e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,505:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.890e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,506:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.246e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,507:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.609e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,507:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.377e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,509:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.704e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,509:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.366e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,509:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.379e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,509:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.366e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,510:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.838e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,511:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.550e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,511:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.463e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,512:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.270e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,513:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.270e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,513:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.198e+01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,513:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.207e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,514:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.222e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,514:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.153e+01, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,515:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.903e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,515:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.589e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,516:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.711e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,516:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.164e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,516:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.711e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,516:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.649e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,517:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.324e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,517:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.612e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,517:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.495e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,517:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.813e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,518:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.245e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,518:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.495e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,518:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.758e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,518:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.654e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,518:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.065e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,518:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.797e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,518:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.608e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,519:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.206e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,519:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=7.149e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,520:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.818e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,519:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.019e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,521:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.798e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,522:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.333e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,523:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.749e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,523:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.991e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,523:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.778e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,524:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.352e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,524:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.776e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,524:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.352e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,525:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.763e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,525:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.726e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,525:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.966e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,525:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.261e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,525:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.429e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,525:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.208e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,526:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.180e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,526:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.077e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,526:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.315e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,527:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.079e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,527:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.284e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,527:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.102e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,527:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.006e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,527:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=9.850e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,527:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=9.600e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,528:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=9.956e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,528:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=8.632e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,528:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.235e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,528:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=6.595e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.092e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.047e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.162e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.863e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.878e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.384e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,529:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.754e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,530:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.189e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,530:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.751e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,530:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.708e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,530:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.338e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,530:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.251e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,531:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.731e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,531:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.251e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,531:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.284e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,531:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.328e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,531:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.792e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,531:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.896e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,531:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.427e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,532:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.713e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,532:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.228e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,532:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.913e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,532:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.961e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,533:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.391e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,533:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.450e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,533:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.309e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,533:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.067e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,534:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.190e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,534:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.869e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,535:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=7.580e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,535:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.105e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,535:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=6.603e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,535:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.105e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,535:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.179e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,536:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.059e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,536:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.394e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,541:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.662e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,542:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.622e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,542:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.594e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,543:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.026e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,544:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.007e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,544:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.674e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,544:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.402e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,545:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.474e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,545:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.128e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,547:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.127e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,547:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.043e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,548:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.127e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,548:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=7.724e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,548:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.662e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,551:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.078e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,551:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.222e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,552:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.207e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,552:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.296e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,556:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.296e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,558:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.100e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,558:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.062e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,559:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.062e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,560:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.441e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,782:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,791:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.366e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,797:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.941e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,800:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.039e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.011e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,802:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=8.967e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,803:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=6.920e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,804:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.475e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.161e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,808:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=4.940e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,808:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.590e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,809:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.287e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,809:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.175e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,809:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.163e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,810:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.471e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,810:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.116e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,811:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.451e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,812:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,812:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.073e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,813:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.063e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,814:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.626e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,815:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.623e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,815:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.258e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.723e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.682e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,817:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.495e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.057e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.612e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,826:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,828:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:12,830:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.391e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,831:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.085e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,832:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.427e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.427e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.818e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,834:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.716e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,835:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.229e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,835:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.603e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,836:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.603e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,837:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.722e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,839:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.680e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,840:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.772e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,842:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.384e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,843:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.130e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,843:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.220e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,846:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.758e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,846:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.399e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,846:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.399e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,846:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.626e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,847:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.653e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,847:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.166e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,848:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.026e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,848:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.741e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,848:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.575e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.121e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.725e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,849:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.121e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.326e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.064e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.686e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.520e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,851:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.894e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,851:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.143e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,853:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.626e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,854:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.404e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,857:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.040e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,853:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.402e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,857:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.372e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,858:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.314e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,858:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.566e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,858:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.566e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,859:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.266e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,859:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.680e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,859:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.872e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,860:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.751e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,860:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.077e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,860:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.090e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,861:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=8.235e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,861:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.507e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,861:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.753e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.553e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.638e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.977e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.610e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.726e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,862:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.137e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.976e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=8.762e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.848e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.093e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.865e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.029e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,863:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.833e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.710e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.974e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.621e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.494e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.090e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.078e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.117e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.117e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.625e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=4.962e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=4.265e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.581e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,866:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.418e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,866:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.178e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,866:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.936e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,867:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.510e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,868:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.087e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,870:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.450e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,871:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.232e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,874:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.525e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,875:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.506e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,876:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.368e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,876:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.196e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,877:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.110e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,877:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.539e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,878:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.372e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:12,878:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=2.469e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,048:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:13,054:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.345e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,057:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.133e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,059:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.296e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,060:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.738e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,061:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.738e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,062:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.195e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,062:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.194e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,062:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.101e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,063:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.686e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,063:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.895e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,063:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.589e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,064:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=6.456e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,064:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.073e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,064:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.586e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,064:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.970e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,065:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.927e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,065:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.388e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,065:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.033e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,065:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.674e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,065:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.391e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,066:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.058e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,066:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.047e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,066:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.019e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,067:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.242e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,067:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.746e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,067:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.403e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,067:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.335e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,067:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.321e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,068:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.302e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,068:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.286e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,069:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.235e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,069:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.925e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,080:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:13,083:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.585e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,085:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.690e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,092:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.470e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,092:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.470e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,093:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.356e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,093:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.257e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,093:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.220e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,093:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.176e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,094:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.401e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,095:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.401e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,095:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.909e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,096:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.438e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,096:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.438e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,096:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.345e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,097:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.274e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,097:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=4.826e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,098:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=4.589e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,098:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=4.379e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,099:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.001e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,099:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.158e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,100:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=7.825e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,100:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.345e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,100:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.095e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,100:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.062e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.208e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.528e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.595e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.315e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,165:INFO:Calculating mean and std
2022-10-21 16:35:13,168:INFO:Creating metrics dataframe
2022-10-21 16:35:13,175:INFO:Uploading results into container
2022-10-21 16:35:13,176:INFO:Uploading model into container now
2022-10-21 16:35:13,177:INFO:master_model_container: 5
2022-10-21 16:35:13,177:INFO:display_container: 2
2022-10-21 16:35:13,178:INFO:Lars(random_state=123)
2022-10-21 16:35:13,178:INFO:create_model() successfully completed......................................
2022-10-21 16:35:13,322:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:13,322:INFO:Creating metrics dataframe
2022-10-21 16:35:13,340:INFO:Initializing Lasso Least Angle Regression
2022-10-21 16:35:13,340:INFO:Total runtime is 0.1573156992594401 minutes
2022-10-21 16:35:13,346:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:13,347:INFO:Initializing create_model()
2022-10-21 16:35:13,347:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:13,347:INFO:Checking exceptions
2022-10-21 16:35:13,350:INFO:Importing libraries
2022-10-21 16:35:13,350:INFO:Copying training dataset
2022-10-21 16:35:13,384:INFO:Defining folds
2022-10-21 16:35:13,384:INFO:Declaring metric variables
2022-10-21 16:35:13,399:INFO:Importing untrained model
2022-10-21 16:35:13,416:INFO:Lasso Least Angle Regression Imported successfully
2022-10-21 16:35:13,435:INFO:Starting cross validation
2022-10-21 16:35:13,457:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:13,674:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:13,680:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.609e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,687:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.358e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,688:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.330e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,694:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.261e+00, previous alpha=1.732e+00, with an active set of 23 regressors.
  warnings.warn(

2022-10-21 16:35:13,694:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:13,700:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.910e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,701:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:13,701:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.130e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,703:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.033e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,707:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=5.069e+00, previous alpha=4.920e+00, with an active set of 15 regressors.
  warnings.warn(

2022-10-21 16:35:13,710:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.220e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,717:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.188e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,717:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:13,720:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=2.040e+00, previous alpha=1.875e+00, with an active set of 23 regressors.
  warnings.warn(

2022-10-21 16:35:13,727:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.082e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,733:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.775e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,736:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.599e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,743:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.416e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,745:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.270e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,745:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.270e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,746:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.236e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,939:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:13,943:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:13,947:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.969e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,948:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.366e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,954:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.002e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,955:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.002e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,963:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.112e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,963:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.112e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,964:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.018e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,965:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.564e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,966:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.564e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:13,968:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.351e+00, previous alpha=1.213e+00, with an active set of 29 regressors.
  warnings.warn(

2022-10-21 16:35:13,995:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:13,996:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:14,000:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.716e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,007:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.405e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,008:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.335e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,011:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.162e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,013:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.319e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,013:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.764e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,013:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.319e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,014:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.287e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,014:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.755e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,014:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.740e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,016:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.632e+00, previous alpha=1.585e+00, with an active set of 23 regressors.
  warnings.warn(

2022-10-21 16:35:14,146:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:14,148:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:35:14,151:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.665e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,151:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.585e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,152:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.934e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,152:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.934e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,153:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.511e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,154:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.511e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,155:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.448e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,155:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.066e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,156:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.867e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,157:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.774e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,158:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.554e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,160:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.406e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,160:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=2.488e+00, previous alpha=2.202e+00, with an active set of 19 regressors.
  warnings.warn(

2022-10-21 16:35:14,160:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.376e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:35:14,163:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.381e+00, previous alpha=1.193e+00, with an active set of 23 regressors.
  warnings.warn(

2022-10-21 16:35:14,209:INFO:Calculating mean and std
2022-10-21 16:35:14,211:INFO:Creating metrics dataframe
2022-10-21 16:35:14,216:INFO:Uploading results into container
2022-10-21 16:35:14,217:INFO:Uploading model into container now
2022-10-21 16:35:14,218:INFO:master_model_container: 6
2022-10-21 16:35:14,218:INFO:display_container: 2
2022-10-21 16:35:14,219:INFO:LassoLars(random_state=123)
2022-10-21 16:35:14,219:INFO:create_model() successfully completed......................................
2022-10-21 16:35:14,364:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:14,364:INFO:Creating metrics dataframe
2022-10-21 16:35:14,383:INFO:Initializing Orthogonal Matching Pursuit
2022-10-21 16:35:14,383:INFO:Total runtime is 0.17469701369603474 minutes
2022-10-21 16:35:14,394:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:14,394:INFO:Initializing create_model()
2022-10-21 16:35:14,394:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:14,394:INFO:Checking exceptions
2022-10-21 16:35:14,397:INFO:Importing libraries
2022-10-21 16:35:14,398:INFO:Copying training dataset
2022-10-21 16:35:14,419:INFO:Defining folds
2022-10-21 16:35:14,420:INFO:Declaring metric variables
2022-10-21 16:35:14,445:INFO:Importing untrained model
2022-10-21 16:35:14,452:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-21 16:35:14,503:INFO:Starting cross validation
2022-10-21 16:35:14,507:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:14,726:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:14,734:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:14,744:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:14,761:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:14,983:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:15,002:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:15,012:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:15,030:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:15,191:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:15,197:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:35:15,250:INFO:Calculating mean and std
2022-10-21 16:35:15,253:INFO:Creating metrics dataframe
2022-10-21 16:35:15,260:INFO:Uploading results into container
2022-10-21 16:35:15,261:INFO:Uploading model into container now
2022-10-21 16:35:15,262:INFO:master_model_container: 7
2022-10-21 16:35:15,262:INFO:display_container: 2
2022-10-21 16:35:15,262:INFO:OrthogonalMatchingPursuit()
2022-10-21 16:35:15,262:INFO:create_model() successfully completed......................................
2022-10-21 16:35:15,410:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:15,410:INFO:Creating metrics dataframe
2022-10-21 16:35:15,428:INFO:Initializing Bayesian Ridge
2022-10-21 16:35:15,428:INFO:Total runtime is 0.19211213191350301 minutes
2022-10-21 16:35:15,432:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:15,433:INFO:Initializing create_model()
2022-10-21 16:35:15,433:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:15,433:INFO:Checking exceptions
2022-10-21 16:35:15,437:INFO:Importing libraries
2022-10-21 16:35:15,437:INFO:Copying training dataset
2022-10-21 16:35:15,445:INFO:Defining folds
2022-10-21 16:35:15,445:INFO:Declaring metric variables
2022-10-21 16:35:15,469:INFO:Importing untrained model
2022-10-21 16:35:15,494:INFO:Bayesian Ridge Imported successfully
2022-10-21 16:35:15,535:INFO:Starting cross validation
2022-10-21 16:35:15,547:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:16,320:INFO:Calculating mean and std
2022-10-21 16:35:16,324:INFO:Creating metrics dataframe
2022-10-21 16:35:16,328:INFO:Uploading results into container
2022-10-21 16:35:16,329:INFO:Uploading model into container now
2022-10-21 16:35:16,330:INFO:master_model_container: 8
2022-10-21 16:35:16,330:INFO:display_container: 2
2022-10-21 16:35:16,331:INFO:BayesianRidge()
2022-10-21 16:35:16,331:INFO:create_model() successfully completed......................................
2022-10-21 16:35:16,473:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:16,473:INFO:Creating metrics dataframe
2022-10-21 16:35:16,489:INFO:Initializing Passive Aggressive Regressor
2022-10-21 16:35:16,490:INFO:Total runtime is 0.2098112185796102 minutes
2022-10-21 16:35:16,495:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:16,496:INFO:Initializing create_model()
2022-10-21 16:35:16,496:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:16,496:INFO:Checking exceptions
2022-10-21 16:35:16,499:INFO:Importing libraries
2022-10-21 16:35:16,500:INFO:Copying training dataset
2022-10-21 16:35:16,510:INFO:Defining folds
2022-10-21 16:35:16,511:INFO:Declaring metric variables
2022-10-21 16:35:16,537:INFO:Importing untrained model
2022-10-21 16:35:16,548:INFO:Passive Aggressive Regressor Imported successfully
2022-10-21 16:35:16,569:INFO:Starting cross validation
2022-10-21 16:35:16,571:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:18,397:INFO:Calculating mean and std
2022-10-21 16:35:18,399:INFO:Creating metrics dataframe
2022-10-21 16:35:18,405:INFO:Uploading results into container
2022-10-21 16:35:18,407:INFO:Uploading model into container now
2022-10-21 16:35:18,408:INFO:master_model_container: 9
2022-10-21 16:35:18,408:INFO:display_container: 2
2022-10-21 16:35:18,408:INFO:PassiveAggressiveRegressor(random_state=123)
2022-10-21 16:35:18,409:INFO:create_model() successfully completed......................................
2022-10-21 16:35:18,559:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:18,560:INFO:Creating metrics dataframe
2022-10-21 16:35:18,578:INFO:Initializing Huber Regressor
2022-10-21 16:35:18,578:INFO:Total runtime is 0.24462206761042277 minutes
2022-10-21 16:35:18,584:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:18,584:INFO:Initializing create_model()
2022-10-21 16:35:18,585:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:18,585:INFO:Checking exceptions
2022-10-21 16:35:18,589:INFO:Importing libraries
2022-10-21 16:35:18,590:INFO:Copying training dataset
2022-10-21 16:35:18,600:INFO:Defining folds
2022-10-21 16:35:18,601:INFO:Declaring metric variables
2022-10-21 16:35:18,614:INFO:Importing untrained model
2022-10-21 16:35:18,635:INFO:Huber Regressor Imported successfully
2022-10-21 16:35:18,664:INFO:Starting cross validation
2022-10-21 16:35:18,666:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:19,112:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:19,127:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:19,159:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:19,164:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:19,557:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:19,576:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:19,615:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:19,633:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:20,223:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:20,235:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:35:20,299:INFO:Calculating mean and std
2022-10-21 16:35:20,302:INFO:Creating metrics dataframe
2022-10-21 16:35:20,312:INFO:Uploading results into container
2022-10-21 16:35:20,314:INFO:Uploading model into container now
2022-10-21 16:35:20,314:INFO:master_model_container: 10
2022-10-21 16:35:20,315:INFO:display_container: 2
2022-10-21 16:35:20,315:INFO:HuberRegressor()
2022-10-21 16:35:20,315:INFO:create_model() successfully completed......................................
2022-10-21 16:35:20,484:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:20,484:INFO:Creating metrics dataframe
2022-10-21 16:35:20,509:INFO:Initializing K Neighbors Regressor
2022-10-21 16:35:20,509:INFO:Total runtime is 0.276798415184021 minutes
2022-10-21 16:35:20,517:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:20,517:INFO:Initializing create_model()
2022-10-21 16:35:20,517:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:20,517:INFO:Checking exceptions
2022-10-21 16:35:20,523:INFO:Importing libraries
2022-10-21 16:35:20,524:INFO:Copying training dataset
2022-10-21 16:35:20,545:INFO:Defining folds
2022-10-21 16:35:20,546:INFO:Declaring metric variables
2022-10-21 16:35:20,579:INFO:Importing untrained model
2022-10-21 16:35:20,613:INFO:K Neighbors Regressor Imported successfully
2022-10-21 16:35:20,664:INFO:Starting cross validation
2022-10-21 16:35:20,670:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:23,024:INFO:Calculating mean and std
2022-10-21 16:35:23,030:INFO:Creating metrics dataframe
2022-10-21 16:35:23,036:INFO:Uploading results into container
2022-10-21 16:35:23,038:INFO:Uploading model into container now
2022-10-21 16:35:23,039:INFO:master_model_container: 11
2022-10-21 16:35:23,039:INFO:display_container: 2
2022-10-21 16:35:23,040:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-21 16:35:23,040:INFO:create_model() successfully completed......................................
2022-10-21 16:35:23,824:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:23,824:INFO:Creating metrics dataframe
2022-10-21 16:35:23,895:INFO:Initializing Decision Tree Regressor
2022-10-21 16:35:23,895:INFO:Total runtime is 0.3332392652829488 minutes
2022-10-21 16:35:23,951:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:23,959:INFO:Initializing create_model()
2022-10-21 16:35:23,959:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:23,959:INFO:Checking exceptions
2022-10-21 16:35:23,962:INFO:Importing libraries
2022-10-21 16:35:23,962:INFO:Copying training dataset
2022-10-21 16:35:24,011:INFO:Defining folds
2022-10-21 16:35:24,016:INFO:Declaring metric variables
2022-10-21 16:35:24,044:INFO:Importing untrained model
2022-10-21 16:35:24,119:INFO:Decision Tree Regressor Imported successfully
2022-10-21 16:35:24,194:INFO:Starting cross validation
2022-10-21 16:35:24,202:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:26,479:INFO:Calculating mean and std
2022-10-21 16:35:26,482:INFO:Creating metrics dataframe
2022-10-21 16:35:26,491:INFO:Uploading results into container
2022-10-21 16:35:26,492:INFO:Uploading model into container now
2022-10-21 16:35:26,494:INFO:master_model_container: 12
2022-10-21 16:35:26,494:INFO:display_container: 2
2022-10-21 16:35:26,495:INFO:DecisionTreeRegressor(random_state=123)
2022-10-21 16:35:26,495:INFO:create_model() successfully completed......................................
2022-10-21 16:35:26,835:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:26,835:INFO:Creating metrics dataframe
2022-10-21 16:35:26,878:INFO:Initializing Random Forest Regressor
2022-10-21 16:35:26,878:INFO:Total runtime is 0.3829549670219421 minutes
2022-10-21 16:35:26,947:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:26,950:INFO:Initializing create_model()
2022-10-21 16:35:26,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:26,950:INFO:Checking exceptions
2022-10-21 16:35:26,965:INFO:Importing libraries
2022-10-21 16:35:26,966:INFO:Copying training dataset
2022-10-21 16:35:27,011:INFO:Defining folds
2022-10-21 16:35:27,011:INFO:Declaring metric variables
2022-10-21 16:35:27,032:INFO:Importing untrained model
2022-10-21 16:35:27,078:INFO:Random Forest Regressor Imported successfully
2022-10-21 16:35:27,135:INFO:Starting cross validation
2022-10-21 16:35:27,146:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:35,028:INFO:Calculating mean and std
2022-10-21 16:35:35,031:INFO:Creating metrics dataframe
2022-10-21 16:35:35,041:INFO:Uploading results into container
2022-10-21 16:35:35,042:INFO:Uploading model into container now
2022-10-21 16:35:35,045:INFO:master_model_container: 13
2022-10-21 16:35:35,045:INFO:display_container: 2
2022-10-21 16:35:35,045:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2022-10-21 16:35:35,045:INFO:create_model() successfully completed......................................
2022-10-21 16:35:35,299:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:35,299:INFO:Creating metrics dataframe
2022-10-21 16:35:35,329:INFO:Initializing Extra Trees Regressor
2022-10-21 16:35:35,329:INFO:Total runtime is 0.5238009333610534 minutes
2022-10-21 16:35:35,347:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:35,362:INFO:Initializing create_model()
2022-10-21 16:35:35,362:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:35,365:INFO:Checking exceptions
2022-10-21 16:35:35,435:INFO:Importing libraries
2022-10-21 16:35:35,435:INFO:Copying training dataset
2022-10-21 16:35:35,464:INFO:Defining folds
2022-10-21 16:35:35,465:INFO:Declaring metric variables
2022-10-21 16:35:35,485:INFO:Importing untrained model
2022-10-21 16:35:35,502:INFO:Extra Trees Regressor Imported successfully
2022-10-21 16:35:35,568:INFO:Starting cross validation
2022-10-21 16:35:35,578:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:42,546:INFO:Calculating mean and std
2022-10-21 16:35:42,550:INFO:Creating metrics dataframe
2022-10-21 16:35:42,555:INFO:Uploading results into container
2022-10-21 16:35:42,558:INFO:Uploading model into container now
2022-10-21 16:35:42,559:INFO:master_model_container: 14
2022-10-21 16:35:42,559:INFO:display_container: 2
2022-10-21 16:35:42,560:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2022-10-21 16:35:42,560:INFO:create_model() successfully completed......................................
2022-10-21 16:35:42,718:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:42,718:INFO:Creating metrics dataframe
2022-10-21 16:35:42,748:INFO:Initializing AdaBoost Regressor
2022-10-21 16:35:42,748:INFO:Total runtime is 0.6474503676096597 minutes
2022-10-21 16:35:42,754:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:42,754:INFO:Initializing create_model()
2022-10-21 16:35:42,754:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:42,754:INFO:Checking exceptions
2022-10-21 16:35:42,761:INFO:Importing libraries
2022-10-21 16:35:42,762:INFO:Copying training dataset
2022-10-21 16:35:42,769:INFO:Defining folds
2022-10-21 16:35:42,769:INFO:Declaring metric variables
2022-10-21 16:35:42,778:INFO:Importing untrained model
2022-10-21 16:35:42,792:INFO:AdaBoost Regressor Imported successfully
2022-10-21 16:35:42,806:INFO:Starting cross validation
2022-10-21 16:35:42,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:44,459:INFO:Calculating mean and std
2022-10-21 16:35:44,462:INFO:Creating metrics dataframe
2022-10-21 16:35:44,467:INFO:Uploading results into container
2022-10-21 16:35:44,467:INFO:Uploading model into container now
2022-10-21 16:35:44,468:INFO:master_model_container: 15
2022-10-21 16:35:44,468:INFO:display_container: 2
2022-10-21 16:35:44,469:INFO:AdaBoostRegressor(random_state=123)
2022-10-21 16:35:44,469:INFO:create_model() successfully completed......................................
2022-10-21 16:35:44,664:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:44,665:INFO:Creating metrics dataframe
2022-10-21 16:35:44,692:INFO:Initializing Gradient Boosting Regressor
2022-10-21 16:35:44,693:INFO:Total runtime is 0.6798605481783547 minutes
2022-10-21 16:35:44,701:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:44,702:INFO:Initializing create_model()
2022-10-21 16:35:44,702:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:44,702:INFO:Checking exceptions
2022-10-21 16:35:44,714:INFO:Importing libraries
2022-10-21 16:35:44,714:INFO:Copying training dataset
2022-10-21 16:35:44,766:INFO:Defining folds
2022-10-21 16:35:44,766:INFO:Declaring metric variables
2022-10-21 16:35:44,795:INFO:Importing untrained model
2022-10-21 16:35:44,801:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:35:44,845:INFO:Starting cross validation
2022-10-21 16:35:44,851:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:48,243:INFO:Calculating mean and std
2022-10-21 16:35:48,247:INFO:Creating metrics dataframe
2022-10-21 16:35:48,252:INFO:Uploading results into container
2022-10-21 16:35:48,252:INFO:Uploading model into container now
2022-10-21 16:35:48,254:INFO:master_model_container: 16
2022-10-21 16:35:48,254:INFO:display_container: 2
2022-10-21 16:35:48,254:INFO:GradientBoostingRegressor(random_state=123)
2022-10-21 16:35:48,254:INFO:create_model() successfully completed......................................
2022-10-21 16:35:48,429:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:48,430:INFO:Creating metrics dataframe
2022-10-21 16:35:48,453:INFO:Initializing Light Gradient Boosting Machine
2022-10-21 16:35:48,453:INFO:Total runtime is 0.7425305008888243 minutes
2022-10-21 16:35:48,464:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:48,465:INFO:Initializing create_model()
2022-10-21 16:35:48,465:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:48,465:INFO:Checking exceptions
2022-10-21 16:35:48,468:INFO:Importing libraries
2022-10-21 16:35:48,468:INFO:Copying training dataset
2022-10-21 16:35:48,485:INFO:Defining folds
2022-10-21 16:35:48,485:INFO:Declaring metric variables
2022-10-21 16:35:48,532:INFO:Importing untrained model
2022-10-21 16:35:48,563:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-21 16:35:48,597:INFO:Starting cross validation
2022-10-21 16:35:48,617:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:35:50,929:INFO:Calculating mean and std
2022-10-21 16:35:50,932:INFO:Creating metrics dataframe
2022-10-21 16:35:50,938:INFO:Uploading results into container
2022-10-21 16:35:50,940:INFO:Uploading model into container now
2022-10-21 16:35:50,941:INFO:master_model_container: 17
2022-10-21 16:35:50,941:INFO:display_container: 2
2022-10-21 16:35:50,942:INFO:LGBMRegressor(random_state=123)
2022-10-21 16:35:50,942:INFO:create_model() successfully completed......................................
2022-10-21 16:35:51,150:INFO:SubProcess create_model() end ==================================
2022-10-21 16:35:51,151:INFO:Creating metrics dataframe
2022-10-21 16:35:51,174:INFO:Initializing CatBoost Regressor
2022-10-21 16:35:51,174:INFO:Total runtime is 0.7878833651542663 minutes
2022-10-21 16:35:51,185:INFO:SubProcess create_model() called ==================================
2022-10-21 16:35:51,186:INFO:Initializing create_model()
2022-10-21 16:35:51,186:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:35:51,186:INFO:Checking exceptions
2022-10-21 16:35:51,191:INFO:Importing libraries
2022-10-21 16:35:51,191:INFO:Copying training dataset
2022-10-21 16:35:51,216:INFO:Defining folds
2022-10-21 16:35:51,216:INFO:Declaring metric variables
2022-10-21 16:35:51,263:INFO:Importing untrained model
2022-10-21 16:35:51,317:INFO:CatBoost Regressor Imported successfully
2022-10-21 16:35:51,384:INFO:Starting cross validation
2022-10-21 16:35:51,389:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:36:33,269:INFO:Calculating mean and std
2022-10-21 16:36:33,276:INFO:Creating metrics dataframe
2022-10-21 16:36:33,282:INFO:Uploading results into container
2022-10-21 16:36:33,283:INFO:Uploading model into container now
2022-10-21 16:36:33,284:INFO:master_model_container: 18
2022-10-21 16:36:33,284:INFO:display_container: 2
2022-10-21 16:36:33,284:INFO:<catboost.core.CatBoostRegressor object at 0x7faca00bbd00>
2022-10-21 16:36:33,284:INFO:create_model() successfully completed......................................
2022-10-21 16:36:34,215:INFO:SubProcess create_model() end ==================================
2022-10-21 16:36:34,218:INFO:Creating metrics dataframe
2022-10-21 16:36:34,307:INFO:Initializing Dummy Regressor
2022-10-21 16:36:34,309:INFO:Total runtime is 1.5067927837371826 minutes
2022-10-21 16:36:34,328:INFO:SubProcess create_model() called ==================================
2022-10-21 16:36:34,329:INFO:Initializing create_model()
2022-10-21 16:36:34,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca18d43a0>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:36:34,329:INFO:Checking exceptions
2022-10-21 16:36:34,342:INFO:Importing libraries
2022-10-21 16:36:34,342:INFO:Copying training dataset
2022-10-21 16:36:34,420:INFO:Defining folds
2022-10-21 16:36:34,441:INFO:Declaring metric variables
2022-10-21 16:36:34,476:INFO:Importing untrained model
2022-10-21 16:36:34,510:INFO:Dummy Regressor Imported successfully
2022-10-21 16:36:34,574:INFO:Starting cross validation
2022-10-21 16:36:34,587:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:36:36,223:INFO:Calculating mean and std
2022-10-21 16:36:36,227:INFO:Creating metrics dataframe
2022-10-21 16:36:36,233:INFO:Uploading results into container
2022-10-21 16:36:36,234:INFO:Uploading model into container now
2022-10-21 16:36:36,234:INFO:master_model_container: 19
2022-10-21 16:36:36,235:INFO:display_container: 2
2022-10-21 16:36:36,235:INFO:DummyRegressor()
2022-10-21 16:36:36,235:INFO:create_model() successfully completed......................................
2022-10-21 16:36:36,403:INFO:SubProcess create_model() end ==================================
2022-10-21 16:36:36,403:INFO:Creating metrics dataframe
2022-10-21 16:36:36,495:INFO:Initializing create_model()
2022-10-21 16:36:36,495:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:36:36,495:INFO:Checking exceptions
2022-10-21 16:36:36,503:INFO:Importing libraries
2022-10-21 16:36:36,503:INFO:Copying training dataset
2022-10-21 16:36:36,554:INFO:Defining folds
2022-10-21 16:36:36,559:INFO:Declaring metric variables
2022-10-21 16:36:36,560:INFO:Importing untrained model
2022-10-21 16:36:36,561:INFO:Declaring custom model
2022-10-21 16:36:36,562:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:36:36,570:INFO:Cross validation set to False
2022-10-21 16:36:36,570:INFO:Fitting Model
2022-10-21 16:36:37,418:INFO:GradientBoostingRegressor(random_state=123)
2022-10-21 16:36:37,418:INFO:create_model() successfully completed......................................
2022-10-21 16:36:37,913:INFO:master_model_container: 19
2022-10-21 16:36:37,914:INFO:display_container: 2
2022-10-21 16:36:37,915:INFO:GradientBoostingRegressor(random_state=123)
2022-10-21 16:36:37,915:INFO:compare_models() successfully completed......................................
2022-10-21 16:36:37,920:INFO:Initializing evaluate_model()
2022-10-21 16:36:37,920:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=GradientBoostingRegressor(random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-21 16:36:38,095:INFO:Initializing plot_model()
2022-10-21 16:36:38,096:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, system=True)
2022-10-21 16:36:38,096:INFO:Checking exceptions
2022-10-21 16:36:38,131:INFO:Preloading libraries
2022-10-21 16:36:38,178:INFO:Copying training dataset
2022-10-21 16:36:38,178:INFO:Plot type: pipeline
2022-10-21 16:36:38,791:INFO:Visual Rendered Successfully
2022-10-21 16:36:38,996:INFO:plot_model() successfully completed......................................
2022-10-21 16:36:38,999:INFO:Initializing predict_model()
2022-10-21 16:36:39,000:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca02fcee0>)
2022-10-21 16:36:39,001:INFO:Checking exceptions
2022-10-21 16:36:39,001:INFO:Preloading libraries
2022-10-21 16:36:39,431:INFO:Initializing save_model()
2022-10-21 16:36:39,431:INFO:save_model(model=GradientBoostingRegressor(random_state=123), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 16:36:39,431:INFO:Adding model into prep_pipe
2022-10-21 16:36:39,460:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 16:36:39,488:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=123))])
2022-10-21 16:36:39,488:INFO:save_model() successfully completed......................................
2022-10-21 16:36:47,814:INFO:Initializing plot_model()
2022-10-21 16:36:47,816:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, system=True)
2022-10-21 16:36:47,817:INFO:Checking exceptions
2022-10-21 16:36:47,825:INFO:Preloading libraries
2022-10-21 16:36:47,853:INFO:Copying training dataset
2022-10-21 16:36:47,853:INFO:Plot type: parameter
2022-10-21 16:36:47,869:INFO:Visual Rendered Successfully
2022-10-21 16:36:48,172:INFO:plot_model() successfully completed......................................
2022-10-21 16:36:54,132:INFO:Initializing plot_model()
2022-10-21 16:36:54,132:INFO:plot_model(plot=residuals, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, system=True)
2022-10-21 16:36:54,133:INFO:Checking exceptions
2022-10-21 16:36:54,136:INFO:Preloading libraries
2022-10-21 16:36:54,155:INFO:Copying training dataset
2022-10-21 16:36:54,155:INFO:Plot type: residuals
2022-10-21 16:36:54,568:INFO:Fitting Model
2022-10-21 16:36:54,635:INFO:Scoring test/hold-out set
2022-10-21 16:36:55,767:INFO:Visual Rendered Successfully
2022-10-21 16:36:55,960:INFO:plot_model() successfully completed......................................
2022-10-21 16:36:56,010:INFO:Initializing plot_model()
2022-10-21 16:36:56,013:INFO:plot_model(plot=residuals_interactive, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, system=True)
2022-10-21 16:36:56,013:INFO:Checking exceptions
2022-10-21 16:36:56,020:INFO:Preloading libraries
2022-10-21 16:36:56,045:INFO:Copying training dataset
2022-10-21 16:36:56,045:INFO:Plot type: residuals_interactive
2022-10-21 16:36:56,278:INFO:Calculated model residuals
2022-10-21 16:36:59,132:INFO:Calculated Tunkey-Anscombe Plot
2022-10-21 16:36:59,609:INFO:Calculated Normal QQ Plot
2022-10-21 16:36:59,898:INFO:Calculated Scale-Location Plot
2022-10-21 16:37:07,794:INFO:Initializing plot_model()
2022-10-21 16:37:07,796:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca18d4400>, system=True)
2022-10-21 16:37:07,796:INFO:Checking exceptions
2022-10-21 16:37:07,800:INFO:Preloading libraries
2022-10-21 16:37:07,818:INFO:Copying training dataset
2022-10-21 16:37:07,818:INFO:Plot type: error
2022-10-21 16:37:08,035:INFO:Fitting Model
2022-10-21 16:37:08,035:INFO:Scoring test/hold-out set
2022-10-21 16:37:08,379:INFO:Visual Rendered Successfully
2022-10-21 16:37:08,516:INFO:plot_model() successfully completed......................................
2022-10-21 16:37:23,699:INFO:Initializing predict_model()
2022-10-21 16:37:23,712:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9c357beca0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f9c3618a3a0>)
2022-10-21 16:37:23,713:INFO:Checking exceptions
2022-10-21 16:37:23,713:INFO:Preloading libraries
2022-10-21 16:37:23,722:INFO:Set up data.
2022-10-21 16:37:23,787:INFO:Set up index.
2022-10-21 16:37:47,531:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:37:47,531:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:37:47,531:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:37:47,531:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:37:50,178:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 16:37:51,426:INFO:Initializing load_model()
2022-10-21 16:37:51,426:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 16:38:06,392:INFO:Initializing predict_model()
2022-10-21 16:38:06,393:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fb8387e5b20>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fb8387af1f0>)
2022-10-21 16:38:06,393:INFO:Checking exceptions
2022-10-21 16:38:06,393:INFO:Preloading libraries
2022-10-21 16:38:06,394:INFO:Set up data.
2022-10-21 16:38:06,412:INFO:Set up index.
2022-10-21 16:38:06,668:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)

2022-10-21 16:38:45,591:INFO:PyCaret RegressionExperiment
2022-10-21 16:38:45,596:INFO:Logging name: reg-default-name
2022-10-21 16:38:45,598:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 16:38:45,598:INFO:version 3.0.0.rc4
2022-10-21 16:38:45,598:INFO:Initializing setup()
2022-10-21 16:38:45,598:INFO:self.USI: ef30
2022-10-21 16:38:45,598:INFO:self.variable_keys: {'_available_plots', '_all_metrics', '_all_models_internal', 'X_train', 'display_container', 'gpu_param', 'idx', 'transform_target_method_param', 'html_param', 'exp_id', 'exp_name_log', 'USI', 'log_plots_param', 'variable_keys', 'seed', 'n_jobs_param', 'y_test', 'y_train', '_all_models', 'transform_target_param', '_ml_usecase', 'logging_param', 'data', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'X', 'fold_generator', 'y', 'master_model_container', 'X_test', 'target_param', '_gpu_n_jobs_param', 'pipeline'}
2022-10-21 16:38:45,599:INFO:Checking environment
2022-10-21 16:38:45,599:INFO:python_version: 3.9.7
2022-10-21 16:38:45,600:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 16:38:45,600:INFO:machine: x86_64
2022-10-21 16:38:45,600:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:38:45,600:INFO:Memory: svmem(total=8589934592, available=2307784704, percent=73.1, used=4214067200, free=19111936, active=2290446336, inactive=2287661056, wired=1923620864)
2022-10-21 16:38:45,600:INFO:Physical Core: 2
2022-10-21 16:38:45,600:INFO:Logical Core: 4
2022-10-21 16:38:45,600:INFO:Checking libraries
2022-10-21 16:38:45,601:INFO:System:
2022-10-21 16:38:45,601:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 16:38:45,601:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 16:38:45,601:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:38:45,603:INFO:PyCaret required dependencies:
2022-10-21 16:38:45,603:INFO:                 pip: 21.2.4
2022-10-21 16:38:45,603:INFO:          setuptools: 58.0.4
2022-10-21 16:38:45,604:INFO:             pycaret: 3.0.0rc4
2022-10-21 16:38:45,604:INFO:             IPython: 7.29.0
2022-10-21 16:38:45,604:INFO:          ipywidgets: 7.6.5
2022-10-21 16:38:45,605:INFO:                tqdm: 4.62.3
2022-10-21 16:38:45,605:INFO:               numpy: 1.22.4
2022-10-21 16:38:45,605:INFO:              pandas: 1.4.4
2022-10-21 16:38:45,605:INFO:              jinja2: 3.1.2
2022-10-21 16:38:45,605:INFO:               scipy: 1.8.1
2022-10-21 16:38:45,605:INFO:              joblib: 1.1.0
2022-10-21 16:38:45,606:INFO:             sklearn: 1.0.2
2022-10-21 16:38:45,606:INFO:                pyod: 1.0.5
2022-10-21 16:38:45,606:INFO:            imblearn: 0.9.0
2022-10-21 16:38:45,606:INFO:   category_encoders: 2.5.1.post0
2022-10-21 16:38:45,607:INFO:            lightgbm: 3.3.2
2022-10-21 16:38:45,607:INFO:               numba: 0.55.2
2022-10-21 16:38:45,607:INFO:            requests: 2.28.1
2022-10-21 16:38:45,607:INFO:          matplotlib: 3.4.3
2022-10-21 16:38:45,607:INFO:          scikitplot: 0.3.7
2022-10-21 16:38:45,607:INFO:         yellowbrick: 1.4
2022-10-21 16:38:45,607:INFO:              plotly: 5.5.0
2022-10-21 16:38:45,607:INFO:             kaleido: 0.2.1
2022-10-21 16:38:45,607:INFO:         statsmodels: 0.13.2
2022-10-21 16:38:45,607:INFO:              sktime: 0.13.4
2022-10-21 16:38:45,607:INFO:               tbats: 1.1.1
2022-10-21 16:38:45,607:INFO:            pmdarima: 1.8.5
2022-10-21 16:38:45,607:INFO:              psutil: 5.9.2
2022-10-21 16:38:45,607:INFO:PyCaret optional dependencies:
2022-10-21 16:38:45,607:INFO:                shap: 0.41.0
2022-10-21 16:38:45,607:INFO:           interpret: Not installed
2022-10-21 16:38:45,607:INFO:                umap: Not installed
2022-10-21 16:38:45,607:INFO:    pandas_profiling: Not installed
2022-10-21 16:38:45,608:INFO:  explainerdashboard: Not installed
2022-10-21 16:38:45,608:INFO:             autoviz: Not installed
2022-10-21 16:38:45,608:INFO:           fairlearn: Not installed
2022-10-21 16:38:45,608:INFO:             xgboost: Not installed
2022-10-21 16:38:45,608:INFO:            catboost: 1.1
2022-10-21 16:38:45,608:INFO:              kmodes: Not installed
2022-10-21 16:38:45,608:INFO:             mlxtend: Not installed
2022-10-21 16:38:45,608:INFO:       statsforecast: 1.1.1
2022-10-21 16:38:45,608:INFO:        tune_sklearn: Not installed
2022-10-21 16:38:45,608:INFO:                 ray: Not installed
2022-10-21 16:38:45,608:INFO:            hyperopt: Not installed
2022-10-21 16:38:45,608:INFO:              optuna: Not installed
2022-10-21 16:38:45,609:INFO:               skopt: Not installed
2022-10-21 16:38:45,612:INFO:              mlflow: 1.29.0
2022-10-21 16:38:45,612:INFO:              gradio: Not installed
2022-10-21 16:38:45,613:INFO:             fastapi: Not installed
2022-10-21 16:38:45,613:INFO:             uvicorn: Not installed
2022-10-21 16:38:45,613:INFO:              m2cgen: Not installed
2022-10-21 16:38:45,613:INFO:           evidently: Not installed
2022-10-21 16:38:45,613:INFO:                nltk: 3.6.5
2022-10-21 16:38:45,613:INFO:            pyLDAvis: Not installed
2022-10-21 16:38:45,613:INFO:              gensim: Not installed
2022-10-21 16:38:45,614:INFO:               spacy: Not installed
2022-10-21 16:38:45,614:INFO:           wordcloud: Not installed
2022-10-21 16:38:45,614:INFO:            textblob: Not installed
2022-10-21 16:38:45,614:INFO:               fugue: Not installed
2022-10-21 16:38:45,614:INFO:           streamlit: Not installed
2022-10-21 16:38:45,615:INFO:             prophet: 1.1.1
2022-10-21 16:38:45,615:INFO:None
2022-10-21 16:38:45,615:INFO:Set up data.
2022-10-21 16:38:45,650:INFO:Set up train/test split.
2022-10-21 16:38:45,674:INFO:Set up index.
2022-10-21 16:38:45,676:INFO:Set up folding strategy.
2022-10-21 16:38:45,676:INFO:Assigning column types.
2022-10-21 16:38:45,687:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 16:38:45,688:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:38:45,704:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:38:45,717:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:38:45,863:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:45,952:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:45,954:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:45,954:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:45,956:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:38:45,969:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:38:45,984:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,157:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,236:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,237:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:46,238:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:46,238:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 16:38:46,248:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,254:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,350:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,425:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,428:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:46,428:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:46,436:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,443:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,553:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,647:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,648:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:46,648:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:46,649:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 16:38:46,666:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,785:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,863:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,863:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:46,864:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:46,881:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:38:46,978:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:47,069:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:47,070:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:47,071:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:47,072:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 16:38:47,232:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:47,321:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:47,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:47,323:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:47,484:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:47,572:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:38:47,574:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:47,574:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:47,575:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 16:38:47,712:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:47,786:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:47,787:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:47,902:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:38:48,005:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:48,006:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:48,007:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 16:38:48,290:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:48,290:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:48,574:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:48,574:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:48,583:INFO:Preparing preprocessing pipeline...
2022-10-21 16:38:48,584:INFO:Set up simple imputation.
2022-10-21 16:38:48,594:INFO:Set up encoding of ordinal features.
2022-10-21 16:38:48,602:INFO:Set up encoding of categorical features.
2022-10-21 16:38:48,602:INFO:Set up polynomial features.
2022-10-21 16:38:48,602:INFO:Set up variance threshold.
2022-10-21 16:38:48,603:INFO:Set up feature normalization.
2022-10-21 16:38:48,734:INFO:Finished creating preprocessing pipeline.
2022-10-21 16:38:48,759:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2022-10-21 16:38:48,759:INFO:Creating final display dataframe.
2022-10-21 16:38:49,417:INFO:Setup display_container:                  Description             Value
0                 Session id               122
1                     Target           charges
2                Target type        Regression
3                 Data shape        (1338, 49)
4           Train data shape         (936, 49)
5            Test data shape         (402, 49)
6           Ordinal features                 2
7           Numeric features                 3
8       Categorical features                 3
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15       Polynomial features              True
16         Polynomial degree                 2
17    Low variance threshold                 0
18                 Normalize              True
19          Normalize method            zscore
20            Fold Generator             KFold
21               Fold Number                10
22                  CPU Jobs                -1
23                   Use GPU             False
24            Log Experiment             False
25           Experiment Name  reg-default-name
26                       USI              ef30
2022-10-21 16:38:49,711:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:49,713:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:49,904:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:38:49,905:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:38:49,919:INFO:setup() successfully completed in 4.33s...............
2022-10-21 16:38:49,920:INFO:Initializing compare_models()
2022-10-21 16:38:49,920:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-21 16:38:49,920:INFO:Checking exceptions
2022-10-21 16:38:49,923:INFO:Preparing display monitor
2022-10-21 16:38:50,095:INFO:Initializing Linear Regression
2022-10-21 16:38:50,095:INFO:Total runtime is 3.2862027486165364e-06 minutes
2022-10-21 16:38:50,113:INFO:SubProcess create_model() called ==================================
2022-10-21 16:38:50,114:INFO:Initializing create_model()
2022-10-21 16:38:50,114:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:38:50,115:INFO:Checking exceptions
2022-10-21 16:38:50,122:INFO:Importing libraries
2022-10-21 16:38:50,122:INFO:Copying training dataset
2022-10-21 16:38:50,131:INFO:Defining folds
2022-10-21 16:38:50,131:INFO:Declaring metric variables
2022-10-21 16:38:50,143:INFO:Importing untrained model
2022-10-21 16:38:50,157:INFO:Linear Regression Imported successfully
2022-10-21 16:38:50,183:INFO:Starting cross validation
2022-10-21 16:38:50,187:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:38:51,667:INFO:Calculating mean and std
2022-10-21 16:38:51,669:INFO:Creating metrics dataframe
2022-10-21 16:38:51,674:INFO:Uploading results into container
2022-10-21 16:38:51,675:INFO:Uploading model into container now
2022-10-21 16:38:51,676:INFO:master_model_container: 1
2022-10-21 16:38:51,677:INFO:display_container: 2
2022-10-21 16:38:51,678:INFO:LinearRegression(n_jobs=-1)
2022-10-21 16:38:51,678:INFO:create_model() successfully completed......................................
2022-10-21 16:38:52,121:INFO:SubProcess create_model() end ==================================
2022-10-21 16:38:52,121:INFO:Creating metrics dataframe
2022-10-21 16:38:52,152:INFO:Initializing Lasso Regression
2022-10-21 16:38:52,152:INFO:Total runtime is 0.034283769130706784 minutes
2022-10-21 16:38:52,158:INFO:SubProcess create_model() called ==================================
2022-10-21 16:38:52,158:INFO:Initializing create_model()
2022-10-21 16:38:52,158:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:38:52,158:INFO:Checking exceptions
2022-10-21 16:38:52,163:INFO:Importing libraries
2022-10-21 16:38:52,163:INFO:Copying training dataset
2022-10-21 16:38:52,178:INFO:Defining folds
2022-10-21 16:38:52,178:INFO:Declaring metric variables
2022-10-21 16:38:52,209:INFO:Importing untrained model
2022-10-21 16:38:52,233:INFO:Lasso Regression Imported successfully
2022-10-21 16:38:52,264:INFO:Starting cross validation
2022-10-21 16:38:52,266:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:38:52,684:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.872e+09, tolerance: 1.200e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:52,688:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+09, tolerance: 1.200e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:52,731:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.938e+09, tolerance: 1.211e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:52,732:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+09, tolerance: 1.201e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:53,158:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+09, tolerance: 1.242e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:53,159:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.622e+09, tolerance: 1.201e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:53,180:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.374e+09, tolerance: 1.231e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:53,204:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.006e+09, tolerance: 1.207e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:53,454:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+09, tolerance: 1.261e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:53,473:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.154e+09, tolerance: 1.240e+07
  model = cd_fast.enet_coordinate_descent(

2022-10-21 16:38:53,528:INFO:Calculating mean and std
2022-10-21 16:38:53,530:INFO:Creating metrics dataframe
2022-10-21 16:38:53,536:INFO:Uploading results into container
2022-10-21 16:38:53,537:INFO:Uploading model into container now
2022-10-21 16:38:53,538:INFO:master_model_container: 2
2022-10-21 16:38:53,538:INFO:display_container: 2
2022-10-21 16:38:53,539:INFO:Lasso(random_state=122)
2022-10-21 16:38:53,539:INFO:create_model() successfully completed......................................
2022-10-21 16:38:53,724:INFO:SubProcess create_model() end ==================================
2022-10-21 16:38:53,725:INFO:Creating metrics dataframe
2022-10-21 16:38:53,746:INFO:Initializing Ridge Regression
2022-10-21 16:38:53,746:INFO:Total runtime is 0.06085600058237711 minutes
2022-10-21 16:38:53,754:INFO:SubProcess create_model() called ==================================
2022-10-21 16:38:53,754:INFO:Initializing create_model()
2022-10-21 16:38:53,754:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:38:53,755:INFO:Checking exceptions
2022-10-21 16:38:53,762:INFO:Importing libraries
2022-10-21 16:38:53,763:INFO:Copying training dataset
2022-10-21 16:38:53,773:INFO:Defining folds
2022-10-21 16:38:53,774:INFO:Declaring metric variables
2022-10-21 16:38:53,823:INFO:Importing untrained model
2022-10-21 16:38:53,852:INFO:Ridge Regression Imported successfully
2022-10-21 16:38:54,047:INFO:Starting cross validation
2022-10-21 16:38:54,055:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:38:56,145:INFO:Calculating mean and std
2022-10-21 16:38:56,149:INFO:Creating metrics dataframe
2022-10-21 16:38:56,157:INFO:Uploading results into container
2022-10-21 16:38:56,159:INFO:Uploading model into container now
2022-10-21 16:38:56,161:INFO:master_model_container: 3
2022-10-21 16:38:56,161:INFO:display_container: 2
2022-10-21 16:38:56,162:INFO:Ridge(random_state=122)
2022-10-21 16:38:56,162:INFO:create_model() successfully completed......................................
2022-10-21 16:38:57,007:INFO:SubProcess create_model() end ==================================
2022-10-21 16:38:57,007:INFO:Creating metrics dataframe
2022-10-21 16:38:57,115:INFO:Initializing Elastic Net
2022-10-21 16:38:57,116:INFO:Total runtime is 0.1170000672340393 minutes
2022-10-21 16:38:57,137:INFO:SubProcess create_model() called ==================================
2022-10-21 16:38:57,152:INFO:Initializing create_model()
2022-10-21 16:38:57,152:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:38:57,153:INFO:Checking exceptions
2022-10-21 16:38:57,158:INFO:Importing libraries
2022-10-21 16:38:57,158:INFO:Copying training dataset
2022-10-21 16:38:57,197:INFO:Defining folds
2022-10-21 16:38:57,197:INFO:Declaring metric variables
2022-10-21 16:38:57,217:INFO:Importing untrained model
2022-10-21 16:38:57,350:INFO:Elastic Net Imported successfully
2022-10-21 16:38:57,399:INFO:Starting cross validation
2022-10-21 16:38:57,401:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:38:59,104:INFO:Calculating mean and std
2022-10-21 16:38:59,107:INFO:Creating metrics dataframe
2022-10-21 16:38:59,114:INFO:Uploading results into container
2022-10-21 16:38:59,115:INFO:Uploading model into container now
2022-10-21 16:38:59,116:INFO:master_model_container: 4
2022-10-21 16:38:59,116:INFO:display_container: 2
2022-10-21 16:38:59,117:INFO:ElasticNet(random_state=122)
2022-10-21 16:38:59,118:INFO:create_model() successfully completed......................................
2022-10-21 16:38:59,404:INFO:SubProcess create_model() end ==================================
2022-10-21 16:38:59,405:INFO:Creating metrics dataframe
2022-10-21 16:38:59,438:INFO:Initializing Least Angle Regression
2022-10-21 16:38:59,438:INFO:Total runtime is 0.15571468273798625 minutes
2022-10-21 16:38:59,552:INFO:SubProcess create_model() called ==================================
2022-10-21 16:38:59,553:INFO:Initializing create_model()
2022-10-21 16:38:59,553:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:38:59,554:INFO:Checking exceptions
2022-10-21 16:38:59,561:INFO:Importing libraries
2022-10-21 16:38:59,561:INFO:Copying training dataset
2022-10-21 16:38:59,570:INFO:Defining folds
2022-10-21 16:38:59,584:INFO:Declaring metric variables
2022-10-21 16:38:59,619:INFO:Importing untrained model
2022-10-21 16:38:59,637:INFO:Least Angle Regression Imported successfully
2022-10-21 16:38:59,718:INFO:Starting cross validation
2022-10-21 16:38:59,734:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:00,119:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,129:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,134:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.030e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,135:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.030e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,136:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.972e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,138:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.748e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,141:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.562e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,157:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.508e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,158:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.009e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,165:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.614e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,270:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,301:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.041e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,302:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.147e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,303:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.036e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,303:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.471e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,305:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.656e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,305:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.656e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,306:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.947e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,306:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.125e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,306:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.240e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,306:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.947e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,307:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.129e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,316:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.850e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,317:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.212e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,317:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.145e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,318:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.327e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,318:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.931e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,319:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.043e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 9.365e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,319:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.077e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,320:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.773e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,320:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.360e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,322:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.311e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,322:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.093e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,324:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.479e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,324:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.434e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,324:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.826e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,325:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.823e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,325:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.813e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,329:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.185e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,329:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.174e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,330:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.610e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.365e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,331:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.131e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,331:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.036e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.490e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.645e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.818e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.082e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.464e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.068e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.860e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.704e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.786e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,333:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.234e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.624e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.882e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.369e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.830e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.804e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.777e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,334:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.378e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.767e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.010e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.122e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,335:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.810e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,336:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.085e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,336:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.226e-05, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,338:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.432e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,339:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.374e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,340:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.372e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,340:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.365e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,340:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.855e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,341:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.914e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,355:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.063e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,356:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.961e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,357:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.731e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,358:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,358:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.191e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,359:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.070e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,360:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.048e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,360:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.025e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,360:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.916e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,366:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.280e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,366:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.066e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,367:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.016e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,367:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.685e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,367:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=6.466e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,368:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.238e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,368:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.047e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,368:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.803e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,368:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.293e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,369:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.358e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,369:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.797e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,370:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.410e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,402:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.588e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,405:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.240e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,406:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.027e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,406:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.613e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,406:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.457e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,414:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.230e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,414:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.201e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,415:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.097e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,416:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.900e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,416:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=9.487e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,416:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.190e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,417:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.083e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,417:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.947e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,417:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.916e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,418:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.242e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,457:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.885e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,458:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.199e-01, with an active set of 33 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,462:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.452e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,463:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.315e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,464:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.193e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,465:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.107e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,466:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.009e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,466:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=9.216e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,466:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=7.358e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,468:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.358e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,468:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.531e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,469:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.110e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,469:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.848e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,470:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.691e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,470:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.652e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,470:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.078e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,471:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.859e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,471:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.687e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,471:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.232e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,472:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.232e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,771:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,784:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,784:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,786:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.858e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,788:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.049e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,790:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.988e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,791:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.225e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,791:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.266e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,795:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.143e+01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,792:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.607e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,796:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.193e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,796:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.607e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,797:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.862e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,798:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.106e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,798:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.106e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,799:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.979e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,799:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.211e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,800:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.655e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.388e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.280e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.345e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,803:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.163e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,803:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.008e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,804:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.156e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,804:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.855e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,804:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.806e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,805:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.526e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,805:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.473e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,805:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.330e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,805:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.326e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,806:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.145e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,806:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.855e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,806:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.183e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=9.114e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.940e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.846e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,807:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.571e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,810:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.398e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.432e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.170e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.091e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,816:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.130e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,817:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.100e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,817:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.908e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,817:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.665e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,817:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=6.807e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,817:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.908e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.825e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.175e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.811e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.478e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,818:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.600e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,819:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.199e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,819:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.169e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,819:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.379e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,819:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.572e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.507e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.714e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.282e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.472e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.624e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:00,821:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.047e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,821:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.577e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,821:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.490e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,821:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.187e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,821:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.486e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,821:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.850e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,822:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.475e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,822:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.444e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,822:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.933e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,822:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.426e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,822:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.139e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,822:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.931e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.043e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.566e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.014e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.557e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,823:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.152e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,824:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.613e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,824:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.643e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,825:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.138e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,831:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.482e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,832:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.696e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,832:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.981e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.736e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.148e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,835:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.570e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,835:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.618e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,835:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.436e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,836:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.192e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,836:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.199e-02, with an active set of 40 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,837:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.730e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,839:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.191e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,841:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.055e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.438e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.428e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.427e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,850:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.405e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,851:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.145e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,852:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.599e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,852:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.575e-01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,853:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.996e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,854:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.038e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,855:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.957e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,855:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.007e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,855:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.974e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,856:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.490e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,856:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.958e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,856:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.886e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,856:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.022e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,857:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.015e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,859:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.754e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,864:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.214e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.077e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,865:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.533e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,866:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.494e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,866:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.387e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,866:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.365e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,867:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.274e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,867:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.489e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,867:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.993e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:00,868:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.922e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,085:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:01,087:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:01,089:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.852e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,097:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.666e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,097:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.666e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,097:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.136e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,098:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.249e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,099:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.507e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,099:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.078e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,100:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.078e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,100:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.720e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.717e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.918e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.357e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,101:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.675e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,102:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.027e+00, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,102:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.653e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,102:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.639e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,102:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.436e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,103:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.620e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,103:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.436e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,103:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.544e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,103:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.371e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,103:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.931e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,103:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.508e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,104:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.432e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,104:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.320e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,104:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.348e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,104:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.115e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,104:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.897e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.059e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.610e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.020e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.416e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.713e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.999e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.644e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.184e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,105:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.563e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.210e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.397e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.345e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.716e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.080e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.674e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.035e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,106:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.415e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,107:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.522e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,107:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.664e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,107:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,107:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.223e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,107:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.293e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.965e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.130e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.271e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.822e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.176e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.374e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,108:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.063e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,109:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.060e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,113:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.133e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,113:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.256e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,114:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.987e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,114:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.502e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:01,169:INFO:Calculating mean and std
2022-10-21 16:39:01,171:INFO:Creating metrics dataframe
2022-10-21 16:39:01,177:INFO:Uploading results into container
2022-10-21 16:39:01,179:INFO:Uploading model into container now
2022-10-21 16:39:01,181:INFO:master_model_container: 5
2022-10-21 16:39:01,181:INFO:display_container: 2
2022-10-21 16:39:01,182:INFO:Lars(random_state=122)
2022-10-21 16:39:01,182:INFO:create_model() successfully completed......................................
2022-10-21 16:39:01,401:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:01,402:INFO:Creating metrics dataframe
2022-10-21 16:39:01,433:INFO:Initializing Lasso Least Angle Regression
2022-10-21 16:39:01,434:INFO:Total runtime is 0.1889788826306661 minutes
2022-10-21 16:39:01,440:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:01,441:INFO:Initializing create_model()
2022-10-21 16:39:01,441:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:01,535:INFO:Checking exceptions
2022-10-21 16:39:01,604:INFO:Importing libraries
2022-10-21 16:39:01,605:INFO:Copying training dataset
2022-10-21 16:39:01,632:INFO:Defining folds
2022-10-21 16:39:01,633:INFO:Declaring metric variables
2022-10-21 16:39:01,655:INFO:Importing untrained model
2022-10-21 16:39:01,669:INFO:Lasso Least Angle Regression Imported successfully
2022-10-21 16:39:01,695:INFO:Starting cross validation
2022-10-21 16:39:01,701:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:02,119:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,121:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,129:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,131:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.293e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,132:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.410e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,132:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.688e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,133:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,134:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.731e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,135:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.731e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,135:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.830e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,136:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.471e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,136:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.941e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,138:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.262e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,138:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.216e+00, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,139:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.043e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,150:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.574e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,150:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.125e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,152:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.355e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,152:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.206e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,153:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.206e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,154:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.214e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,523:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,532:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,534:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,536:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.980e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,537:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.769e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,537:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.225e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,537:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.073e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,539:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.193e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,539:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.836e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,539:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,540:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.862e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,547:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.163e+00, previous alpha=4.862e+00, with an active set of 15 regressors.
  warnings.warn(

2022-10-21 16:39:02,549:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.044e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,551:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.647e+00, previous alpha=1.480e+00, with an active set of 24 regressors.
  warnings.warn(

2022-10-21 16:39:02,551:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.992e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,552:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.992e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,553:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=2.933e+00, previous alpha=2.630e+00, with an active set of 18 regressors.
  warnings.warn(

2022-10-21 16:39:02,553:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.570e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,562:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.348e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,564:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.348e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,566:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.323e+00, previous alpha=1.117e+00, with an active set of 27 regressors.
  warnings.warn(

2022-10-21 16:39:02,804:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,817:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:39:02,820:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.843e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,821:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.018e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,824:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.759e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,829:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.371e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,830:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.717e+00, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,831:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.331e+00, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,831:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.479e+00, previous alpha=1.249e+00, with an active set of 27 regressors.
  warnings.warn(

2022-10-21 16:39:02,832:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.529e+00, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-10-21 16:39:02,833:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.413e+00, previous alpha=1.396e+00, with an active set of 24 regressors.
  warnings.warn(

2022-10-21 16:39:02,959:INFO:Calculating mean and std
2022-10-21 16:39:02,964:INFO:Creating metrics dataframe
2022-10-21 16:39:02,969:INFO:Uploading results into container
2022-10-21 16:39:02,971:INFO:Uploading model into container now
2022-10-21 16:39:02,972:INFO:master_model_container: 6
2022-10-21 16:39:02,972:INFO:display_container: 2
2022-10-21 16:39:02,973:INFO:LassoLars(random_state=122)
2022-10-21 16:39:02,973:INFO:create_model() successfully completed......................................
2022-10-21 16:39:03,187:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:03,187:INFO:Creating metrics dataframe
2022-10-21 16:39:03,205:INFO:Initializing Orthogonal Matching Pursuit
2022-10-21 16:39:03,205:INFO:Total runtime is 0.218497633934021 minutes
2022-10-21 16:39:03,222:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:03,222:INFO:Initializing create_model()
2022-10-21 16:39:03,223:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:03,223:INFO:Checking exceptions
2022-10-21 16:39:03,225:INFO:Importing libraries
2022-10-21 16:39:03,226:INFO:Copying training dataset
2022-10-21 16:39:03,318:INFO:Defining folds
2022-10-21 16:39:03,319:INFO:Declaring metric variables
2022-10-21 16:39:03,337:INFO:Importing untrained model
2022-10-21 16:39:03,369:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-21 16:39:03,433:INFO:Starting cross validation
2022-10-21 16:39:03,436:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:03,736:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:03,795:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:03,872:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:03,922:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:04,249:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:04,280:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:04,282:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:04,290:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:04,715:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:04,747:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:39:04,863:INFO:Calculating mean and std
2022-10-21 16:39:04,871:INFO:Creating metrics dataframe
2022-10-21 16:39:04,881:INFO:Uploading results into container
2022-10-21 16:39:04,883:INFO:Uploading model into container now
2022-10-21 16:39:04,885:INFO:master_model_container: 7
2022-10-21 16:39:04,885:INFO:display_container: 2
2022-10-21 16:39:04,885:INFO:OrthogonalMatchingPursuit()
2022-10-21 16:39:04,885:INFO:create_model() successfully completed......................................
2022-10-21 16:39:05,535:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:05,536:INFO:Creating metrics dataframe
2022-10-21 16:39:05,635:INFO:Initializing Bayesian Ridge
2022-10-21 16:39:05,637:INFO:Total runtime is 0.2590318520863851 minutes
2022-10-21 16:39:05,765:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:05,768:INFO:Initializing create_model()
2022-10-21 16:39:05,768:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:05,768:INFO:Checking exceptions
2022-10-21 16:39:05,773:INFO:Importing libraries
2022-10-21 16:39:05,773:INFO:Copying training dataset
2022-10-21 16:39:05,887:INFO:Defining folds
2022-10-21 16:39:05,887:INFO:Declaring metric variables
2022-10-21 16:39:05,922:INFO:Importing untrained model
2022-10-21 16:39:05,975:INFO:Bayesian Ridge Imported successfully
2022-10-21 16:39:06,087:INFO:Starting cross validation
2022-10-21 16:39:06,100:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:07,534:INFO:Calculating mean and std
2022-10-21 16:39:07,538:INFO:Creating metrics dataframe
2022-10-21 16:39:07,544:INFO:Uploading results into container
2022-10-21 16:39:07,545:INFO:Uploading model into container now
2022-10-21 16:39:07,546:INFO:master_model_container: 8
2022-10-21 16:39:07,546:INFO:display_container: 2
2022-10-21 16:39:07,548:INFO:BayesianRidge()
2022-10-21 16:39:07,548:INFO:create_model() successfully completed......................................
2022-10-21 16:39:07,751:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:07,751:INFO:Creating metrics dataframe
2022-10-21 16:39:07,773:INFO:Initializing Passive Aggressive Regressor
2022-10-21 16:39:07,773:INFO:Total runtime is 0.29463510115941366 minutes
2022-10-21 16:39:07,793:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:07,793:INFO:Initializing create_model()
2022-10-21 16:39:07,798:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:07,798:INFO:Checking exceptions
2022-10-21 16:39:07,802:INFO:Importing libraries
2022-10-21 16:39:07,802:INFO:Copying training dataset
2022-10-21 16:39:07,843:INFO:Defining folds
2022-10-21 16:39:07,843:INFO:Declaring metric variables
2022-10-21 16:39:07,935:INFO:Importing untrained model
2022-10-21 16:39:07,951:INFO:Passive Aggressive Regressor Imported successfully
2022-10-21 16:39:07,991:INFO:Starting cross validation
2022-10-21 16:39:07,999:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:10,049:INFO:Calculating mean and std
2022-10-21 16:39:10,052:INFO:Creating metrics dataframe
2022-10-21 16:39:10,057:INFO:Uploading results into container
2022-10-21 16:39:10,058:INFO:Uploading model into container now
2022-10-21 16:39:10,060:INFO:master_model_container: 9
2022-10-21 16:39:10,060:INFO:display_container: 2
2022-10-21 16:39:10,061:INFO:PassiveAggressiveRegressor(random_state=122)
2022-10-21 16:39:10,061:INFO:create_model() successfully completed......................................
2022-10-21 16:39:10,268:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:10,268:INFO:Creating metrics dataframe
2022-10-21 16:39:10,304:INFO:Initializing Huber Regressor
2022-10-21 16:39:10,304:INFO:Total runtime is 0.33681601683298745 minutes
2022-10-21 16:39:10,383:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:10,386:INFO:Initializing create_model()
2022-10-21 16:39:10,387:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:10,387:INFO:Checking exceptions
2022-10-21 16:39:10,390:INFO:Importing libraries
2022-10-21 16:39:10,390:INFO:Copying training dataset
2022-10-21 16:39:10,405:INFO:Defining folds
2022-10-21 16:39:10,405:INFO:Declaring metric variables
2022-10-21 16:39:10,441:INFO:Importing untrained model
2022-10-21 16:39:10,470:INFO:Huber Regressor Imported successfully
2022-10-21 16:39:10,559:INFO:Starting cross validation
2022-10-21 16:39:10,568:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:11,790:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:11,801:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:11,858:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:11,900:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:13,244:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:13,269:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:13,283:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:13,295:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:13,871:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:13,989:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:39:14,072:INFO:Calculating mean and std
2022-10-21 16:39:14,074:INFO:Creating metrics dataframe
2022-10-21 16:39:14,084:INFO:Uploading results into container
2022-10-21 16:39:14,085:INFO:Uploading model into container now
2022-10-21 16:39:14,086:INFO:master_model_container: 10
2022-10-21 16:39:14,086:INFO:display_container: 2
2022-10-21 16:39:14,087:INFO:HuberRegressor()
2022-10-21 16:39:14,087:INFO:create_model() successfully completed......................................
2022-10-21 16:39:14,295:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:14,295:INFO:Creating metrics dataframe
2022-10-21 16:39:14,323:INFO:Initializing K Neighbors Regressor
2022-10-21 16:39:14,324:INFO:Total runtime is 0.40380966663360596 minutes
2022-10-21 16:39:14,333:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:14,333:INFO:Initializing create_model()
2022-10-21 16:39:14,334:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:14,334:INFO:Checking exceptions
2022-10-21 16:39:14,337:INFO:Importing libraries
2022-10-21 16:39:14,338:INFO:Copying training dataset
2022-10-21 16:39:14,358:INFO:Defining folds
2022-10-21 16:39:14,358:INFO:Declaring metric variables
2022-10-21 16:39:14,483:INFO:Importing untrained model
2022-10-21 16:39:14,489:INFO:K Neighbors Regressor Imported successfully
2022-10-21 16:39:14,584:INFO:Starting cross validation
2022-10-21 16:39:14,586:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:16,635:INFO:Calculating mean and std
2022-10-21 16:39:16,639:INFO:Creating metrics dataframe
2022-10-21 16:39:16,646:INFO:Uploading results into container
2022-10-21 16:39:16,648:INFO:Uploading model into container now
2022-10-21 16:39:16,648:INFO:master_model_container: 11
2022-10-21 16:39:16,649:INFO:display_container: 2
2022-10-21 16:39:16,649:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-21 16:39:16,649:INFO:create_model() successfully completed......................................
2022-10-21 16:39:16,814:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:16,814:INFO:Creating metrics dataframe
2022-10-21 16:39:16,839:INFO:Initializing Decision Tree Regressor
2022-10-21 16:39:16,839:INFO:Total runtime is 0.4457364996274312 minutes
2022-10-21 16:39:16,855:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:16,855:INFO:Initializing create_model()
2022-10-21 16:39:16,855:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:16,856:INFO:Checking exceptions
2022-10-21 16:39:16,859:INFO:Importing libraries
2022-10-21 16:39:16,861:INFO:Copying training dataset
2022-10-21 16:39:16,872:INFO:Defining folds
2022-10-21 16:39:16,875:INFO:Declaring metric variables
2022-10-21 16:39:16,901:INFO:Importing untrained model
2022-10-21 16:39:16,913:INFO:Decision Tree Regressor Imported successfully
2022-10-21 16:39:16,935:INFO:Starting cross validation
2022-10-21 16:39:16,938:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:18,864:INFO:Calculating mean and std
2022-10-21 16:39:18,870:INFO:Creating metrics dataframe
2022-10-21 16:39:18,877:INFO:Uploading results into container
2022-10-21 16:39:18,878:INFO:Uploading model into container now
2022-10-21 16:39:18,880:INFO:master_model_container: 12
2022-10-21 16:39:18,880:INFO:display_container: 2
2022-10-21 16:39:18,881:INFO:DecisionTreeRegressor(random_state=122)
2022-10-21 16:39:18,881:INFO:create_model() successfully completed......................................
2022-10-21 16:39:19,237:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:19,237:INFO:Creating metrics dataframe
2022-10-21 16:39:19,255:INFO:Initializing Random Forest Regressor
2022-10-21 16:39:19,255:INFO:Total runtime is 0.4860064506530762 minutes
2022-10-21 16:39:19,293:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:19,293:INFO:Initializing create_model()
2022-10-21 16:39:19,294:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:19,294:INFO:Checking exceptions
2022-10-21 16:39:19,297:INFO:Importing libraries
2022-10-21 16:39:19,297:INFO:Copying training dataset
2022-10-21 16:39:19,306:INFO:Defining folds
2022-10-21 16:39:19,307:INFO:Declaring metric variables
2022-10-21 16:39:19,350:INFO:Importing untrained model
2022-10-21 16:39:19,386:INFO:Random Forest Regressor Imported successfully
2022-10-21 16:39:19,524:INFO:Starting cross validation
2022-10-21 16:39:19,528:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:29,034:INFO:Calculating mean and std
2022-10-21 16:39:29,038:INFO:Creating metrics dataframe
2022-10-21 16:39:29,046:INFO:Uploading results into container
2022-10-21 16:39:29,048:INFO:Uploading model into container now
2022-10-21 16:39:29,049:INFO:master_model_container: 13
2022-10-21 16:39:29,049:INFO:display_container: 2
2022-10-21 16:39:29,052:INFO:RandomForestRegressor(n_jobs=-1, random_state=122)
2022-10-21 16:39:29,052:INFO:create_model() successfully completed......................................
2022-10-21 16:39:29,356:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:29,356:INFO:Creating metrics dataframe
2022-10-21 16:39:29,379:INFO:Initializing Extra Trees Regressor
2022-10-21 16:39:29,379:INFO:Total runtime is 0.6547321796417236 minutes
2022-10-21 16:39:29,474:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:29,476:INFO:Initializing create_model()
2022-10-21 16:39:29,488:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:29,488:INFO:Checking exceptions
2022-10-21 16:39:29,491:INFO:Importing libraries
2022-10-21 16:39:29,491:INFO:Copying training dataset
2022-10-21 16:39:29,518:INFO:Defining folds
2022-10-21 16:39:29,518:INFO:Declaring metric variables
2022-10-21 16:39:29,538:INFO:Importing untrained model
2022-10-21 16:39:29,552:INFO:Extra Trees Regressor Imported successfully
2022-10-21 16:39:29,588:INFO:Starting cross validation
2022-10-21 16:39:29,603:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:35,677:INFO:Calculating mean and std
2022-10-21 16:39:35,683:INFO:Creating metrics dataframe
2022-10-21 16:39:35,691:INFO:Uploading results into container
2022-10-21 16:39:35,692:INFO:Uploading model into container now
2022-10-21 16:39:35,693:INFO:master_model_container: 14
2022-10-21 16:39:35,694:INFO:display_container: 2
2022-10-21 16:39:35,696:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=122)
2022-10-21 16:39:35,697:INFO:create_model() successfully completed......................................
2022-10-21 16:39:35,880:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:35,880:INFO:Creating metrics dataframe
2022-10-21 16:39:35,903:INFO:Initializing AdaBoost Regressor
2022-10-21 16:39:35,903:INFO:Total runtime is 0.7634635329246521 minutes
2022-10-21 16:39:35,907:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:35,908:INFO:Initializing create_model()
2022-10-21 16:39:35,908:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:35,909:INFO:Checking exceptions
2022-10-21 16:39:35,913:INFO:Importing libraries
2022-10-21 16:39:35,913:INFO:Copying training dataset
2022-10-21 16:39:35,921:INFO:Defining folds
2022-10-21 16:39:35,921:INFO:Declaring metric variables
2022-10-21 16:39:35,937:INFO:Importing untrained model
2022-10-21 16:39:35,952:INFO:AdaBoost Regressor Imported successfully
2022-10-21 16:39:35,970:INFO:Starting cross validation
2022-10-21 16:39:35,972:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:37,405:INFO:Calculating mean and std
2022-10-21 16:39:37,407:INFO:Creating metrics dataframe
2022-10-21 16:39:37,413:INFO:Uploading results into container
2022-10-21 16:39:37,415:INFO:Uploading model into container now
2022-10-21 16:39:37,415:INFO:master_model_container: 15
2022-10-21 16:39:37,416:INFO:display_container: 2
2022-10-21 16:39:37,416:INFO:AdaBoostRegressor(random_state=122)
2022-10-21 16:39:37,416:INFO:create_model() successfully completed......................................
2022-10-21 16:39:37,582:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:37,583:INFO:Creating metrics dataframe
2022-10-21 16:39:37,601:INFO:Initializing Gradient Boosting Regressor
2022-10-21 16:39:37,601:INFO:Total runtime is 0.7917655348777771 minutes
2022-10-21 16:39:37,605:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:37,606:INFO:Initializing create_model()
2022-10-21 16:39:37,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:37,607:INFO:Checking exceptions
2022-10-21 16:39:37,610:INFO:Importing libraries
2022-10-21 16:39:37,612:INFO:Copying training dataset
2022-10-21 16:39:37,625:INFO:Defining folds
2022-10-21 16:39:37,625:INFO:Declaring metric variables
2022-10-21 16:39:37,689:INFO:Importing untrained model
2022-10-21 16:39:37,719:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:39:37,833:INFO:Starting cross validation
2022-10-21 16:39:37,836:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:40,254:INFO:Calculating mean and std
2022-10-21 16:39:40,256:INFO:Creating metrics dataframe
2022-10-21 16:39:40,262:INFO:Uploading results into container
2022-10-21 16:39:40,263:INFO:Uploading model into container now
2022-10-21 16:39:40,264:INFO:master_model_container: 16
2022-10-21 16:39:40,265:INFO:display_container: 2
2022-10-21 16:39:40,266:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 16:39:40,266:INFO:create_model() successfully completed......................................
2022-10-21 16:39:40,429:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:40,429:INFO:Creating metrics dataframe
2022-10-21 16:39:40,451:INFO:Initializing Light Gradient Boosting Machine
2022-10-21 16:39:40,452:INFO:Total runtime is 0.8392758011817932 minutes
2022-10-21 16:39:40,457:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:40,458:INFO:Initializing create_model()
2022-10-21 16:39:40,458:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:40,458:INFO:Checking exceptions
2022-10-21 16:39:40,463:INFO:Importing libraries
2022-10-21 16:39:40,463:INFO:Copying training dataset
2022-10-21 16:39:40,471:INFO:Defining folds
2022-10-21 16:39:40,471:INFO:Declaring metric variables
2022-10-21 16:39:40,515:INFO:Importing untrained model
2022-10-21 16:39:40,542:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-21 16:39:40,582:INFO:Starting cross validation
2022-10-21 16:39:40,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:39:42,037:INFO:Calculating mean and std
2022-10-21 16:39:42,039:INFO:Creating metrics dataframe
2022-10-21 16:39:42,044:INFO:Uploading results into container
2022-10-21 16:39:42,046:INFO:Uploading model into container now
2022-10-21 16:39:42,048:INFO:master_model_container: 17
2022-10-21 16:39:42,048:INFO:display_container: 2
2022-10-21 16:39:42,049:INFO:LGBMRegressor(random_state=122)
2022-10-21 16:39:42,049:INFO:create_model() successfully completed......................................
2022-10-21 16:39:42,216:INFO:SubProcess create_model() end ==================================
2022-10-21 16:39:42,217:INFO:Creating metrics dataframe
2022-10-21 16:39:42,236:INFO:Initializing CatBoost Regressor
2022-10-21 16:39:42,236:INFO:Total runtime is 0.8690132816632589 minutes
2022-10-21 16:39:42,243:INFO:SubProcess create_model() called ==================================
2022-10-21 16:39:42,243:INFO:Initializing create_model()
2022-10-21 16:39:42,243:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:39:42,244:INFO:Checking exceptions
2022-10-21 16:39:42,248:INFO:Importing libraries
2022-10-21 16:39:42,248:INFO:Copying training dataset
2022-10-21 16:39:42,259:INFO:Defining folds
2022-10-21 16:39:42,259:INFO:Declaring metric variables
2022-10-21 16:39:42,292:INFO:Importing untrained model
2022-10-21 16:39:42,316:INFO:CatBoost Regressor Imported successfully
2022-10-21 16:39:42,356:INFO:Starting cross validation
2022-10-21 16:39:42,371:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:40:11,798:INFO:Calculating mean and std
2022-10-21 16:40:11,809:INFO:Creating metrics dataframe
2022-10-21 16:40:11,822:INFO:Uploading results into container
2022-10-21 16:40:11,823:INFO:Uploading model into container now
2022-10-21 16:40:11,824:INFO:master_model_container: 18
2022-10-21 16:40:11,824:INFO:display_container: 2
2022-10-21 16:40:11,824:INFO:<catboost.core.CatBoostRegressor object at 0x7faca3604520>
2022-10-21 16:40:11,824:INFO:create_model() successfully completed......................................
2022-10-21 16:40:12,312:INFO:SubProcess create_model() end ==================================
2022-10-21 16:40:12,313:INFO:Creating metrics dataframe
2022-10-21 16:40:12,440:INFO:Initializing Dummy Regressor
2022-10-21 16:40:12,441:INFO:Total runtime is 1.3724258820215862 minutes
2022-10-21 16:40:12,463:INFO:SubProcess create_model() called ==================================
2022-10-21 16:40:12,464:INFO:Initializing create_model()
2022-10-21 16:40:12,464:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1a3fc40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:40:12,468:INFO:Checking exceptions
2022-10-21 16:40:12,474:INFO:Importing libraries
2022-10-21 16:40:12,474:INFO:Copying training dataset
2022-10-21 16:40:12,497:INFO:Defining folds
2022-10-21 16:40:12,497:INFO:Declaring metric variables
2022-10-21 16:40:12,507:INFO:Importing untrained model
2022-10-21 16:40:12,526:INFO:Dummy Regressor Imported successfully
2022-10-21 16:40:12,555:INFO:Starting cross validation
2022-10-21 16:40:12,560:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:40:14,405:INFO:Calculating mean and std
2022-10-21 16:40:14,409:INFO:Creating metrics dataframe
2022-10-21 16:40:14,416:INFO:Uploading results into container
2022-10-21 16:40:14,418:INFO:Uploading model into container now
2022-10-21 16:40:14,419:INFO:master_model_container: 19
2022-10-21 16:40:14,420:INFO:display_container: 2
2022-10-21 16:40:14,420:INFO:DummyRegressor()
2022-10-21 16:40:14,420:INFO:create_model() successfully completed......................................
2022-10-21 16:40:14,768:INFO:SubProcess create_model() end ==================================
2022-10-21 16:40:14,768:INFO:Creating metrics dataframe
2022-10-21 16:40:14,982:INFO:Initializing create_model()
2022-10-21 16:40:14,985:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=GradientBoostingRegressor(random_state=122), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:40:14,985:INFO:Checking exceptions
2022-10-21 16:40:15,010:INFO:Importing libraries
2022-10-21 16:40:15,010:INFO:Copying training dataset
2022-10-21 16:40:15,020:INFO:Defining folds
2022-10-21 16:40:15,020:INFO:Declaring metric variables
2022-10-21 16:40:15,021:INFO:Importing untrained model
2022-10-21 16:40:15,021:INFO:Declaring custom model
2022-10-21 16:40:15,023:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:40:15,038:INFO:Cross validation set to False
2022-10-21 16:40:15,038:INFO:Fitting Model
2022-10-21 16:40:16,835:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 16:40:16,837:INFO:create_model() successfully completed......................................
2022-10-21 16:40:17,933:INFO:master_model_container: 19
2022-10-21 16:40:17,935:INFO:display_container: 2
2022-10-21 16:40:17,937:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 16:40:17,937:INFO:compare_models() successfully completed......................................
2022-10-21 16:40:17,941:INFO:Initializing evaluate_model()
2022-10-21 16:40:17,941:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=GradientBoostingRegressor(random_state=122), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-21 16:40:18,032:INFO:Initializing plot_model()
2022-10-21 16:40:18,033:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, system=True)
2022-10-21 16:40:18,033:INFO:Checking exceptions
2022-10-21 16:40:18,064:INFO:Preloading libraries
2022-10-21 16:40:18,086:INFO:Copying training dataset
2022-10-21 16:40:18,087:INFO:Plot type: pipeline
2022-10-21 16:40:18,886:INFO:Visual Rendered Successfully
2022-10-21 16:40:19,380:INFO:plot_model() successfully completed......................................
2022-10-21 16:40:19,435:INFO:Initializing predict_model()
2022-10-21 16:40:19,436:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca1452a90>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca387cb80>)
2022-10-21 16:40:19,437:INFO:Checking exceptions
2022-10-21 16:40:19,437:INFO:Preloading libraries
2022-10-21 16:40:20,836:INFO:Initializing save_model()
2022-10-21 16:40:20,836:INFO:save_model(model=GradientBoostingRegressor(random_state=122), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 16:40:20,836:INFO:Adding model into prep_pipe
2022-10-21 16:40:20,864:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 16:40:20,905:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 16:40:20,905:INFO:save_model() successfully completed......................................
2022-10-21 16:43:17,282:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:43:17,284:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:43:17,284:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:43:17,284:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:43:19,628:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 16:43:20,975:INFO:Initializing load_model()
2022-10-21 16:43:20,975:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 16:45:59,109:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:45:59,110:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:45:59,110:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:45:59,110:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 16:46:01,233:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 16:46:02,383:INFO:Initializing load_model()
2022-10-21 16:46:02,384:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 16:51:02,511:INFO:PyCaret RegressionExperiment
2022-10-21 16:51:02,514:INFO:Logging name: insurance1
2022-10-21 16:51:02,514:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-21 16:51:02,515:INFO:version 3.0.0.rc4
2022-10-21 16:51:02,515:INFO:Initializing setup()
2022-10-21 16:51:02,516:INFO:self.USI: ca67
2022-10-21 16:51:02,516:INFO:self.variable_keys: {'_available_plots', '_all_metrics', '_all_models_internal', 'X_train', 'display_container', 'gpu_param', 'idx', 'transform_target_method_param', 'html_param', 'exp_id', 'exp_name_log', 'USI', 'log_plots_param', 'variable_keys', 'seed', 'n_jobs_param', 'y_test', 'y_train', '_all_models', 'transform_target_param', '_ml_usecase', 'logging_param', 'data', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'X', 'fold_generator', 'y', 'master_model_container', 'X_test', 'target_param', '_gpu_n_jobs_param', 'pipeline'}
2022-10-21 16:51:02,517:INFO:Checking environment
2022-10-21 16:51:02,519:INFO:python_version: 3.9.7
2022-10-21 16:51:02,519:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-21 16:51:02,520:INFO:machine: x86_64
2022-10-21 16:51:02,520:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:51:02,526:INFO:Memory: svmem(total=8589934592, available=2233110528, percent=74.0, used=4085747712, free=32497664, active=2201972736, inactive=2200109056, wired=1883774976)
2022-10-21 16:51:02,526:INFO:Physical Core: 2
2022-10-21 16:51:02,528:INFO:Logical Core: 4
2022-10-21 16:51:02,528:INFO:Checking libraries
2022-10-21 16:51:02,528:INFO:System:
2022-10-21 16:51:02,528:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-21 16:51:02,528:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-21 16:51:02,528:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-21 16:51:02,528:INFO:PyCaret required dependencies:
2022-10-21 16:51:02,529:INFO:                 pip: 21.2.4
2022-10-21 16:51:02,530:INFO:          setuptools: 58.0.4
2022-10-21 16:51:02,530:INFO:             pycaret: 3.0.0rc4
2022-10-21 16:51:02,530:INFO:             IPython: 7.29.0
2022-10-21 16:51:02,530:INFO:          ipywidgets: 7.6.5
2022-10-21 16:51:02,535:INFO:                tqdm: 4.62.3
2022-10-21 16:51:02,535:INFO:               numpy: 1.22.4
2022-10-21 16:51:02,536:INFO:              pandas: 1.4.4
2022-10-21 16:51:02,536:INFO:              jinja2: 3.1.2
2022-10-21 16:51:02,536:INFO:               scipy: 1.8.1
2022-10-21 16:51:02,536:INFO:              joblib: 1.1.0
2022-10-21 16:51:02,537:INFO:             sklearn: 1.0.2
2022-10-21 16:51:02,537:INFO:                pyod: 1.0.5
2022-10-21 16:51:02,537:INFO:            imblearn: 0.9.0
2022-10-21 16:51:02,537:INFO:   category_encoders: 2.5.1.post0
2022-10-21 16:51:02,537:INFO:            lightgbm: 3.3.2
2022-10-21 16:51:02,537:INFO:               numba: 0.55.2
2022-10-21 16:51:02,537:INFO:            requests: 2.28.1
2022-10-21 16:51:02,537:INFO:          matplotlib: 3.4.3
2022-10-21 16:51:02,537:INFO:          scikitplot: 0.3.7
2022-10-21 16:51:02,537:INFO:         yellowbrick: 1.4
2022-10-21 16:51:02,537:INFO:              plotly: 5.5.0
2022-10-21 16:51:02,537:INFO:             kaleido: 0.2.1
2022-10-21 16:51:02,537:INFO:         statsmodels: 0.13.2
2022-10-21 16:51:02,537:INFO:              sktime: 0.13.4
2022-10-21 16:51:02,537:INFO:               tbats: 1.1.1
2022-10-21 16:51:02,537:INFO:            pmdarima: 1.8.5
2022-10-21 16:51:02,537:INFO:              psutil: 5.9.2
2022-10-21 16:51:02,537:INFO:PyCaret optional dependencies:
2022-10-21 16:51:02,538:INFO:                shap: 0.41.0
2022-10-21 16:51:02,538:INFO:           interpret: Not installed
2022-10-21 16:51:02,538:INFO:                umap: Not installed
2022-10-21 16:51:02,538:INFO:    pandas_profiling: Not installed
2022-10-21 16:51:02,538:INFO:  explainerdashboard: Not installed
2022-10-21 16:51:02,538:INFO:             autoviz: Not installed
2022-10-21 16:51:02,538:INFO:           fairlearn: Not installed
2022-10-21 16:51:02,538:INFO:             xgboost: Not installed
2022-10-21 16:51:02,538:INFO:            catboost: 1.1
2022-10-21 16:51:02,538:INFO:              kmodes: Not installed
2022-10-21 16:51:02,538:INFO:             mlxtend: Not installed
2022-10-21 16:51:02,538:INFO:       statsforecast: 1.1.1
2022-10-21 16:51:02,538:INFO:        tune_sklearn: Not installed
2022-10-21 16:51:02,538:INFO:                 ray: Not installed
2022-10-21 16:51:02,539:INFO:            hyperopt: Not installed
2022-10-21 16:51:02,539:INFO:              optuna: Not installed
2022-10-21 16:51:02,539:INFO:               skopt: Not installed
2022-10-21 16:51:02,539:INFO:              mlflow: 1.29.0
2022-10-21 16:51:02,539:INFO:              gradio: Not installed
2022-10-21 16:51:02,539:INFO:             fastapi: Not installed
2022-10-21 16:51:02,539:INFO:             uvicorn: Not installed
2022-10-21 16:51:02,539:INFO:              m2cgen: Not installed
2022-10-21 16:51:02,539:INFO:           evidently: Not installed
2022-10-21 16:51:02,540:INFO:                nltk: 3.6.5
2022-10-21 16:51:02,540:INFO:            pyLDAvis: Not installed
2022-10-21 16:51:02,541:INFO:              gensim: Not installed
2022-10-21 16:51:02,541:INFO:               spacy: Not installed
2022-10-21 16:51:02,542:INFO:           wordcloud: Not installed
2022-10-21 16:51:02,543:INFO:            textblob: Not installed
2022-10-21 16:51:02,543:INFO:               fugue: Not installed
2022-10-21 16:51:02,544:INFO:           streamlit: Not installed
2022-10-21 16:51:02,544:INFO:             prophet: 1.1.1
2022-10-21 16:51:02,544:INFO:None
2022-10-21 16:51:02,544:INFO:Set up data.
2022-10-21 16:51:02,739:INFO:Set up train/test split.
2022-10-21 16:51:02,789:INFO:Set up index.
2022-10-21 16:51:02,789:INFO:Set up folding strategy.
2022-10-21 16:51:02,790:INFO:Assigning column types.
2022-10-21 16:51:02,801:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-21 16:51:02,802:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:51:02,813:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:51:02,859:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,150:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,277:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,278:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:03,279:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:03,281:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,290:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,299:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,424:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,588:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,589:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:03,590:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:03,591:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-21 16:51:03,602:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,610:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,727:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,818:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,819:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:03,820:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:03,829:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,843:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:51:03,955:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,038:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,039:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:04,040:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:04,041:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-21 16:51:04,058:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,313:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,462:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,463:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:04,463:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:04,501:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,685:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,790:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:04,790:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:04,791:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:04,791:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-21 16:51:04,970:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:05,045:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:05,046:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:05,046:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:05,164:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:05,244:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-21 16:51:05,245:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:05,245:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:05,245:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-21 16:51:05,362:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:05,440:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:05,440:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:05,552:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-21 16:51:05,641:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:05,641:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:05,642:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-21 16:51:05,859:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:05,859:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:06,213:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:06,213:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:06,220:INFO:Preparing preprocessing pipeline...
2022-10-21 16:51:06,222:INFO:Set up simple imputation.
2022-10-21 16:51:06,238:INFO:Set up encoding of ordinal features.
2022-10-21 16:51:06,244:INFO:Set up encoding of categorical features.
2022-10-21 16:51:06,244:INFO:Set up variance threshold.
2022-10-21 16:51:06,374:INFO:Finished creating preprocessing pipeline.
2022-10-21 16:51:06,422:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-21 16:51:06,423:INFO:Creating final display dataframe.
2022-10-21 16:51:07,158:INFO:Setup display_container:                  Description         Value
0                 Session id           122
1                     Target       charges
2                Target type    Regression
3                 Data shape    (1338, 10)
4           Train data shape     (936, 10)
5            Test data shape     (402, 10)
6           Ordinal features             2
7           Numeric features             3
8       Categorical features             3
9                 Preprocess          True
10           Imputation type        simple
11        Numeric imputation          mean
12    Categorical imputation      constant
13  Maximum one-hot encoding             5
14           Encoding method          None
15    Low variance threshold             0
16            Fold Generator         KFold
17               Fold Number            10
18                  CPU Jobs            -1
19                   Use GPU         False
20            Log Experiment  MlflowLogger
21           Experiment Name    insurance1
22                       USI          ca67
2022-10-21 16:51:07,522:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:07,522:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:07,778:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-21 16:51:07,779:INFO:Soft dependency imported: catboost: 1.1
2022-10-21 16:51:07,794:INFO:Logging experiment in loggers
2022-10-21 16:51:08,638:INFO:SubProcess save_model() called ==================================
2022-10-21 16:51:08,803:INFO:Initializing save_model()
2022-10-21 16:51:08,804:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), model_name=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/tmp_p7e0ct8/Transformation Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 16:51:08,804:INFO:Adding model into prep_pipe
2022-10-21 16:51:08,813:WARNING:Only Model saved as it was a pipeline.
2022-10-21 16:51:08,846:INFO:/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/tmp_p7e0ct8/Transformation Pipeline.pkl saved in current working directory
2022-10-21 16:51:09,023:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-21 16:51:09,023:INFO:save_model() successfully completed......................................
2022-10-21 16:51:10,958:INFO:SubProcess save_model() end ==================================
2022-10-21 16:51:10,990:INFO:setup() successfully completed in 5.3s...............
2022-10-21 16:51:10,991:INFO:Initializing compare_models()
2022-10-21 16:51:11,004:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, include=None, fold=5, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-21 16:51:11,004:INFO:Checking exceptions
2022-10-21 16:51:11,009:INFO:Preparing display monitor
2022-10-21 16:51:11,120:INFO:Initializing Linear Regression
2022-10-21 16:51:11,120:INFO:Total runtime is 3.6676724751790363e-06 minutes
2022-10-21 16:51:11,251:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:11,252:INFO:Initializing create_model()
2022-10-21 16:51:11,253:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=lr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:11,253:INFO:Checking exceptions
2022-10-21 16:51:11,263:INFO:Importing libraries
2022-10-21 16:51:11,263:INFO:Copying training dataset
2022-10-21 16:51:11,275:INFO:Defining folds
2022-10-21 16:51:11,275:INFO:Declaring metric variables
2022-10-21 16:51:11,290:INFO:Importing untrained model
2022-10-21 16:51:11,314:INFO:Linear Regression Imported successfully
2022-10-21 16:51:11,343:INFO:Starting cross validation
2022-10-21 16:51:11,345:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:36,085:INFO:Calculating mean and std
2022-10-21 16:51:36,094:INFO:Creating metrics dataframe
2022-10-21 16:51:36,105:INFO:Uploading results into container
2022-10-21 16:51:36,106:INFO:Uploading model into container now
2022-10-21 16:51:36,107:INFO:master_model_container: 1
2022-10-21 16:51:36,107:INFO:display_container: 2
2022-10-21 16:51:36,108:INFO:LinearRegression(n_jobs=-1)
2022-10-21 16:51:36,108:INFO:create_model() successfully completed......................................
2022-10-21 16:51:36,340:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:36,340:INFO:Creating metrics dataframe
2022-10-21 16:51:36,356:INFO:Initializing Lasso Regression
2022-10-21 16:51:36,357:INFO:Total runtime is 0.4206156015396118 minutes
2022-10-21 16:51:36,364:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:36,365:INFO:Initializing create_model()
2022-10-21 16:51:36,365:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=lasso, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:36,365:INFO:Checking exceptions
2022-10-21 16:51:36,371:INFO:Importing libraries
2022-10-21 16:51:36,371:INFO:Copying training dataset
2022-10-21 16:51:36,411:INFO:Defining folds
2022-10-21 16:51:36,415:INFO:Declaring metric variables
2022-10-21 16:51:36,431:INFO:Importing untrained model
2022-10-21 16:51:36,453:INFO:Lasso Regression Imported successfully
2022-10-21 16:51:36,497:INFO:Starting cross validation
2022-10-21 16:51:36,505:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:38,109:INFO:Calculating mean and std
2022-10-21 16:51:38,112:INFO:Creating metrics dataframe
2022-10-21 16:51:38,119:INFO:Uploading results into container
2022-10-21 16:51:38,120:INFO:Uploading model into container now
2022-10-21 16:51:38,122:INFO:master_model_container: 2
2022-10-21 16:51:38,123:INFO:display_container: 2
2022-10-21 16:51:38,124:INFO:Lasso(random_state=122)
2022-10-21 16:51:38,124:INFO:create_model() successfully completed......................................
2022-10-21 16:51:38,295:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:38,295:INFO:Creating metrics dataframe
2022-10-21 16:51:38,314:INFO:Initializing Ridge Regression
2022-10-21 16:51:38,314:INFO:Total runtime is 0.45324296553929644 minutes
2022-10-21 16:51:38,323:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:38,324:INFO:Initializing create_model()
2022-10-21 16:51:38,324:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=ridge, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:38,324:INFO:Checking exceptions
2022-10-21 16:51:38,328:INFO:Importing libraries
2022-10-21 16:51:38,328:INFO:Copying training dataset
2022-10-21 16:51:38,338:INFO:Defining folds
2022-10-21 16:51:38,338:INFO:Declaring metric variables
2022-10-21 16:51:38,548:INFO:Importing untrained model
2022-10-21 16:51:38,558:INFO:Ridge Regression Imported successfully
2022-10-21 16:51:38,647:INFO:Starting cross validation
2022-10-21 16:51:38,654:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:39,064:INFO:Calculating mean and std
2022-10-21 16:51:39,069:INFO:Creating metrics dataframe
2022-10-21 16:51:39,077:INFO:Uploading results into container
2022-10-21 16:51:39,078:INFO:Uploading model into container now
2022-10-21 16:51:39,079:INFO:master_model_container: 3
2022-10-21 16:51:39,079:INFO:display_container: 2
2022-10-21 16:51:39,080:INFO:Ridge(random_state=122)
2022-10-21 16:51:39,080:INFO:create_model() successfully completed......................................
2022-10-21 16:51:39,269:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:39,269:INFO:Creating metrics dataframe
2022-10-21 16:51:39,282:INFO:Initializing Elastic Net
2022-10-21 16:51:39,282:INFO:Total runtime is 0.46936978499094645 minutes
2022-10-21 16:51:39,290:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:39,292:INFO:Initializing create_model()
2022-10-21 16:51:39,292:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=en, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:39,292:INFO:Checking exceptions
2022-10-21 16:51:39,295:INFO:Importing libraries
2022-10-21 16:51:39,295:INFO:Copying training dataset
2022-10-21 16:51:39,307:INFO:Defining folds
2022-10-21 16:51:39,307:INFO:Declaring metric variables
2022-10-21 16:51:39,339:INFO:Importing untrained model
2022-10-21 16:51:39,357:INFO:Elastic Net Imported successfully
2022-10-21 16:51:39,379:INFO:Starting cross validation
2022-10-21 16:51:39,381:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:39,812:INFO:Calculating mean and std
2022-10-21 16:51:39,815:INFO:Creating metrics dataframe
2022-10-21 16:51:39,826:INFO:Uploading results into container
2022-10-21 16:51:39,827:INFO:Uploading model into container now
2022-10-21 16:51:39,827:INFO:master_model_container: 4
2022-10-21 16:51:39,827:INFO:display_container: 2
2022-10-21 16:51:39,828:INFO:ElasticNet(random_state=122)
2022-10-21 16:51:39,828:INFO:create_model() successfully completed......................................
2022-10-21 16:51:39,993:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:39,993:INFO:Creating metrics dataframe
2022-10-21 16:51:40,009:INFO:Initializing Least Angle Regression
2022-10-21 16:51:40,009:INFO:Total runtime is 0.4814890662829081 minutes
2022-10-21 16:51:40,015:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:40,016:INFO:Initializing create_model()
2022-10-21 16:51:40,016:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=lar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:40,016:INFO:Checking exceptions
2022-10-21 16:51:40,022:INFO:Importing libraries
2022-10-21 16:51:40,022:INFO:Copying training dataset
2022-10-21 16:51:40,030:INFO:Defining folds
2022-10-21 16:51:40,030:INFO:Declaring metric variables
2022-10-21 16:51:40,062:INFO:Importing untrained model
2022-10-21 16:51:40,081:INFO:Least Angle Regression Imported successfully
2022-10-21 16:51:40,111:INFO:Starting cross validation
2022-10-21 16:51:40,120:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:40,328:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:40,332:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:40,343:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:40,360:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:40,474:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:40,509:INFO:Calculating mean and std
2022-10-21 16:51:40,511:INFO:Creating metrics dataframe
2022-10-21 16:51:40,518:INFO:Uploading results into container
2022-10-21 16:51:40,519:INFO:Uploading model into container now
2022-10-21 16:51:40,520:INFO:master_model_container: 5
2022-10-21 16:51:40,521:INFO:display_container: 2
2022-10-21 16:51:40,522:INFO:Lars(random_state=122)
2022-10-21 16:51:40,522:INFO:create_model() successfully completed......................................
2022-10-21 16:51:40,696:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:40,696:INFO:Creating metrics dataframe
2022-10-21 16:51:40,714:INFO:Initializing Lasso Least Angle Regression
2022-10-21 16:51:40,714:INFO:Total runtime is 0.4932443539301554 minutes
2022-10-21 16:51:40,724:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:40,725:INFO:Initializing create_model()
2022-10-21 16:51:40,725:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=llar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:40,725:INFO:Checking exceptions
2022-10-21 16:51:40,729:INFO:Importing libraries
2022-10-21 16:51:40,730:INFO:Copying training dataset
2022-10-21 16:51:40,740:INFO:Defining folds
2022-10-21 16:51:40,757:INFO:Declaring metric variables
2022-10-21 16:51:40,776:INFO:Importing untrained model
2022-10-21 16:51:40,789:INFO:Lasso Least Angle Regression Imported successfully
2022-10-21 16:51:40,821:INFO:Starting cross validation
2022-10-21 16:51:40,825:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:41,021:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:51:41,028:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:51:41,041:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:51:41,058:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:51:41,231:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-21 16:51:41,265:INFO:Calculating mean and std
2022-10-21 16:51:41,268:INFO:Creating metrics dataframe
2022-10-21 16:51:41,273:INFO:Uploading results into container
2022-10-21 16:51:41,274:INFO:Uploading model into container now
2022-10-21 16:51:41,275:INFO:master_model_container: 6
2022-10-21 16:51:41,275:INFO:display_container: 2
2022-10-21 16:51:41,275:INFO:LassoLars(random_state=122)
2022-10-21 16:51:41,275:INFO:create_model() successfully completed......................................
2022-10-21 16:51:41,453:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:41,453:INFO:Creating metrics dataframe
2022-10-21 16:51:41,477:INFO:Initializing Orthogonal Matching Pursuit
2022-10-21 16:51:41,477:INFO:Total runtime is 0.5059517820676168 minutes
2022-10-21 16:51:41,487:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:41,488:INFO:Initializing create_model()
2022-10-21 16:51:41,488:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=omp, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:41,488:INFO:Checking exceptions
2022-10-21 16:51:41,492:INFO:Importing libraries
2022-10-21 16:51:41,492:INFO:Copying training dataset
2022-10-21 16:51:41,505:INFO:Defining folds
2022-10-21 16:51:41,534:INFO:Declaring metric variables
2022-10-21 16:51:41,748:INFO:Importing untrained model
2022-10-21 16:51:41,785:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-21 16:51:41,829:INFO:Starting cross validation
2022-10-21 16:51:41,831:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:42,177:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:42,181:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:42,183:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:42,198:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:42,430:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-21 16:51:42,504:INFO:Calculating mean and std
2022-10-21 16:51:42,507:INFO:Creating metrics dataframe
2022-10-21 16:51:42,512:INFO:Uploading results into container
2022-10-21 16:51:42,513:INFO:Uploading model into container now
2022-10-21 16:51:42,514:INFO:master_model_container: 7
2022-10-21 16:51:42,514:INFO:display_container: 2
2022-10-21 16:51:42,515:INFO:OrthogonalMatchingPursuit()
2022-10-21 16:51:42,516:INFO:create_model() successfully completed......................................
2022-10-21 16:51:42,700:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:42,700:INFO:Creating metrics dataframe
2022-10-21 16:51:42,720:INFO:Initializing Bayesian Ridge
2022-10-21 16:51:42,721:INFO:Total runtime is 0.5266809701919556 minutes
2022-10-21 16:51:42,726:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:42,727:INFO:Initializing create_model()
2022-10-21 16:51:42,727:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=br, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:42,727:INFO:Checking exceptions
2022-10-21 16:51:42,731:INFO:Importing libraries
2022-10-21 16:51:42,732:INFO:Copying training dataset
2022-10-21 16:51:42,742:INFO:Defining folds
2022-10-21 16:51:42,743:INFO:Declaring metric variables
2022-10-21 16:51:42,763:INFO:Importing untrained model
2022-10-21 16:51:42,786:INFO:Bayesian Ridge Imported successfully
2022-10-21 16:51:42,810:INFO:Starting cross validation
2022-10-21 16:51:42,814:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:43,244:INFO:Calculating mean and std
2022-10-21 16:51:43,247:INFO:Creating metrics dataframe
2022-10-21 16:51:43,253:INFO:Uploading results into container
2022-10-21 16:51:43,255:INFO:Uploading model into container now
2022-10-21 16:51:43,256:INFO:master_model_container: 8
2022-10-21 16:51:43,256:INFO:display_container: 2
2022-10-21 16:51:43,257:INFO:BayesianRidge()
2022-10-21 16:51:43,257:INFO:create_model() successfully completed......................................
2022-10-21 16:51:43,423:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:43,424:INFO:Creating metrics dataframe
2022-10-21 16:51:43,444:INFO:Initializing Passive Aggressive Regressor
2022-10-21 16:51:43,445:INFO:Total runtime is 0.5387508511543274 minutes
2022-10-21 16:51:43,460:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:43,461:INFO:Initializing create_model()
2022-10-21 16:51:43,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=par, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:43,461:INFO:Checking exceptions
2022-10-21 16:51:43,464:INFO:Importing libraries
2022-10-21 16:51:43,464:INFO:Copying training dataset
2022-10-21 16:51:43,476:INFO:Defining folds
2022-10-21 16:51:43,476:INFO:Declaring metric variables
2022-10-21 16:51:43,489:INFO:Importing untrained model
2022-10-21 16:51:43,506:INFO:Passive Aggressive Regressor Imported successfully
2022-10-21 16:51:43,523:INFO:Starting cross validation
2022-10-21 16:51:43,525:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:44,018:INFO:Calculating mean and std
2022-10-21 16:51:44,020:INFO:Creating metrics dataframe
2022-10-21 16:51:44,026:INFO:Uploading results into container
2022-10-21 16:51:44,026:INFO:Uploading model into container now
2022-10-21 16:51:44,028:INFO:master_model_container: 9
2022-10-21 16:51:44,028:INFO:display_container: 2
2022-10-21 16:51:44,029:INFO:PassiveAggressiveRegressor(random_state=122)
2022-10-21 16:51:44,029:INFO:create_model() successfully completed......................................
2022-10-21 16:51:44,194:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:44,194:INFO:Creating metrics dataframe
2022-10-21 16:51:44,220:INFO:Initializing Huber Regressor
2022-10-21 16:51:44,220:INFO:Total runtime is 0.5516707181930542 minutes
2022-10-21 16:51:44,249:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:44,250:INFO:Initializing create_model()
2022-10-21 16:51:44,250:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=huber, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:44,250:INFO:Checking exceptions
2022-10-21 16:51:44,253:INFO:Importing libraries
2022-10-21 16:51:44,253:INFO:Copying training dataset
2022-10-21 16:51:44,260:INFO:Defining folds
2022-10-21 16:51:44,260:INFO:Declaring metric variables
2022-10-21 16:51:44,268:INFO:Importing untrained model
2022-10-21 16:51:44,280:INFO:Huber Regressor Imported successfully
2022-10-21 16:51:44,294:INFO:Starting cross validation
2022-10-21 16:51:44,296:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:44,692:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:51:44,712:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:51:44,729:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:51:44,890:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-21 16:51:44,922:INFO:Calculating mean and std
2022-10-21 16:51:44,925:INFO:Creating metrics dataframe
2022-10-21 16:51:44,930:INFO:Uploading results into container
2022-10-21 16:51:44,931:INFO:Uploading model into container now
2022-10-21 16:51:44,932:INFO:master_model_container: 10
2022-10-21 16:51:44,933:INFO:display_container: 2
2022-10-21 16:51:44,934:INFO:HuberRegressor()
2022-10-21 16:51:44,934:INFO:create_model() successfully completed......................................
2022-10-21 16:51:45,103:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:45,103:INFO:Creating metrics dataframe
2022-10-21 16:51:45,121:INFO:Initializing K Neighbors Regressor
2022-10-21 16:51:45,122:INFO:Total runtime is 0.5667006333669027 minutes
2022-10-21 16:51:45,127:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:45,128:INFO:Initializing create_model()
2022-10-21 16:51:45,128:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=knn, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:45,128:INFO:Checking exceptions
2022-10-21 16:51:45,130:INFO:Importing libraries
2022-10-21 16:51:45,131:INFO:Copying training dataset
2022-10-21 16:51:45,147:INFO:Defining folds
2022-10-21 16:51:45,147:INFO:Declaring metric variables
2022-10-21 16:51:45,177:INFO:Importing untrained model
2022-10-21 16:51:45,198:INFO:K Neighbors Regressor Imported successfully
2022-10-21 16:51:45,240:INFO:Starting cross validation
2022-10-21 16:51:45,264:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:45,723:INFO:Calculating mean and std
2022-10-21 16:51:45,725:INFO:Creating metrics dataframe
2022-10-21 16:51:45,736:INFO:Uploading results into container
2022-10-21 16:51:45,737:INFO:Uploading model into container now
2022-10-21 16:51:45,739:INFO:master_model_container: 11
2022-10-21 16:51:45,739:INFO:display_container: 2
2022-10-21 16:51:45,739:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-21 16:51:45,739:INFO:create_model() successfully completed......................................
2022-10-21 16:51:45,909:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:45,909:INFO:Creating metrics dataframe
2022-10-21 16:51:45,955:INFO:Initializing Decision Tree Regressor
2022-10-21 16:51:45,955:INFO:Total runtime is 0.580588714281718 minutes
2022-10-21 16:51:45,972:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:45,975:INFO:Initializing create_model()
2022-10-21 16:51:45,975:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=dt, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:45,975:INFO:Checking exceptions
2022-10-21 16:51:45,978:INFO:Importing libraries
2022-10-21 16:51:45,978:INFO:Copying training dataset
2022-10-21 16:51:46,013:INFO:Defining folds
2022-10-21 16:51:46,013:INFO:Declaring metric variables
2022-10-21 16:51:46,060:INFO:Importing untrained model
2022-10-21 16:51:46,090:INFO:Decision Tree Regressor Imported successfully
2022-10-21 16:51:46,123:INFO:Starting cross validation
2022-10-21 16:51:46,125:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:46,712:INFO:Calculating mean and std
2022-10-21 16:51:46,714:INFO:Creating metrics dataframe
2022-10-21 16:51:46,723:INFO:Uploading results into container
2022-10-21 16:51:46,724:INFO:Uploading model into container now
2022-10-21 16:51:46,724:INFO:master_model_container: 12
2022-10-21 16:51:46,725:INFO:display_container: 2
2022-10-21 16:51:46,725:INFO:DecisionTreeRegressor(random_state=122)
2022-10-21 16:51:46,725:INFO:create_model() successfully completed......................................
2022-10-21 16:51:46,889:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:46,889:INFO:Creating metrics dataframe
2022-10-21 16:51:46,907:INFO:Initializing Random Forest Regressor
2022-10-21 16:51:46,907:INFO:Total runtime is 0.5964575171470643 minutes
2022-10-21 16:51:46,912:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:46,913:INFO:Initializing create_model()
2022-10-21 16:51:46,913:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=rf, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:46,913:INFO:Checking exceptions
2022-10-21 16:51:46,919:INFO:Importing libraries
2022-10-21 16:51:46,919:INFO:Copying training dataset
2022-10-21 16:51:46,928:INFO:Defining folds
2022-10-21 16:51:46,928:INFO:Declaring metric variables
2022-10-21 16:51:46,964:INFO:Importing untrained model
2022-10-21 16:51:46,988:INFO:Random Forest Regressor Imported successfully
2022-10-21 16:51:47,013:INFO:Starting cross validation
2022-10-21 16:51:47,014:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:49,065:INFO:Calculating mean and std
2022-10-21 16:51:49,071:INFO:Creating metrics dataframe
2022-10-21 16:51:49,076:INFO:Uploading results into container
2022-10-21 16:51:49,077:INFO:Uploading model into container now
2022-10-21 16:51:49,077:INFO:master_model_container: 13
2022-10-21 16:51:49,078:INFO:display_container: 2
2022-10-21 16:51:49,078:INFO:RandomForestRegressor(n_jobs=-1, random_state=122)
2022-10-21 16:51:49,078:INFO:create_model() successfully completed......................................
2022-10-21 16:51:49,290:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:49,291:INFO:Creating metrics dataframe
2022-10-21 16:51:49,338:INFO:Initializing Extra Trees Regressor
2022-10-21 16:51:49,342:INFO:Total runtime is 0.6369774341583253 minutes
2022-10-21 16:51:49,358:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:49,359:INFO:Initializing create_model()
2022-10-21 16:51:49,359:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=et, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:49,360:INFO:Checking exceptions
2022-10-21 16:51:49,369:INFO:Importing libraries
2022-10-21 16:51:49,370:INFO:Copying training dataset
2022-10-21 16:51:49,406:INFO:Defining folds
2022-10-21 16:51:49,406:INFO:Declaring metric variables
2022-10-21 16:51:49,441:INFO:Importing untrained model
2022-10-21 16:51:49,473:INFO:Extra Trees Regressor Imported successfully
2022-10-21 16:51:49,563:INFO:Starting cross validation
2022-10-21 16:51:49,586:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:50,877:INFO:Calculating mean and std
2022-10-21 16:51:50,879:INFO:Creating metrics dataframe
2022-10-21 16:51:50,885:INFO:Uploading results into container
2022-10-21 16:51:50,886:INFO:Uploading model into container now
2022-10-21 16:51:50,887:INFO:master_model_container: 14
2022-10-21 16:51:50,887:INFO:display_container: 2
2022-10-21 16:51:50,888:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=122)
2022-10-21 16:51:50,888:INFO:create_model() successfully completed......................................
2022-10-21 16:51:51,075:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:51,075:INFO:Creating metrics dataframe
2022-10-21 16:51:51,094:INFO:Initializing AdaBoost Regressor
2022-10-21 16:51:51,094:INFO:Total runtime is 0.6662398179372153 minutes
2022-10-21 16:51:51,104:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:51,105:INFO:Initializing create_model()
2022-10-21 16:51:51,105:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=ada, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:51,105:INFO:Checking exceptions
2022-10-21 16:51:51,109:INFO:Importing libraries
2022-10-21 16:51:51,110:INFO:Copying training dataset
2022-10-21 16:51:51,145:INFO:Defining folds
2022-10-21 16:51:51,146:INFO:Declaring metric variables
2022-10-21 16:51:51,159:INFO:Importing untrained model
2022-10-21 16:51:51,176:INFO:AdaBoost Regressor Imported successfully
2022-10-21 16:51:51,198:INFO:Starting cross validation
2022-10-21 16:51:51,210:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:51,687:INFO:Calculating mean and std
2022-10-21 16:51:51,690:INFO:Creating metrics dataframe
2022-10-21 16:51:51,696:INFO:Uploading results into container
2022-10-21 16:51:51,697:INFO:Uploading model into container now
2022-10-21 16:51:51,698:INFO:master_model_container: 15
2022-10-21 16:51:51,698:INFO:display_container: 2
2022-10-21 16:51:51,699:INFO:AdaBoostRegressor(random_state=122)
2022-10-21 16:51:51,699:INFO:create_model() successfully completed......................................
2022-10-21 16:51:51,893:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:51,893:INFO:Creating metrics dataframe
2022-10-21 16:51:51,924:INFO:Initializing Gradient Boosting Regressor
2022-10-21 16:51:51,924:INFO:Total runtime is 0.6800765355428061 minutes
2022-10-21 16:51:51,945:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:51,947:INFO:Initializing create_model()
2022-10-21 16:51:51,947:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=gbr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:51,947:INFO:Checking exceptions
2022-10-21 16:51:51,954:INFO:Importing libraries
2022-10-21 16:51:51,954:INFO:Copying training dataset
2022-10-21 16:51:52,026:INFO:Defining folds
2022-10-21 16:51:52,028:INFO:Declaring metric variables
2022-10-21 16:51:52,042:INFO:Importing untrained model
2022-10-21 16:51:52,058:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:51:52,079:INFO:Starting cross validation
2022-10-21 16:51:52,082:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:52,846:INFO:Calculating mean and std
2022-10-21 16:51:52,848:INFO:Creating metrics dataframe
2022-10-21 16:51:52,856:INFO:Uploading results into container
2022-10-21 16:51:52,856:INFO:Uploading model into container now
2022-10-21 16:51:52,857:INFO:master_model_container: 16
2022-10-21 16:51:52,857:INFO:display_container: 2
2022-10-21 16:51:52,858:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 16:51:52,858:INFO:create_model() successfully completed......................................
2022-10-21 16:51:53,015:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:53,015:INFO:Creating metrics dataframe
2022-10-21 16:51:53,040:INFO:Initializing Light Gradient Boosting Machine
2022-10-21 16:51:53,040:INFO:Total runtime is 0.6986692547798158 minutes
2022-10-21 16:51:53,048:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:53,048:INFO:Initializing create_model()
2022-10-21 16:51:53,048:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=lightgbm, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:53,049:INFO:Checking exceptions
2022-10-21 16:51:53,053:INFO:Importing libraries
2022-10-21 16:51:53,054:INFO:Copying training dataset
2022-10-21 16:51:53,062:INFO:Defining folds
2022-10-21 16:51:53,063:INFO:Declaring metric variables
2022-10-21 16:51:53,093:INFO:Importing untrained model
2022-10-21 16:51:53,131:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-21 16:51:53,152:INFO:Starting cross validation
2022-10-21 16:51:53,153:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:51:57,111:INFO:Calculating mean and std
2022-10-21 16:51:57,114:INFO:Creating metrics dataframe
2022-10-21 16:51:57,125:INFO:Uploading results into container
2022-10-21 16:51:57,126:INFO:Uploading model into container now
2022-10-21 16:51:57,129:INFO:master_model_container: 17
2022-10-21 16:51:57,129:INFO:display_container: 2
2022-10-21 16:51:57,130:INFO:LGBMRegressor(random_state=122)
2022-10-21 16:51:57,130:INFO:create_model() successfully completed......................................
2022-10-21 16:51:57,456:INFO:SubProcess create_model() end ==================================
2022-10-21 16:51:57,457:INFO:Creating metrics dataframe
2022-10-21 16:51:57,511:INFO:Initializing CatBoost Regressor
2022-10-21 16:51:57,511:INFO:Total runtime is 0.7731872518857321 minutes
2022-10-21 16:51:57,526:INFO:SubProcess create_model() called ==================================
2022-10-21 16:51:57,527:INFO:Initializing create_model()
2022-10-21 16:51:57,527:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=catboost, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:51:57,527:INFO:Checking exceptions
2022-10-21 16:51:57,531:INFO:Importing libraries
2022-10-21 16:51:57,531:INFO:Copying training dataset
2022-10-21 16:51:57,551:INFO:Defining folds
2022-10-21 16:51:57,554:INFO:Declaring metric variables
2022-10-21 16:51:57,564:INFO:Importing untrained model
2022-10-21 16:51:57,587:INFO:CatBoost Regressor Imported successfully
2022-10-21 16:51:57,610:INFO:Starting cross validation
2022-10-21 16:51:57,613:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:52:02,600:INFO:Calculating mean and std
2022-10-21 16:52:02,604:INFO:Creating metrics dataframe
2022-10-21 16:52:02,609:INFO:Uploading results into container
2022-10-21 16:52:02,611:INFO:Uploading model into container now
2022-10-21 16:52:02,613:INFO:master_model_container: 18
2022-10-21 16:52:02,613:INFO:display_container: 2
2022-10-21 16:52:02,613:INFO:<catboost.core.CatBoostRegressor object at 0x7faca36addf0>
2022-10-21 16:52:02,613:INFO:create_model() successfully completed......................................
2022-10-21 16:52:02,796:INFO:SubProcess create_model() end ==================================
2022-10-21 16:52:02,796:INFO:Creating metrics dataframe
2022-10-21 16:52:02,819:INFO:Initializing Dummy Regressor
2022-10-21 16:52:02,819:INFO:Total runtime is 0.8616612553596498 minutes
2022-10-21 16:52:02,826:INFO:SubProcess create_model() called ==================================
2022-10-21 16:52:02,827:INFO:Initializing create_model()
2022-10-21 16:52:02,827:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=dummy, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca1452c40>, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:52:02,828:INFO:Checking exceptions
2022-10-21 16:52:02,831:INFO:Importing libraries
2022-10-21 16:52:02,831:INFO:Copying training dataset
2022-10-21 16:52:02,845:INFO:Defining folds
2022-10-21 16:52:02,846:INFO:Declaring metric variables
2022-10-21 16:52:02,925:INFO:Importing untrained model
2022-10-21 16:52:02,941:INFO:Dummy Regressor Imported successfully
2022-10-21 16:52:02,991:INFO:Starting cross validation
2022-10-21 16:52:02,995:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 16:52:03,433:INFO:Calculating mean and std
2022-10-21 16:52:03,442:INFO:Creating metrics dataframe
2022-10-21 16:52:03,456:INFO:Uploading results into container
2022-10-21 16:52:03,458:INFO:Uploading model into container now
2022-10-21 16:52:03,459:INFO:master_model_container: 19
2022-10-21 16:52:03,459:INFO:display_container: 2
2022-10-21 16:52:03,459:INFO:DummyRegressor()
2022-10-21 16:52:03,459:INFO:create_model() successfully completed......................................
2022-10-21 16:52:03,652:INFO:SubProcess create_model() end ==================================
2022-10-21 16:52:03,652:INFO:Creating metrics dataframe
2022-10-21 16:52:03,694:INFO:Initializing create_model()
2022-10-21 16:52:03,694:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 16:52:03,694:INFO:Checking exceptions
2022-10-21 16:52:03,701:INFO:Importing libraries
2022-10-21 16:52:03,701:INFO:Copying training dataset
2022-10-21 16:52:03,707:INFO:Defining folds
2022-10-21 16:52:03,708:INFO:Declaring metric variables
2022-10-21 16:52:03,708:INFO:Importing untrained model
2022-10-21 16:52:03,708:INFO:Declaring custom model
2022-10-21 16:52:03,710:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 16:52:03,714:INFO:Cross validation set to False
2022-10-21 16:52:03,714:INFO:Fitting Model
2022-10-21 16:52:04,005:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 16:52:04,005:INFO:create_model() successfully completed......................................
2022-10-21 16:52:04,210:INFO:Creating Dashboard logs
2022-10-21 16:52:04,229:INFO:Model: Gradient Boosting Regressor
2022-10-21 16:52:04,277:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 122, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2022-10-21 16:52:04,346:INFO:Initializing predict_model()
2022-10-21 16:52:04,346:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca375aa60>)
2022-10-21 16:52:04,346:INFO:Checking exceptions
2022-10-21 16:52:04,346:INFO:Preloading libraries
2022-10-21 16:52:06,071:INFO:Creating Dashboard logs
2022-10-21 16:52:06,084:INFO:Model: Random Forest Regressor
2022-10-21 16:52:06,208:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 122, 'verbose': 0, 'warm_start': False}
2022-10-21 16:52:07,294:INFO:Creating Dashboard logs
2022-10-21 16:52:07,306:INFO:Model: Light Gradient Boosting Machine
2022-10-21 16:52:07,381:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 122, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2022-10-21 16:52:08,145:INFO:Creating Dashboard logs
2022-10-21 16:52:08,162:INFO:Model: CatBoost Regressor
2022-10-21 16:52:08,260:WARNING:Couldn't get params for model. Exception:
Traceback (most recent call last):
  File "/Users/sage/opt/anaconda3/lib/python3.9/site-packages/pycaret/loggers/dashboard_logger.py", line 78, in log_model
    params = params.get_all_params()
  File "/Users/sage/opt/anaconda3/lib/python3.9/site-packages/catboost/core.py", line 3413, in get_all_params
    raise CatBoostError("There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.")
_catboost.CatBoostError: There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.

2022-10-21 16:52:08,260:INFO:Logged params: {}
2022-10-21 16:52:08,931:INFO:Creating Dashboard logs
2022-10-21 16:52:08,964:INFO:Model: AdaBoost Regressor
2022-10-21 16:52:09,005:INFO:Logged params: {'base_estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 122}
2022-10-21 16:52:10,089:INFO:Creating Dashboard logs
2022-10-21 16:52:10,120:INFO:Model: Extra Trees Regressor
2022-10-21 16:52:10,178:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 122, 'verbose': 0, 'warm_start': False}
2022-10-21 16:52:11,464:INFO:Creating Dashboard logs
2022-10-21 16:52:11,477:INFO:Model: Bayesian Ridge
2022-10-21 16:52:11,717:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'normalize': 'deprecated', 'tol': 0.001, 'verbose': False}
2022-10-21 16:52:12,266:INFO:Creating Dashboard logs
2022-10-21 16:52:12,301:INFO:Model: Least Angle Regression
2022-10-21 16:52:12,388:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 122, 'verbose': False}
2022-10-21 16:52:13,328:INFO:Creating Dashboard logs
2022-10-21 16:52:13,381:INFO:Model: Least Angle Regression
2022-10-21 16:52:13,586:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 122, 'verbose': False}
2022-10-21 16:52:14,588:INFO:Creating Dashboard logs
2022-10-21 16:52:14,670:INFO:Model: Ridge Regression
2022-10-21 16:52:14,712:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': 122, 'solver': 'auto', 'tol': 0.001}
2022-10-21 16:52:15,451:INFO:Creating Dashboard logs
2022-10-21 16:52:15,477:INFO:Model: Lasso Regression
2022-10-21 16:52:15,523:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 122, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2022-10-21 16:52:16,520:INFO:Creating Dashboard logs
2022-10-21 16:52:16,538:INFO:Model: Linear Regression
2022-10-21 16:52:16,575:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'normalize': 'deprecated', 'positive': False}
2022-10-21 16:52:17,543:INFO:Creating Dashboard logs
2022-10-21 16:52:17,573:INFO:Model: Decision Tree Regressor
2022-10-21 16:52:17,672:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 122, 'splitter': 'best'}
2022-10-21 16:52:18,211:INFO:Creating Dashboard logs
2022-10-21 16:52:18,260:INFO:Model: Huber Regressor
2022-10-21 16:52:18,303:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2022-10-21 16:52:19,126:INFO:Creating Dashboard logs
2022-10-21 16:52:19,197:INFO:Model: Orthogonal Matching Pursuit
2022-10-21 16:52:19,329:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2022-10-21 16:52:20,123:INFO:Creating Dashboard logs
2022-10-21 16:52:20,135:INFO:Model: Passive Aggressive Regressor
2022-10-21 16:52:20,213:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 122, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2022-10-21 16:52:21,015:INFO:Creating Dashboard logs
2022-10-21 16:52:21,064:INFO:Model: Elastic Net
2022-10-21 16:52:21,111:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 122, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2022-10-21 16:52:21,814:INFO:Creating Dashboard logs
2022-10-21 16:52:21,828:INFO:Model: K Neighbors Regressor
2022-10-21 16:52:21,911:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2022-10-21 16:52:22,747:INFO:Creating Dashboard logs
2022-10-21 16:52:22,759:INFO:Model: Dummy Regressor
2022-10-21 16:52:22,874:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2022-10-21 16:52:23,831:INFO:master_model_container: 19
2022-10-21 16:52:23,831:INFO:display_container: 2
2022-10-21 16:52:23,837:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 16:52:23,858:INFO:compare_models() successfully completed......................................
2022-10-21 16:52:58,136:INFO:Initializing evaluate_model()
2022-10-21 16:52:58,138:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-21 16:52:58,206:INFO:Initializing plot_model()
2022-10-21 16:52:58,206:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, system=True)
2022-10-21 16:52:58,207:INFO:Checking exceptions
2022-10-21 16:52:58,220:INFO:Preloading libraries
2022-10-21 16:52:58,245:INFO:Copying training dataset
2022-10-21 16:52:58,245:INFO:Plot type: pipeline
2022-10-21 16:52:59,792:INFO:Visual Rendered Successfully
2022-10-21 16:53:00,276:INFO:plot_model() successfully completed......................................
2022-10-21 16:58:36,779:INFO:Initializing predict_model()
2022-10-21 16:58:36,779:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca3649ee0>)
2022-10-21 16:58:36,779:INFO:Checking exceptions
2022-10-21 16:58:36,779:INFO:Preloading libraries
2022-10-21 17:00:12,982:INFO:Initializing create_model()
2022-10-21 17:00:12,984:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 17:00:12,986:INFO:Checking exceptions
2022-10-21 17:00:13,057:INFO:Importing libraries
2022-10-21 17:00:13,058:INFO:Copying training dataset
2022-10-21 17:00:13,068:INFO:Defining folds
2022-10-21 17:00:13,068:INFO:Declaring metric variables
2022-10-21 17:00:13,150:INFO:Importing untrained model
2022-10-21 17:00:13,150:INFO:Declaring custom model
2022-10-21 17:00:13,170:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 17:00:13,198:INFO:Starting cross validation
2022-10-21 17:00:13,201:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 17:00:29,696:INFO:Calculating mean and std
2022-10-21 17:00:29,704:INFO:Creating metrics dataframe
2022-10-21 17:00:29,720:INFO:Finalizing model
2022-10-21 17:00:29,920:INFO:Creating Dashboard logs
2022-10-21 17:00:29,925:INFO:Model: Gradient Boosting Regressor
2022-10-21 17:00:29,963:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 122, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2022-10-21 17:00:30,193:INFO:Initializing predict_model()
2022-10-21 17:00:30,193:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca375a0d0>)
2022-10-21 17:00:30,195:INFO:Checking exceptions
2022-10-21 17:00:30,195:INFO:Preloading libraries
2022-10-21 17:00:31,633:INFO:Uploading results into container
2022-10-21 17:00:31,641:INFO:Uploading model into container now
2022-10-21 17:00:31,667:INFO:master_model_container: 20
2022-10-21 17:00:31,667:INFO:display_container: 4
2022-10-21 17:00:31,669:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 17:00:31,669:INFO:create_model() successfully completed......................................
2022-10-21 17:00:32,801:INFO:Initializing predict_model()
2022-10-21 17:00:32,801:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca32ad670>)
2022-10-21 17:00:32,801:INFO:Checking exceptions
2022-10-21 17:00:32,801:INFO:Preloading libraries
2022-10-21 17:02:22,493:INFO:Initializing save_model()
2022-10-21 17:02:22,495:INFO:save_model(model=GradientBoostingRegressor(random_state=122), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 17:02:22,495:INFO:Adding model into prep_pipe
2022-10-21 17:02:22,527:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 17:02:22,555:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 17:02:22,555:INFO:save_model() successfully completed......................................
2022-10-21 17:03:21,415:INFO:Initializing interpret_model()
2022-10-21 17:03:21,417:INFO:interpret_model(estimator=GradientBoostingRegressor(random_state=122), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>)
2022-10-21 17:03:21,417:INFO:Checking exceptions
2022-10-21 17:03:21,417:INFO:Soft dependency imported: shap: 0.41.0
2022-10-21 17:04:07,604:INFO:Initializing plot_model()
2022-10-21 17:04:07,606:INFO:plot_model(plot=residuals, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, system=True)
2022-10-21 17:04:07,606:INFO:Checking exceptions
2022-10-21 17:04:07,620:INFO:Preloading libraries
2022-10-21 17:04:07,636:INFO:Copying training dataset
2022-10-21 17:04:07,636:INFO:Plot type: residuals
2022-10-21 17:04:08,018:INFO:Fitting Model
2022-10-21 17:04:08,103:INFO:Scoring test/hold-out set
2022-10-21 17:04:09,296:INFO:Visual Rendered Successfully
2022-10-21 17:04:09,510:INFO:plot_model() successfully completed......................................
2022-10-21 17:05:20,352:INFO:Initializing plot_model()
2022-10-21 17:05:20,354:INFO:plot_model(plot=error, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, system=True)
2022-10-21 17:05:20,355:INFO:Checking exceptions
2022-10-21 17:05:20,373:INFO:Preloading libraries
2022-10-21 17:05:20,388:INFO:Copying training dataset
2022-10-21 17:05:20,388:INFO:Plot type: error
2022-10-21 17:05:20,718:INFO:Fitting Model
2022-10-21 17:05:20,719:INFO:Scoring test/hold-out set
2022-10-21 17:05:21,187:INFO:Visual Rendered Successfully
2022-10-21 17:05:21,513:INFO:plot_model() successfully completed......................................
2022-10-21 17:05:53,157:INFO:Initializing plot_model()
2022-10-21 17:05:53,158:INFO:plot_model(plot=feature, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, system=True)
2022-10-21 17:05:53,158:INFO:Checking exceptions
2022-10-21 17:05:53,169:INFO:Preloading libraries
2022-10-21 17:05:53,186:INFO:Copying training dataset
2022-10-21 17:05:53,186:INFO:Plot type: feature
2022-10-21 17:05:53,186:WARNING:No coef_ found. Trying feature_importances_
2022-10-21 17:05:53,683:INFO:Visual Rendered Successfully
2022-10-21 17:05:53,891:INFO:plot_model() successfully completed......................................
2022-10-21 17:07:19,417:INFO:Initializing evaluate_model()
2022-10-21 17:07:19,419:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-21 17:07:19,460:INFO:Initializing plot_model()
2022-10-21 17:07:19,460:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, system=True)
2022-10-21 17:07:19,460:INFO:Checking exceptions
2022-10-21 17:07:19,466:INFO:Preloading libraries
2022-10-21 17:07:19,484:INFO:Copying training dataset
2022-10-21 17:07:19,484:INFO:Plot type: pipeline
2022-10-21 17:07:21,173:INFO:Visual Rendered Successfully
2022-10-21 17:07:21,910:INFO:plot_model() successfully completed......................................
2022-10-21 17:07:57,723:INFO:Initializing interpret_model()
2022-10-21 17:07:57,726:INFO:interpret_model(estimator=GradientBoostingRegressor(random_state=122), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>)
2022-10-21 17:07:57,731:INFO:Checking exceptions
2022-10-21 17:07:57,731:INFO:Soft dependency imported: shap: 0.41.0
2022-10-21 17:09:01,438:INFO:Initializing predict_model()
2022-10-21 17:09:01,440:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca744d700>)
2022-10-21 17:09:01,440:INFO:Checking exceptions
2022-10-21 17:09:01,441:INFO:Preloading libraries
2022-10-21 17:09:17,936:INFO:Initializing predict_model()
2022-10-21 17:09:17,938:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca744db80>)
2022-10-21 17:09:17,938:INFO:Checking exceptions
2022-10-21 17:09:17,938:INFO:Preloading libraries
2022-10-21 17:09:37,143:INFO:Initializing save_model()
2022-10-21 17:09:37,145:INFO:save_model(model=GradientBoostingRegressor(random_state=122), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 17:09:37,145:INFO:Adding model into prep_pipe
2022-10-21 17:09:37,173:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 17:09:37,203:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 17:09:37,204:INFO:save_model() successfully completed......................................
2022-10-21 17:10:10,117:INFO:Initializing predict_model()
2022-10-21 17:10:10,121:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fb8387c2b20>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fb8389894c0>)
2022-10-21 17:10:10,122:INFO:Checking exceptions
2022-10-21 17:10:10,122:INFO:Preloading libraries
2022-10-21 17:10:10,130:INFO:Set up data.
2022-10-21 17:10:10,178:INFO:Set up index.
2022-10-21 17:10:54,708:INFO:Initializing predict_model()
2022-10-21 17:10:54,709:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7facbbd81e50>)
2022-10-21 17:10:54,710:INFO:Checking exceptions
2022-10-21 17:10:54,710:INFO:Preloading libraries
2022-10-21 17:11:11,902:INFO:Initializing predict_model()
2022-10-21 17:11:11,904:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca744daf0>)
2022-10-21 17:11:11,905:INFO:Checking exceptions
2022-10-21 17:11:11,905:INFO:Preloading libraries
2022-10-21 17:13:20,824:INFO:Initializing create_model()
2022-10-21 17:13:20,825:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 17:13:20,826:INFO:Checking exceptions
2022-10-21 17:13:20,890:INFO:Importing libraries
2022-10-21 17:13:20,890:INFO:Copying training dataset
2022-10-21 17:13:20,901:INFO:Defining folds
2022-10-21 17:13:20,901:INFO:Declaring metric variables
2022-10-21 17:13:20,914:INFO:Importing untrained model
2022-10-21 17:13:20,965:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-21 17:13:20,986:INFO:Starting cross validation
2022-10-21 17:13:20,988:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 17:13:40,906:INFO:Calculating mean and std
2022-10-21 17:13:40,916:INFO:Creating metrics dataframe
2022-10-21 17:13:40,935:INFO:Finalizing model
2022-10-21 17:13:41,157:INFO:Creating Dashboard logs
2022-10-21 17:13:41,165:INFO:Model: Light Gradient Boosting Machine
2022-10-21 17:13:41,239:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 122, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2022-10-21 17:13:41,522:INFO:Initializing predict_model()
2022-10-21 17:13:41,522:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=LGBMRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca754b9d0>)
2022-10-21 17:13:41,522:INFO:Checking exceptions
2022-10-21 17:13:41,522:INFO:Preloading libraries
2022-10-21 17:13:43,376:INFO:Uploading results into container
2022-10-21 17:13:43,377:INFO:Uploading model into container now
2022-10-21 17:13:43,406:INFO:master_model_container: 21
2022-10-21 17:13:43,409:INFO:display_container: 10
2022-10-21 17:13:43,410:INFO:LGBMRegressor(random_state=122)
2022-10-21 17:13:43,412:INFO:create_model() successfully completed......................................
2022-10-21 17:13:44,096:INFO:Initializing predict_model()
2022-10-21 17:13:44,097:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=LGBMRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca00c8e50>)
2022-10-21 17:13:44,097:INFO:Checking exceptions
2022-10-21 17:13:44,097:INFO:Preloading libraries
2022-10-21 17:14:23,018:INFO:Initializing create_model()
2022-10-21 17:14:23,020:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-21 17:14:23,020:INFO:Checking exceptions
2022-10-21 17:14:23,098:INFO:Importing libraries
2022-10-21 17:14:23,099:INFO:Copying training dataset
2022-10-21 17:14:23,111:INFO:Defining folds
2022-10-21 17:14:23,111:INFO:Declaring metric variables
2022-10-21 17:14:23,120:INFO:Importing untrained model
2022-10-21 17:14:23,121:INFO:Declaring custom model
2022-10-21 17:14:23,129:INFO:Gradient Boosting Regressor Imported successfully
2022-10-21 17:14:23,149:INFO:Starting cross validation
2022-10-21 17:14:23,153:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-21 17:14:25,340:INFO:Calculating mean and std
2022-10-21 17:14:25,347:INFO:Creating metrics dataframe
2022-10-21 17:14:25,366:INFO:Finalizing model
2022-10-21 17:14:25,644:INFO:Creating Dashboard logs
2022-10-21 17:14:25,650:INFO:Model: Gradient Boosting Regressor
2022-10-21 17:14:25,718:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 122, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2022-10-21 17:14:25,839:INFO:Initializing predict_model()
2022-10-21 17:14:25,839:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca754b430>)
2022-10-21 17:14:25,839:INFO:Checking exceptions
2022-10-21 17:14:25,839:INFO:Preloading libraries
2022-10-21 17:14:27,379:INFO:Uploading results into container
2022-10-21 17:14:27,383:INFO:Uploading model into container now
2022-10-21 17:14:27,431:INFO:master_model_container: 22
2022-10-21 17:14:27,431:INFO:display_container: 12
2022-10-21 17:14:27,437:INFO:GradientBoostingRegressor(random_state=122)
2022-10-21 17:14:27,438:INFO:create_model() successfully completed......................................
2022-10-21 17:14:28,499:INFO:Initializing predict_model()
2022-10-21 17:14:28,499:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca00c8af0>)
2022-10-21 17:14:28,499:INFO:Checking exceptions
2022-10-21 17:14:28,499:INFO:Preloading libraries
2022-10-21 17:15:09,677:INFO:Initializing load_model()
2022-10-21 17:15:09,679:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 17:16:28,147:INFO:Initializing load_model()
2022-10-21 17:16:28,149:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 17:17:31,374:INFO:Initializing predict_model()
2022-10-21 17:17:31,377:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fb8387bb970>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fb838989550>)
2022-10-21 17:17:31,377:INFO:Checking exceptions
2022-10-21 17:17:31,377:INFO:Preloading libraries
2022-10-21 17:17:31,379:INFO:Set up data.
2022-10-21 17:17:31,395:INFO:Set up index.
2022-10-21 17:21:56,492:INFO:Initializing predict_model()
2022-10-21 17:21:56,493:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca744d820>)
2022-10-21 17:21:56,494:INFO:Checking exceptions
2022-10-21 17:21:56,494:INFO:Preloading libraries
2022-10-21 17:22:02,753:INFO:Initializing save_model()
2022-10-21 17:22:02,753:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 17:22:02,753:INFO:Adding model into prep_pipe
2022-10-21 17:22:02,773:WARNING:Only Model saved as it was a pipeline.
2022-10-21 17:22:02,793:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 17:22:02,823:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 17:22:02,823:INFO:save_model() successfully completed......................................
2022-10-21 17:22:14,501:INFO:Initializing save_model()
2022-10-21 17:22:14,502:INFO:save_model(model=       age  sex        bmi  children  smoker  region_southeast  \
936   47.0  0.0  32.000000       1.0     0.0               0.0   
937   49.0  1.0  30.299999       0.0     0.0               0.0   
938   35.0  1.0  34.770000       2.0     0.0               0.0   
939   37.0  1.0  30.875000       3.0     0.0               0.0   
940   29.0  1.0  33.345001       2.0     0.0               0.0   
...    ...  ...        ...       ...     ...               ...   
1333  50.0  0.0  28.160000       3.0     0.0               1.0   
1334  33.0  0.0  33.500000       0.0     1.0               0.0   
1335  44.0  1.0  32.014999       2.0     0.0               0.0   
1336  54.0  0.0  46.700001       2.0     0.0               0.0   
1337  42.0  1.0  37.180000       2.0     0.0               1.0   

      region_northeast  region_northwest  region_southwest       charges  \
936                0.0               0.0               1.0   8551.346680   
937                0.0               0.0               1.0   8116.680176   
938                0.0               1.0               0.0   5729.005371   
939                0.0               1.0               0.0   6796.863281   
940                0.0               1.0               0.0  19442.353516   
...                ...               ...               ...           ...   
1333               0.0               0.0               0.0  10702.642578   
1334               0.0               0.0               1.0  37079.371094   
1335               0.0               1.0               0.0   8116.269043   
1336               0.0               0.0               1.0  11538.420898   
1337               0.0               0.0               0.0   7162.012207   

             Label  
936    9675.945455  
937    9646.647798  
938    7834.242785  
939    8054.114617  
940    6693.952915  
...            ...  
1333  12715.908015  
1334  38473.417291  
1335  10241.252879  
1336  13079.765854  
1337   7906.048828  

[402 rows x 11 columns], model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 17:22:14,503:INFO:Adding model into prep_pipe
2022-10-21 17:22:14,515:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 17:22:14,808:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
939                0.0               1.0               0.0   6796.863281   
940                0.0               1.0               0.0  19442.353516   
...                ...               ...               ...           ...   
1333               0.0               0.0               0.0  10702.642578   
1334               0.0               0.0               1.0  37079.371094   
1335               0.0               1.0               0.0   8116.269043   
1336               0.0               0.0               1.0  11538.420898   
1337               0.0               0.0               0.0   7162.012207   

             Label  
936    9675.945455  
937    9646.647798  
938    7834.242785  
939    8054.114617  
940    6693.952915  
...            ...  
1333  12715.908015  
1334  38473.417291  
1335  10241.252879  
1336  13079.765854  
1337   7906.048828  

[402 rows x 11 columns])])
2022-10-21 17:22:14,809:INFO:save_model() successfully completed......................................
2022-10-21 17:22:32,621:INFO:Initializing save_model()
2022-10-21 17:22:32,621:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 17:22:32,622:INFO:Adding model into prep_pipe
2022-10-21 17:22:32,644:WARNING:Only Model saved as it was a pipeline.
2022-10-21 17:22:32,677:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 17:22:32,712:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 17:22:32,712:INFO:save_model() successfully completed......................................
2022-10-21 17:23:45,896:INFO:Initializing save_model()
2022-10-21 17:23:45,897:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-21 17:23:45,897:INFO:Adding model into prep_pipe
2022-10-21 17:23:45,933:WARNING:Only Model saved as it was a pipeline.
2022-10-21 17:23:45,958:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-21 17:23:45,993:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-21 17:23:45,993:INFO:save_model() successfully completed......................................
2022-10-21 17:23:49,812:INFO:Initializing load_model()
2022-10-21 17:23:49,813:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-21 17:24:53,337:INFO:Initializing predict_model()
2022-10-21 17:24:53,339:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fb8387cba90>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', GradientBoostingRegressor(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fb838989160>)
2022-10-21 17:24:53,339:INFO:Checking exceptions
2022-10-21 17:24:53,339:INFO:Preloading libraries
2022-10-21 17:24:53,339:INFO:Set up data.
2022-10-21 17:24:53,344:INFO:Set up index.
2022-10-21 17:25:32,492:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 17:25:32,492:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 17:25:32,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 17:25:32,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-21 17:25:35,005:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-21 17:25:36,431:INFO:Initializing load_model()
2022-10-21 17:25:36,431:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-22 19:25:36,464:INFO:Initializing predict_model()
2022-10-22 19:25:36,466:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fe3a27e08e0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fe3a27ae1f0>)
2022-10-22 19:25:36,466:INFO:Checking exceptions
2022-10-22 19:25:36,466:INFO:Preloading libraries
2022-10-22 19:25:36,479:INFO:Set up data.
2022-10-22 19:25:36,551:INFO:Set up index.
2022-10-23 13:17:00,866:INFO:Initializing predict_model()
2022-10-23 13:17:00,878:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca744d700>)
2022-10-23 13:17:00,878:INFO:Checking exceptions
2022-10-23 13:17:00,878:INFO:Preloading libraries
2022-10-23 13:17:00,965:INFO:Set up data.
2022-10-23 13:17:01,180:INFO:Set up index.
2022-10-23 13:17:35,130:INFO:Initializing predict_model()
2022-10-23 13:17:35,132:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca375a280>)
2022-10-23 13:17:35,133:INFO:Checking exceptions
2022-10-23 13:17:35,134:INFO:Preloading libraries
2022-10-23 13:17:35,155:INFO:Set up data.
2022-10-23 13:17:35,179:INFO:Set up index.
2022-10-23 13:17:50,244:INFO:Initializing predict_model()
2022-10-23 13:17:50,245:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca375a0d0>)
2022-10-23 13:17:50,246:INFO:Checking exceptions
2022-10-23 13:17:50,246:INFO:Preloading libraries
2022-10-23 13:17:50,278:INFO:Set up data.
2022-10-23 13:17:50,290:INFO:Set up index.
2022-10-23 13:19:03,768:INFO:Initializing save_model()
2022-10-23 13:19:03,770:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-23 13:19:03,770:INFO:Adding model into prep_pipe
2022-10-23 13:19:03,964:WARNING:Only Model saved as it was a pipeline.
2022-10-23 13:19:04,012:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-23 13:19:04,076:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-23 13:19:04,077:INFO:save_model() successfully completed......................................
2022-10-23 13:23:32,029:INFO:Initializing predict_model()
2022-10-23 13:23:32,032:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fe3a27c57c0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fe3a396b3a0>)
2022-10-23 13:23:32,032:INFO:Checking exceptions
2022-10-23 13:23:32,032:INFO:Preloading libraries
2022-10-23 13:23:32,039:INFO:Set up data.
2022-10-23 13:23:32,077:INFO:Set up index.
2022-10-23 13:24:45,336:INFO:Initializing predict_model()
2022-10-23 13:24:45,337:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7faca14defa0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca375a5e0>)
2022-10-23 13:24:45,337:INFO:Checking exceptions
2022-10-23 13:24:45,337:INFO:Preloading libraries
2022-10-23 13:24:45,383:INFO:Set up data.
2022-10-23 13:24:45,432:INFO:Set up index.
2022-10-23 13:26:04,851:INFO:PyCaret RegressionExperiment
2022-10-23 13:26:04,855:INFO:Logging name: insurance1
2022-10-23 13:26:04,858:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-23 13:26:04,858:INFO:version 3.0.0.rc4
2022-10-23 13:26:04,859:INFO:Initializing setup()
2022-10-23 13:26:04,859:INFO:self.USI: c707
2022-10-23 13:26:04,859:INFO:self.variable_keys: {'_available_plots', '_all_metrics', '_all_models_internal', 'X_train', 'display_container', 'gpu_param', 'idx', 'transform_target_method_param', 'html_param', 'exp_id', 'exp_name_log', 'USI', 'log_plots_param', 'variable_keys', 'seed', 'n_jobs_param', 'y_test', 'y_train', '_all_models', 'transform_target_param', '_ml_usecase', 'logging_param', 'data', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'X', 'fold_generator', 'y', 'master_model_container', 'X_test', 'target_param', '_gpu_n_jobs_param', 'pipeline'}
2022-10-23 13:26:04,860:INFO:Checking environment
2022-10-23 13:26:04,861:INFO:python_version: 3.9.7
2022-10-23 13:26:04,861:INFO:python_build: ('default', 'Sep 16 2021 08:50:36')
2022-10-23 13:26:04,861:INFO:machine: x86_64
2022-10-23 13:26:04,861:INFO:platform: macOS-10.16-x86_64-i386-64bit
2022-10-23 13:26:04,874:INFO:Memory: svmem(total=8589934592, available=2214682624, percent=74.2, used=4175187968, free=17543168, active=2199683072, inactive=2195558400, wired=1975504896)
2022-10-23 13:26:04,874:INFO:Physical Core: 2
2022-10-23 13:26:04,875:INFO:Logical Core: 4
2022-10-23 13:26:04,875:INFO:Checking libraries
2022-10-23 13:26:04,875:INFO:System:
2022-10-23 13:26:04,875:INFO:    python: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ]
2022-10-23 13:26:04,875:INFO:executable: /Users/sage/opt/anaconda3/bin/python
2022-10-23 13:26:04,875:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2022-10-23 13:26:04,875:INFO:PyCaret required dependencies:
2022-10-23 13:26:04,875:INFO:                 pip: 21.2.4
2022-10-23 13:26:04,875:INFO:          setuptools: 58.0.4
2022-10-23 13:26:04,875:INFO:             pycaret: 3.0.0rc4
2022-10-23 13:26:04,875:INFO:             IPython: 7.29.0
2022-10-23 13:26:04,875:INFO:          ipywidgets: 7.6.5
2022-10-23 13:26:04,875:INFO:                tqdm: 4.62.3
2022-10-23 13:26:04,875:INFO:               numpy: 1.22.4
2022-10-23 13:26:04,875:INFO:              pandas: 1.4.4
2022-10-23 13:26:04,875:INFO:              jinja2: 3.1.2
2022-10-23 13:26:04,875:INFO:               scipy: 1.8.1
2022-10-23 13:26:04,876:INFO:              joblib: 1.1.0
2022-10-23 13:26:04,876:INFO:             sklearn: 1.0.2
2022-10-23 13:26:04,876:INFO:                pyod: 1.0.5
2022-10-23 13:26:04,876:INFO:            imblearn: 0.9.0
2022-10-23 13:26:04,876:INFO:   category_encoders: 2.5.1.post0
2022-10-23 13:26:04,876:INFO:            lightgbm: 3.3.2
2022-10-23 13:26:04,876:INFO:               numba: 0.55.2
2022-10-23 13:26:04,876:INFO:            requests: 2.28.1
2022-10-23 13:26:04,876:INFO:          matplotlib: 3.4.3
2022-10-23 13:26:04,876:INFO:          scikitplot: 0.3.7
2022-10-23 13:26:04,876:INFO:         yellowbrick: 1.4
2022-10-23 13:26:04,876:INFO:              plotly: 5.5.0
2022-10-23 13:26:04,876:INFO:             kaleido: 0.2.1
2022-10-23 13:26:04,876:INFO:         statsmodels: 0.13.2
2022-10-23 13:26:04,876:INFO:              sktime: 0.13.4
2022-10-23 13:26:04,877:INFO:               tbats: 1.1.1
2022-10-23 13:26:04,877:INFO:            pmdarima: 1.8.5
2022-10-23 13:26:04,877:INFO:              psutil: 5.9.2
2022-10-23 13:26:04,877:INFO:PyCaret optional dependencies:
2022-10-23 13:26:04,877:INFO:                shap: 0.41.0
2022-10-23 13:26:04,878:INFO:           interpret: Not installed
2022-10-23 13:26:04,878:INFO:                umap: Not installed
2022-10-23 13:26:04,878:INFO:    pandas_profiling: Not installed
2022-10-23 13:26:04,878:INFO:  explainerdashboard: Not installed
2022-10-23 13:26:04,878:INFO:             autoviz: Not installed
2022-10-23 13:26:04,878:INFO:           fairlearn: Not installed
2022-10-23 13:26:04,878:INFO:             xgboost: Not installed
2022-10-23 13:26:04,878:INFO:            catboost: 1.1
2022-10-23 13:26:04,878:INFO:              kmodes: Not installed
2022-10-23 13:26:04,878:INFO:             mlxtend: Not installed
2022-10-23 13:26:04,878:INFO:       statsforecast: 1.1.1
2022-10-23 13:26:04,878:INFO:        tune_sklearn: Not installed
2022-10-23 13:26:04,878:INFO:                 ray: Not installed
2022-10-23 13:26:04,878:INFO:            hyperopt: Not installed
2022-10-23 13:26:04,879:INFO:              optuna: Not installed
2022-10-23 13:26:04,879:INFO:               skopt: Not installed
2022-10-23 13:26:04,879:INFO:              mlflow: 1.29.0
2022-10-23 13:26:04,879:INFO:              gradio: Not installed
2022-10-23 13:26:04,879:INFO:             fastapi: Not installed
2022-10-23 13:26:04,879:INFO:             uvicorn: Not installed
2022-10-23 13:26:04,887:INFO:              m2cgen: Not installed
2022-10-23 13:26:04,887:INFO:           evidently: Not installed
2022-10-23 13:26:04,887:INFO:                nltk: 3.6.5
2022-10-23 13:26:04,887:INFO:            pyLDAvis: Not installed
2022-10-23 13:26:04,887:INFO:              gensim: Not installed
2022-10-23 13:26:04,888:INFO:               spacy: Not installed
2022-10-23 13:26:04,888:INFO:           wordcloud: Not installed
2022-10-23 13:26:04,890:INFO:            textblob: Not installed
2022-10-23 13:26:04,890:INFO:               fugue: Not installed
2022-10-23 13:26:04,890:INFO:           streamlit: Not installed
2022-10-23 13:26:04,891:INFO:             prophet: 1.1.1
2022-10-23 13:26:04,891:INFO:None
2022-10-23 13:26:04,891:INFO:Set up data.
2022-10-23 13:26:04,927:INFO:Set up train/test split.
2022-10-23 13:26:05,090:INFO:Set up index.
2022-10-23 13:26:05,090:INFO:Set up folding strategy.
2022-10-23 13:26:05,090:INFO:Assigning column types.
2022-10-23 13:26:05,099:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-23 13:26:05,100:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-23 13:26:05,160:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-23 13:26:05,215:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-23 13:26:05,845:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:05,995:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:05,997:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:05,998:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:06,006:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,025:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,061:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,288:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,378:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,381:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:06,385:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:06,387:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-23 13:26:06,401:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,415:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,658:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,801:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,802:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:06,803:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:06,831:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-23 13:26:06,866:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-23 13:26:07,103:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:07,380:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:07,389:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:07,390:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:07,393:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-23 13:26:07,425:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-23 13:26:07,658:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:07,864:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:07,866:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:07,867:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:07,898:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-23 13:26:08,055:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:08,159:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:08,160:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:08,160:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:08,161:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-23 13:26:08,327:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:08,538:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:08,539:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:08,540:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:08,714:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:08,799:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-23 13:26:08,799:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:08,800:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:08,800:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-23 13:26:08,929:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:09,011:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:09,012:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:09,145:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-23 13:26:09,234:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:09,235:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:09,236:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-23 13:26:09,470:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:09,473:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:09,708:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:09,709:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:09,712:INFO:Preparing preprocessing pipeline...
2022-10-23 13:26:09,714:INFO:Set up simple imputation.
2022-10-23 13:26:09,739:INFO:Set up encoding of ordinal features.
2022-10-23 13:26:09,746:INFO:Set up encoding of categorical features.
2022-10-23 13:26:09,746:INFO:Set up variance threshold.
2022-10-23 13:26:09,832:INFO:Finished creating preprocessing pipeline.
2022-10-23 13:26:09,855:INFO:Pipeline: Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-23 13:26:09,855:INFO:Creating final display dataframe.
2022-10-23 13:26:10,682:INFO:Setup display_container:                  Description         Value
0                 Session id           122
1                     Target       charges
2                Target type    Regression
3                 Data shape    (1338, 10)
4           Train data shape     (936, 10)
5            Test data shape     (402, 10)
6           Ordinal features             2
7           Numeric features             3
8       Categorical features             3
9                 Preprocess          True
10           Imputation type        simple
11        Numeric imputation          mean
12    Categorical imputation      constant
13  Maximum one-hot encoding             5
14           Encoding method          None
15    Low variance threshold             0
16            Fold Generator         KFold
17               Fold Number            10
18                  CPU Jobs            -1
19                   Use GPU         False
20            Log Experiment  MlflowLogger
21           Experiment Name    insurance1
22                       USI          c707
2022-10-23 13:26:11,136:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:11,137:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:11,503:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-23 13:26:11,503:INFO:Soft dependency imported: catboost: 1.1
2022-10-23 13:26:11,520:INFO:Logging experiment in loggers
2022-10-23 13:26:11,683:INFO:SubProcess save_model() called ==================================
2022-10-23 13:26:11,777:INFO:Initializing save_model()
2022-10-23 13:26:11,778:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), model_name=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/tmpwiumygwx/Transformation Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-23 13:26:11,778:INFO:Adding model into prep_pipe
2022-10-23 13:26:11,781:WARNING:Only Model saved as it was a pipeline.
2022-10-23 13:26:11,790:INFO:/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/tmpwiumygwx/Transformation Pipeline.pkl saved in current working directory
2022-10-23 13:26:11,811:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-23 13:26:11,811:INFO:save_model() successfully completed......................................
2022-10-23 13:26:12,458:INFO:SubProcess save_model() end ==================================
2022-10-23 13:26:12,500:INFO:setup() successfully completed in 6.68s...............
2022-10-23 13:26:12,502:INFO:Initializing compare_models()
2022-10-23 13:26:12,504:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, include=None, fold=5, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-23 13:26:12,505:INFO:Checking exceptions
2022-10-23 13:26:12,533:INFO:Preparing display monitor
2022-10-23 13:26:12,988:INFO:Initializing Linear Regression
2022-10-23 13:26:12,988:INFO:Total runtime is 3.3656756083170574e-06 minutes
2022-10-23 13:26:13,007:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:13,011:INFO:Initializing create_model()
2022-10-23 13:26:13,012:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=lr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:13,012:INFO:Checking exceptions
2022-10-23 13:26:13,019:INFO:Importing libraries
2022-10-23 13:26:13,019:INFO:Copying training dataset
2022-10-23 13:26:13,032:INFO:Defining folds
2022-10-23 13:26:13,032:INFO:Declaring metric variables
2022-10-23 13:26:13,126:INFO:Importing untrained model
2022-10-23 13:26:13,162:INFO:Linear Regression Imported successfully
2022-10-23 13:26:13,248:INFO:Starting cross validation
2022-10-23 13:26:13,251:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:43,094:INFO:Calculating mean and std
2022-10-23 13:26:43,104:INFO:Creating metrics dataframe
2022-10-23 13:26:43,125:INFO:Uploading results into container
2022-10-23 13:26:43,127:INFO:Uploading model into container now
2022-10-23 13:26:43,129:INFO:master_model_container: 1
2022-10-23 13:26:43,129:INFO:display_container: 2
2022-10-23 13:26:43,130:INFO:LinearRegression(n_jobs=-1)
2022-10-23 13:26:43,133:INFO:create_model() successfully completed......................................
2022-10-23 13:26:43,542:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:43,542:INFO:Creating metrics dataframe
2022-10-23 13:26:43,611:INFO:Initializing Lasso Regression
2022-10-23 13:26:43,612:INFO:Total runtime is 0.5104029138882955 minutes
2022-10-23 13:26:43,629:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:43,630:INFO:Initializing create_model()
2022-10-23 13:26:43,630:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=lasso, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:43,630:INFO:Checking exceptions
2022-10-23 13:26:43,635:INFO:Importing libraries
2022-10-23 13:26:43,642:INFO:Copying training dataset
2022-10-23 13:26:43,748:INFO:Defining folds
2022-10-23 13:26:43,749:INFO:Declaring metric variables
2022-10-23 13:26:43,775:INFO:Importing untrained model
2022-10-23 13:26:43,832:INFO:Lasso Regression Imported successfully
2022-10-23 13:26:43,944:INFO:Starting cross validation
2022-10-23 13:26:43,949:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:44,523:INFO:Calculating mean and std
2022-10-23 13:26:44,526:INFO:Creating metrics dataframe
2022-10-23 13:26:44,532:INFO:Uploading results into container
2022-10-23 13:26:44,533:INFO:Uploading model into container now
2022-10-23 13:26:44,535:INFO:master_model_container: 2
2022-10-23 13:26:44,536:INFO:display_container: 2
2022-10-23 13:26:44,539:INFO:Lasso(random_state=122)
2022-10-23 13:26:44,539:INFO:create_model() successfully completed......................................
2022-10-23 13:26:44,752:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:44,752:INFO:Creating metrics dataframe
2022-10-23 13:26:44,774:INFO:Initializing Ridge Regression
2022-10-23 13:26:44,774:INFO:Total runtime is 0.5297764301300049 minutes
2022-10-23 13:26:44,781:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:44,782:INFO:Initializing create_model()
2022-10-23 13:26:44,782:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=ridge, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:44,782:INFO:Checking exceptions
2022-10-23 13:26:44,787:INFO:Importing libraries
2022-10-23 13:26:44,789:INFO:Copying training dataset
2022-10-23 13:26:44,800:INFO:Defining folds
2022-10-23 13:26:44,800:INFO:Declaring metric variables
2022-10-23 13:26:44,845:INFO:Importing untrained model
2022-10-23 13:26:44,889:INFO:Ridge Regression Imported successfully
2022-10-23 13:26:44,962:INFO:Starting cross validation
2022-10-23 13:26:44,969:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:45,529:INFO:Calculating mean and std
2022-10-23 13:26:45,538:INFO:Creating metrics dataframe
2022-10-23 13:26:45,552:INFO:Uploading results into container
2022-10-23 13:26:45,554:INFO:Uploading model into container now
2022-10-23 13:26:45,556:INFO:master_model_container: 3
2022-10-23 13:26:45,556:INFO:display_container: 2
2022-10-23 13:26:45,556:INFO:Ridge(random_state=122)
2022-10-23 13:26:45,556:INFO:create_model() successfully completed......................................
2022-10-23 13:26:45,769:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:45,769:INFO:Creating metrics dataframe
2022-10-23 13:26:45,791:INFO:Initializing Elastic Net
2022-10-23 13:26:45,792:INFO:Total runtime is 0.5467427492141723 minutes
2022-10-23 13:26:45,799:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:45,800:INFO:Initializing create_model()
2022-10-23 13:26:45,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=en, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:45,800:INFO:Checking exceptions
2022-10-23 13:26:45,804:INFO:Importing libraries
2022-10-23 13:26:45,805:INFO:Copying training dataset
2022-10-23 13:26:45,818:INFO:Defining folds
2022-10-23 13:26:45,818:INFO:Declaring metric variables
2022-10-23 13:26:45,828:INFO:Importing untrained model
2022-10-23 13:26:45,839:INFO:Elastic Net Imported successfully
2022-10-23 13:26:45,859:INFO:Starting cross validation
2022-10-23 13:26:45,862:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:46,521:INFO:Calculating mean and std
2022-10-23 13:26:46,525:INFO:Creating metrics dataframe
2022-10-23 13:26:46,531:INFO:Uploading results into container
2022-10-23 13:26:46,532:INFO:Uploading model into container now
2022-10-23 13:26:46,535:INFO:master_model_container: 4
2022-10-23 13:26:46,535:INFO:display_container: 2
2022-10-23 13:26:46,536:INFO:ElasticNet(random_state=122)
2022-10-23 13:26:46,536:INFO:create_model() successfully completed......................................
2022-10-23 13:26:46,783:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:46,783:INFO:Creating metrics dataframe
2022-10-23 13:26:46,807:INFO:Initializing Least Angle Regression
2022-10-23 13:26:46,807:INFO:Total runtime is 0.5636502146720885 minutes
2022-10-23 13:26:46,815:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:46,815:INFO:Initializing create_model()
2022-10-23 13:26:46,815:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=lar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:46,815:INFO:Checking exceptions
2022-10-23 13:26:46,820:INFO:Importing libraries
2022-10-23 13:26:46,820:INFO:Copying training dataset
2022-10-23 13:26:46,832:INFO:Defining folds
2022-10-23 13:26:46,832:INFO:Declaring metric variables
2022-10-23 13:26:46,866:INFO:Importing untrained model
2022-10-23 13:26:46,926:INFO:Least Angle Regression Imported successfully
2022-10-23 13:26:47,025:INFO:Starting cross validation
2022-10-23 13:26:47,029:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:47,304:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:47,304:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:47,316:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:47,377:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:47,546:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:47,593:INFO:Calculating mean and std
2022-10-23 13:26:47,596:INFO:Creating metrics dataframe
2022-10-23 13:26:47,606:INFO:Uploading results into container
2022-10-23 13:26:47,608:INFO:Uploading model into container now
2022-10-23 13:26:47,611:INFO:master_model_container: 5
2022-10-23 13:26:47,611:INFO:display_container: 2
2022-10-23 13:26:47,616:INFO:Lars(random_state=122)
2022-10-23 13:26:47,616:INFO:create_model() successfully completed......................................
2022-10-23 13:26:47,852:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:47,852:INFO:Creating metrics dataframe
2022-10-23 13:26:47,879:INFO:Initializing Lasso Least Angle Regression
2022-10-23 13:26:47,880:INFO:Total runtime is 0.5815320134162902 minutes
2022-10-23 13:26:47,899:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:47,900:INFO:Initializing create_model()
2022-10-23 13:26:47,900:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=llar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:47,900:INFO:Checking exceptions
2022-10-23 13:26:47,906:INFO:Importing libraries
2022-10-23 13:26:47,907:INFO:Copying training dataset
2022-10-23 13:26:47,985:INFO:Defining folds
2022-10-23 13:26:47,987:INFO:Declaring metric variables
2022-10-23 13:26:48,005:INFO:Importing untrained model
2022-10-23 13:26:48,029:INFO:Lasso Least Angle Regression Imported successfully
2022-10-23 13:26:48,132:INFO:Starting cross validation
2022-10-23 13:26:48,139:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:48,393:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-23 13:26:48,399:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-23 13:26:48,425:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-23 13:26:48,499:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-23 13:26:48,632:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-10-23 13:26:48,679:INFO:Calculating mean and std
2022-10-23 13:26:48,682:INFO:Creating metrics dataframe
2022-10-23 13:26:48,690:INFO:Uploading results into container
2022-10-23 13:26:48,691:INFO:Uploading model into container now
2022-10-23 13:26:48,691:INFO:master_model_container: 6
2022-10-23 13:26:48,691:INFO:display_container: 2
2022-10-23 13:26:48,692:INFO:LassoLars(random_state=122)
2022-10-23 13:26:48,692:INFO:create_model() successfully completed......................................
2022-10-23 13:26:48,899:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:48,900:INFO:Creating metrics dataframe
2022-10-23 13:26:48,930:INFO:Initializing Orthogonal Matching Pursuit
2022-10-23 13:26:48,930:INFO:Total runtime is 0.5990405639012654 minutes
2022-10-23 13:26:48,943:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:48,945:INFO:Initializing create_model()
2022-10-23 13:26:48,946:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=omp, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:48,946:INFO:Checking exceptions
2022-10-23 13:26:48,952:INFO:Importing libraries
2022-10-23 13:26:48,952:INFO:Copying training dataset
2022-10-23 13:26:48,984:INFO:Defining folds
2022-10-23 13:26:48,985:INFO:Declaring metric variables
2022-10-23 13:26:49,073:INFO:Importing untrained model
2022-10-23 13:26:49,094:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-23 13:26:49,167:INFO:Starting cross validation
2022-10-23 13:26:49,176:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:49,424:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:49,456:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:49,503:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:49,505:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:49,685:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-10-23 13:26:49,732:INFO:Calculating mean and std
2022-10-23 13:26:49,735:INFO:Creating metrics dataframe
2022-10-23 13:26:49,739:INFO:Uploading results into container
2022-10-23 13:26:49,741:INFO:Uploading model into container now
2022-10-23 13:26:49,742:INFO:master_model_container: 7
2022-10-23 13:26:49,742:INFO:display_container: 2
2022-10-23 13:26:49,743:INFO:OrthogonalMatchingPursuit()
2022-10-23 13:26:49,743:INFO:create_model() successfully completed......................................
2022-10-23 13:26:49,966:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:49,966:INFO:Creating metrics dataframe
2022-10-23 13:26:50,002:INFO:Initializing Bayesian Ridge
2022-10-23 13:26:50,002:INFO:Total runtime is 0.616911514600118 minutes
2022-10-23 13:26:50,010:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:50,012:INFO:Initializing create_model()
2022-10-23 13:26:50,012:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=br, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:50,012:INFO:Checking exceptions
2022-10-23 13:26:50,017:INFO:Importing libraries
2022-10-23 13:26:50,017:INFO:Copying training dataset
2022-10-23 13:26:50,041:INFO:Defining folds
2022-10-23 13:26:50,042:INFO:Declaring metric variables
2022-10-23 13:26:50,103:INFO:Importing untrained model
2022-10-23 13:26:50,133:INFO:Bayesian Ridge Imported successfully
2022-10-23 13:26:50,205:INFO:Starting cross validation
2022-10-23 13:26:50,209:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:50,785:INFO:Calculating mean and std
2022-10-23 13:26:50,789:INFO:Creating metrics dataframe
2022-10-23 13:26:50,798:INFO:Uploading results into container
2022-10-23 13:26:50,799:INFO:Uploading model into container now
2022-10-23 13:26:50,800:INFO:master_model_container: 8
2022-10-23 13:26:50,800:INFO:display_container: 2
2022-10-23 13:26:50,801:INFO:BayesianRidge()
2022-10-23 13:26:50,801:INFO:create_model() successfully completed......................................
2022-10-23 13:26:51,024:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:51,025:INFO:Creating metrics dataframe
2022-10-23 13:26:51,049:INFO:Initializing Passive Aggressive Regressor
2022-10-23 13:26:51,049:INFO:Total runtime is 0.6343546152114867 minutes
2022-10-23 13:26:51,060:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:51,061:INFO:Initializing create_model()
2022-10-23 13:26:51,061:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=par, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:51,062:INFO:Checking exceptions
2022-10-23 13:26:51,066:INFO:Importing libraries
2022-10-23 13:26:51,066:INFO:Copying training dataset
2022-10-23 13:26:51,140:INFO:Defining folds
2022-10-23 13:26:51,140:INFO:Declaring metric variables
2022-10-23 13:26:51,188:INFO:Importing untrained model
2022-10-23 13:26:51,209:INFO:Passive Aggressive Regressor Imported successfully
2022-10-23 13:26:51,260:INFO:Starting cross validation
2022-10-23 13:26:51,264:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:51,866:INFO:Calculating mean and std
2022-10-23 13:26:51,869:INFO:Creating metrics dataframe
2022-10-23 13:26:51,874:INFO:Uploading results into container
2022-10-23 13:26:51,875:INFO:Uploading model into container now
2022-10-23 13:26:51,876:INFO:master_model_container: 9
2022-10-23 13:26:51,878:INFO:display_container: 2
2022-10-23 13:26:51,879:INFO:PassiveAggressiveRegressor(random_state=122)
2022-10-23 13:26:51,879:INFO:create_model() successfully completed......................................
2022-10-23 13:26:52,110:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:52,110:INFO:Creating metrics dataframe
2022-10-23 13:26:52,132:INFO:Initializing Huber Regressor
2022-10-23 13:26:52,133:INFO:Total runtime is 0.6524143815040587 minutes
2022-10-23 13:26:52,138:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:52,140:INFO:Initializing create_model()
2022-10-23 13:26:52,140:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=huber, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:52,140:INFO:Checking exceptions
2022-10-23 13:26:52,146:INFO:Importing libraries
2022-10-23 13:26:52,146:INFO:Copying training dataset
2022-10-23 13:26:52,194:INFO:Defining folds
2022-10-23 13:26:52,198:INFO:Declaring metric variables
2022-10-23 13:26:52,300:INFO:Importing untrained model
2022-10-23 13:26:52,340:INFO:Huber Regressor Imported successfully
2022-10-23 13:26:52,400:INFO:Starting cross validation
2022-10-23 13:26:52,404:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:53,070:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-23 13:26:53,150:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-23 13:26:53,191:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-23 13:26:53,377:WARNING:/Users/sage/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-23 13:26:53,448:INFO:Calculating mean and std
2022-10-23 13:26:53,452:INFO:Creating metrics dataframe
2022-10-23 13:26:53,461:INFO:Uploading results into container
2022-10-23 13:26:53,463:INFO:Uploading model into container now
2022-10-23 13:26:53,464:INFO:master_model_container: 10
2022-10-23 13:26:53,464:INFO:display_container: 2
2022-10-23 13:26:53,465:INFO:HuberRegressor()
2022-10-23 13:26:53,465:INFO:create_model() successfully completed......................................
2022-10-23 13:26:53,785:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:53,790:INFO:Creating metrics dataframe
2022-10-23 13:26:53,825:INFO:Initializing K Neighbors Regressor
2022-10-23 13:26:53,826:INFO:Total runtime is 0.68063112894694 minutes
2022-10-23 13:26:53,843:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:53,845:INFO:Initializing create_model()
2022-10-23 13:26:53,846:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=knn, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:53,860:INFO:Checking exceptions
2022-10-23 13:26:53,878:INFO:Importing libraries
2022-10-23 13:26:53,878:INFO:Copying training dataset
2022-10-23 13:26:53,906:INFO:Defining folds
2022-10-23 13:26:53,907:INFO:Declaring metric variables
2022-10-23 13:26:53,965:INFO:Importing untrained model
2022-10-23 13:26:53,976:INFO:K Neighbors Regressor Imported successfully
2022-10-23 13:26:54,010:INFO:Starting cross validation
2022-10-23 13:26:54,016:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:54,824:INFO:Calculating mean and std
2022-10-23 13:26:54,827:INFO:Creating metrics dataframe
2022-10-23 13:26:54,835:INFO:Uploading results into container
2022-10-23 13:26:54,837:INFO:Uploading model into container now
2022-10-23 13:26:54,838:INFO:master_model_container: 11
2022-10-23 13:26:54,839:INFO:display_container: 2
2022-10-23 13:26:54,840:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-23 13:26:54,840:INFO:create_model() successfully completed......................................
2022-10-23 13:26:55,078:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:55,078:INFO:Creating metrics dataframe
2022-10-23 13:26:55,121:INFO:Initializing Decision Tree Regressor
2022-10-23 13:26:55,121:INFO:Total runtime is 0.7022230307261148 minutes
2022-10-23 13:26:55,133:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:55,134:INFO:Initializing create_model()
2022-10-23 13:26:55,138:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=dt, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:55,138:INFO:Checking exceptions
2022-10-23 13:26:55,144:INFO:Importing libraries
2022-10-23 13:26:55,144:INFO:Copying training dataset
2022-10-23 13:26:55,176:INFO:Defining folds
2022-10-23 13:26:55,176:INFO:Declaring metric variables
2022-10-23 13:26:55,233:INFO:Importing untrained model
2022-10-23 13:26:55,274:INFO:Decision Tree Regressor Imported successfully
2022-10-23 13:26:55,345:INFO:Starting cross validation
2022-10-23 13:26:55,353:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:55,920:INFO:Calculating mean and std
2022-10-23 13:26:55,923:INFO:Creating metrics dataframe
2022-10-23 13:26:55,937:INFO:Uploading results into container
2022-10-23 13:26:55,939:INFO:Uploading model into container now
2022-10-23 13:26:55,941:INFO:master_model_container: 12
2022-10-23 13:26:55,941:INFO:display_container: 2
2022-10-23 13:26:55,942:INFO:DecisionTreeRegressor(random_state=122)
2022-10-23 13:26:55,942:INFO:create_model() successfully completed......................................
2022-10-23 13:26:56,165:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:56,165:INFO:Creating metrics dataframe
2022-10-23 13:26:56,190:INFO:Initializing Random Forest Regressor
2022-10-23 13:26:56,191:INFO:Total runtime is 0.7200468818346658 minutes
2022-10-23 13:26:56,199:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:56,202:INFO:Initializing create_model()
2022-10-23 13:26:56,202:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=rf, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:56,202:INFO:Checking exceptions
2022-10-23 13:26:56,208:INFO:Importing libraries
2022-10-23 13:26:56,209:INFO:Copying training dataset
2022-10-23 13:26:56,224:INFO:Defining folds
2022-10-23 13:26:56,224:INFO:Declaring metric variables
2022-10-23 13:26:56,291:INFO:Importing untrained model
2022-10-23 13:26:56,314:INFO:Random Forest Regressor Imported successfully
2022-10-23 13:26:56,376:INFO:Starting cross validation
2022-10-23 13:26:56,380:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:26:58,481:INFO:Calculating mean and std
2022-10-23 13:26:58,484:INFO:Creating metrics dataframe
2022-10-23 13:26:58,492:INFO:Uploading results into container
2022-10-23 13:26:58,493:INFO:Uploading model into container now
2022-10-23 13:26:58,494:INFO:master_model_container: 13
2022-10-23 13:26:58,494:INFO:display_container: 2
2022-10-23 13:26:58,495:INFO:RandomForestRegressor(n_jobs=-1, random_state=122)
2022-10-23 13:26:58,495:INFO:create_model() successfully completed......................................
2022-10-23 13:26:58,714:INFO:SubProcess create_model() end ==================================
2022-10-23 13:26:58,714:INFO:Creating metrics dataframe
2022-10-23 13:26:58,744:INFO:Initializing Extra Trees Regressor
2022-10-23 13:26:58,745:INFO:Total runtime is 0.7626163641611734 minutes
2022-10-23 13:26:58,759:INFO:SubProcess create_model() called ==================================
2022-10-23 13:26:58,761:INFO:Initializing create_model()
2022-10-23 13:26:58,761:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=et, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:26:58,761:INFO:Checking exceptions
2022-10-23 13:26:58,771:INFO:Importing libraries
2022-10-23 13:26:58,772:INFO:Copying training dataset
2022-10-23 13:26:58,795:INFO:Defining folds
2022-10-23 13:26:58,796:INFO:Declaring metric variables
2022-10-23 13:26:58,850:INFO:Importing untrained model
2022-10-23 13:26:58,899:INFO:Extra Trees Regressor Imported successfully
2022-10-23 13:26:58,945:INFO:Starting cross validation
2022-10-23 13:26:58,951:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:27:00,701:INFO:Calculating mean and std
2022-10-23 13:27:00,707:INFO:Creating metrics dataframe
2022-10-23 13:27:00,714:INFO:Uploading results into container
2022-10-23 13:27:00,716:INFO:Uploading model into container now
2022-10-23 13:27:00,716:INFO:master_model_container: 14
2022-10-23 13:27:00,716:INFO:display_container: 2
2022-10-23 13:27:00,717:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=122)
2022-10-23 13:27:00,717:INFO:create_model() successfully completed......................................
2022-10-23 13:27:00,942:INFO:SubProcess create_model() end ==================================
2022-10-23 13:27:00,942:INFO:Creating metrics dataframe
2022-10-23 13:27:00,970:INFO:Initializing AdaBoost Regressor
2022-10-23 13:27:00,970:INFO:Total runtime is 0.7997084816296894 minutes
2022-10-23 13:27:00,983:INFO:SubProcess create_model() called ==================================
2022-10-23 13:27:00,983:INFO:Initializing create_model()
2022-10-23 13:27:00,983:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=ada, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:27:00,984:INFO:Checking exceptions
2022-10-23 13:27:00,988:INFO:Importing libraries
2022-10-23 13:27:00,988:INFO:Copying training dataset
2022-10-23 13:27:01,027:INFO:Defining folds
2022-10-23 13:27:01,028:INFO:Declaring metric variables
2022-10-23 13:27:01,060:INFO:Importing untrained model
2022-10-23 13:27:01,097:INFO:AdaBoost Regressor Imported successfully
2022-10-23 13:27:01,148:INFO:Starting cross validation
2022-10-23 13:27:01,152:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:27:01,755:INFO:Calculating mean and std
2022-10-23 13:27:01,759:INFO:Creating metrics dataframe
2022-10-23 13:27:01,766:INFO:Uploading results into container
2022-10-23 13:27:01,767:INFO:Uploading model into container now
2022-10-23 13:27:01,768:INFO:master_model_container: 15
2022-10-23 13:27:01,768:INFO:display_container: 2
2022-10-23 13:27:01,768:INFO:AdaBoostRegressor(random_state=122)
2022-10-23 13:27:01,769:INFO:create_model() successfully completed......................................
2022-10-23 13:27:01,976:INFO:SubProcess create_model() end ==================================
2022-10-23 13:27:01,976:INFO:Creating metrics dataframe
2022-10-23 13:27:02,009:INFO:Initializing Gradient Boosting Regressor
2022-10-23 13:27:02,009:INFO:Total runtime is 0.8170218308766682 minutes
2022-10-23 13:27:02,018:INFO:SubProcess create_model() called ==================================
2022-10-23 13:27:02,018:INFO:Initializing create_model()
2022-10-23 13:27:02,018:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=gbr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:27:02,019:INFO:Checking exceptions
2022-10-23 13:27:02,029:INFO:Importing libraries
2022-10-23 13:27:02,029:INFO:Copying training dataset
2022-10-23 13:27:02,079:INFO:Defining folds
2022-10-23 13:27:02,080:INFO:Declaring metric variables
2022-10-23 13:27:02,107:INFO:Importing untrained model
2022-10-23 13:27:02,130:INFO:Gradient Boosting Regressor Imported successfully
2022-10-23 13:27:02,151:INFO:Starting cross validation
2022-10-23 13:27:02,163:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:27:03,120:INFO:Calculating mean and std
2022-10-23 13:27:03,122:INFO:Creating metrics dataframe
2022-10-23 13:27:03,128:INFO:Uploading results into container
2022-10-23 13:27:03,129:INFO:Uploading model into container now
2022-10-23 13:27:03,130:INFO:master_model_container: 16
2022-10-23 13:27:03,130:INFO:display_container: 2
2022-10-23 13:27:03,131:INFO:GradientBoostingRegressor(random_state=122)
2022-10-23 13:27:03,132:INFO:create_model() successfully completed......................................
2022-10-23 13:27:03,398:INFO:SubProcess create_model() end ==================================
2022-10-23 13:27:03,398:INFO:Creating metrics dataframe
2022-10-23 13:27:03,427:INFO:Initializing Light Gradient Boosting Machine
2022-10-23 13:27:03,427:INFO:Total runtime is 0.8406570315361023 minutes
2022-10-23 13:27:03,442:INFO:SubProcess create_model() called ==================================
2022-10-23 13:27:03,444:INFO:Initializing create_model()
2022-10-23 13:27:03,444:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=lightgbm, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:27:03,444:INFO:Checking exceptions
2022-10-23 13:27:03,451:INFO:Importing libraries
2022-10-23 13:27:03,451:INFO:Copying training dataset
2022-10-23 13:27:03,500:INFO:Defining folds
2022-10-23 13:27:03,500:INFO:Declaring metric variables
2022-10-23 13:27:03,547:INFO:Importing untrained model
2022-10-23 13:27:03,561:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-23 13:27:03,607:INFO:Starting cross validation
2022-10-23 13:27:03,609:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:27:07,889:INFO:Calculating mean and std
2022-10-23 13:27:07,894:INFO:Creating metrics dataframe
2022-10-23 13:27:07,902:INFO:Uploading results into container
2022-10-23 13:27:07,903:INFO:Uploading model into container now
2022-10-23 13:27:07,904:INFO:master_model_container: 17
2022-10-23 13:27:07,904:INFO:display_container: 2
2022-10-23 13:27:07,905:INFO:LGBMRegressor(random_state=122)
2022-10-23 13:27:07,905:INFO:create_model() successfully completed......................................
2022-10-23 13:27:08,175:INFO:SubProcess create_model() end ==================================
2022-10-23 13:27:08,175:INFO:Creating metrics dataframe
2022-10-23 13:27:08,196:INFO:Initializing CatBoost Regressor
2022-10-23 13:27:08,197:INFO:Total runtime is 0.9201470295588176 minutes
2022-10-23 13:27:08,203:INFO:SubProcess create_model() called ==================================
2022-10-23 13:27:08,204:INFO:Initializing create_model()
2022-10-23 13:27:08,204:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=catboost, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:27:08,204:INFO:Checking exceptions
2022-10-23 13:27:08,210:INFO:Importing libraries
2022-10-23 13:27:08,210:INFO:Copying training dataset
2022-10-23 13:27:08,221:INFO:Defining folds
2022-10-23 13:27:08,226:INFO:Declaring metric variables
2022-10-23 13:27:08,384:INFO:Importing untrained model
2022-10-23 13:27:08,445:INFO:CatBoost Regressor Imported successfully
2022-10-23 13:27:08,516:INFO:Starting cross validation
2022-10-23 13:27:08,519:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:27:13,526:INFO:Calculating mean and std
2022-10-23 13:27:13,528:INFO:Creating metrics dataframe
2022-10-23 13:27:13,534:INFO:Uploading results into container
2022-10-23 13:27:13,535:INFO:Uploading model into container now
2022-10-23 13:27:13,536:INFO:master_model_container: 18
2022-10-23 13:27:13,536:INFO:display_container: 2
2022-10-23 13:27:13,536:INFO:<catboost.core.CatBoostRegressor object at 0x7faca75b4250>
2022-10-23 13:27:13,536:INFO:create_model() successfully completed......................................
2022-10-23 13:27:13,801:INFO:SubProcess create_model() end ==================================
2022-10-23 13:27:13,801:INFO:Creating metrics dataframe
2022-10-23 13:27:13,828:INFO:Initializing Dummy Regressor
2022-10-23 13:27:13,829:INFO:Total runtime is 1.014014732837677 minutes
2022-10-23 13:27:13,845:INFO:SubProcess create_model() called ==================================
2022-10-23 13:27:13,846:INFO:Initializing create_model()
2022-10-23 13:27:13,846:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=dummy, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7faca39d9f10>, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:27:13,846:INFO:Checking exceptions
2022-10-23 13:27:13,855:INFO:Importing libraries
2022-10-23 13:27:13,855:INFO:Copying training dataset
2022-10-23 13:27:13,861:INFO:Defining folds
2022-10-23 13:27:13,862:INFO:Declaring metric variables
2022-10-23 13:27:13,916:INFO:Importing untrained model
2022-10-23 13:27:13,986:INFO:Dummy Regressor Imported successfully
2022-10-23 13:27:14,026:INFO:Starting cross validation
2022-10-23 13:27:14,030:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2022-10-23 13:27:14,564:INFO:Calculating mean and std
2022-10-23 13:27:14,566:INFO:Creating metrics dataframe
2022-10-23 13:27:14,571:INFO:Uploading results into container
2022-10-23 13:27:14,574:INFO:Uploading model into container now
2022-10-23 13:27:14,575:INFO:master_model_container: 19
2022-10-23 13:27:14,575:INFO:display_container: 2
2022-10-23 13:27:14,575:INFO:DummyRegressor()
2022-10-23 13:27:14,576:INFO:create_model() successfully completed......................................
2022-10-23 13:27:14,795:INFO:SubProcess create_model() end ==================================
2022-10-23 13:27:14,795:INFO:Creating metrics dataframe
2022-10-23 13:27:14,859:INFO:Initializing create_model()
2022-10-23 13:27:14,860:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=GradientBoostingRegressor(random_state=122), fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-23 13:27:14,860:INFO:Checking exceptions
2022-10-23 13:27:14,921:INFO:Importing libraries
2022-10-23 13:27:14,921:INFO:Copying training dataset
2022-10-23 13:27:14,928:INFO:Defining folds
2022-10-23 13:27:14,929:INFO:Declaring metric variables
2022-10-23 13:27:14,936:INFO:Importing untrained model
2022-10-23 13:27:14,945:INFO:Declaring custom model
2022-10-23 13:27:14,951:INFO:Gradient Boosting Regressor Imported successfully
2022-10-23 13:27:14,954:INFO:Cross validation set to False
2022-10-23 13:27:14,954:INFO:Fitting Model
2022-10-23 13:27:15,173:INFO:GradientBoostingRegressor(random_state=122)
2022-10-23 13:27:15,174:INFO:create_model() successfully completed......................................
2022-10-23 13:27:15,418:INFO:Creating Dashboard logs
2022-10-23 13:27:15,433:INFO:Model: Gradient Boosting Regressor
2022-10-23 13:27:15,544:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 122, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2022-10-23 13:27:15,688:INFO:Initializing predict_model()
2022-10-23 13:27:15,688:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=GradientBoostingRegressor(random_state=122), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca7aff4c0>)
2022-10-23 13:27:15,688:INFO:Checking exceptions
2022-10-23 13:27:15,688:INFO:Preloading libraries
2022-10-23 13:27:18,012:INFO:Creating Dashboard logs
2022-10-23 13:27:18,102:INFO:Model: Random Forest Regressor
2022-10-23 13:27:18,209:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 122, 'verbose': 0, 'warm_start': False}
2022-10-23 13:27:19,219:INFO:Creating Dashboard logs
2022-10-23 13:27:19,292:INFO:Model: Light Gradient Boosting Machine
2022-10-23 13:27:19,481:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 122, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2022-10-23 13:27:20,305:INFO:Creating Dashboard logs
2022-10-23 13:27:20,403:INFO:Model: CatBoost Regressor
2022-10-23 13:27:20,590:WARNING:Couldn't get params for model. Exception:
Traceback (most recent call last):
  File "/Users/sage/opt/anaconda3/lib/python3.9/site-packages/pycaret/loggers/dashboard_logger.py", line 78, in log_model
    params = params.get_all_params()
  File "/Users/sage/opt/anaconda3/lib/python3.9/site-packages/catboost/core.py", line 3413, in get_all_params
    raise CatBoostError("There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.")
_catboost.CatBoostError: There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.

2022-10-23 13:27:20,590:INFO:Logged params: {}
2022-10-23 13:27:21,799:INFO:Creating Dashboard logs
2022-10-23 13:27:21,821:INFO:Model: AdaBoost Regressor
2022-10-23 13:27:22,016:INFO:Logged params: {'base_estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 122}
2022-10-23 13:27:22,991:INFO:Creating Dashboard logs
2022-10-23 13:27:22,998:INFO:Model: Extra Trees Regressor
2022-10-23 13:27:23,081:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 122, 'verbose': 0, 'warm_start': False}
2022-10-23 13:27:23,850:INFO:Creating Dashboard logs
2022-10-23 13:27:23,863:INFO:Model: Bayesian Ridge
2022-10-23 13:27:24,051:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'normalize': 'deprecated', 'tol': 0.001, 'verbose': False}
2022-10-23 13:27:25,094:INFO:Creating Dashboard logs
2022-10-23 13:27:25,120:INFO:Model: Least Angle Regression
2022-10-23 13:27:25,190:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 122, 'verbose': False}
2022-10-23 13:27:26,190:INFO:Creating Dashboard logs
2022-10-23 13:27:26,209:INFO:Model: Least Angle Regression
2022-10-23 13:27:26,309:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 122, 'verbose': False}
2022-10-23 13:27:27,354:INFO:Creating Dashboard logs
2022-10-23 13:27:27,370:INFO:Model: Ridge Regression
2022-10-23 13:27:27,607:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': 122, 'solver': 'auto', 'tol': 0.001}
2022-10-23 13:27:28,390:INFO:Creating Dashboard logs
2022-10-23 13:27:28,445:INFO:Model: Lasso Regression
2022-10-23 13:27:28,600:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 122, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2022-10-23 13:27:29,809:INFO:Creating Dashboard logs
2022-10-23 13:27:29,837:INFO:Model: Linear Regression
2022-10-23 13:27:29,987:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'normalize': 'deprecated', 'positive': False}
2022-10-23 13:27:30,655:INFO:Creating Dashboard logs
2022-10-23 13:27:30,692:INFO:Model: Decision Tree Regressor
2022-10-23 13:27:30,789:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 122, 'splitter': 'best'}
2022-10-23 13:27:31,653:INFO:Creating Dashboard logs
2022-10-23 13:27:31,796:INFO:Model: Huber Regressor
2022-10-23 13:27:31,945:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2022-10-23 13:27:33,022:INFO:Creating Dashboard logs
2022-10-23 13:27:33,050:INFO:Model: Orthogonal Matching Pursuit
2022-10-23 13:27:33,150:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2022-10-23 13:27:34,426:INFO:Creating Dashboard logs
2022-10-23 13:27:34,495:INFO:Model: Passive Aggressive Regressor
2022-10-23 13:27:34,611:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 122, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2022-10-23 13:27:35,760:INFO:Creating Dashboard logs
2022-10-23 13:27:35,777:INFO:Model: Elastic Net
2022-10-23 13:27:35,835:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 122, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2022-10-23 13:27:36,795:INFO:Creating Dashboard logs
2022-10-23 13:27:36,876:INFO:Model: K Neighbors Regressor
2022-10-23 13:27:37,014:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2022-10-23 13:27:38,078:INFO:Creating Dashboard logs
2022-10-23 13:27:38,161:INFO:Model: Dummy Regressor
2022-10-23 13:27:38,376:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2022-10-23 13:27:39,779:INFO:master_model_container: 19
2022-10-23 13:27:39,779:INFO:display_container: 2
2022-10-23 13:27:39,780:INFO:GradientBoostingRegressor(random_state=122)
2022-10-23 13:27:39,781:INFO:compare_models() successfully completed......................................
2022-10-23 13:52:03,362:INFO:Initializing evaluate_model()
2022-10-23 13:52:03,366:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=GradientBoostingRegressor(random_state=122), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-10-23 13:52:03,531:INFO:Initializing plot_model()
2022-10-23 13:52:03,532:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:03,532:INFO:Checking exceptions
2022-10-23 13:52:03,670:INFO:Preloading libraries
2022-10-23 13:52:03,727:INFO:Copying training dataset
2022-10-23 13:52:03,727:INFO:Plot type: pipeline
2022-10-23 13:52:05,183:INFO:Visual Rendered Successfully
2022-10-23 13:52:07,106:INFO:plot_model() successfully completed......................................
2022-10-23 13:52:08,753:INFO:Initializing plot_model()
2022-10-23 13:52:08,758:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:08,759:INFO:Checking exceptions
2022-10-23 13:52:08,767:INFO:Preloading libraries
2022-10-23 13:52:08,798:INFO:Copying training dataset
2022-10-23 13:52:08,799:INFO:Plot type: parameter
2022-10-23 13:52:08,811:INFO:Visual Rendered Successfully
2022-10-23 13:52:09,277:INFO:plot_model() successfully completed......................................
2022-10-23 13:52:13,334:INFO:Initializing plot_model()
2022-10-23 13:52:13,335:INFO:plot_model(plot=residuals, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:13,335:INFO:Checking exceptions
2022-10-23 13:52:13,342:INFO:Preloading libraries
2022-10-23 13:52:13,368:INFO:Copying training dataset
2022-10-23 13:52:13,368:INFO:Plot type: residuals
2022-10-23 13:52:13,676:INFO:Fitting Model
2022-10-23 13:52:13,736:INFO:Scoring test/hold-out set
2022-10-23 13:52:14,713:INFO:Visual Rendered Successfully
2022-10-23 13:52:15,029:INFO:plot_model() successfully completed......................................
2022-10-23 13:52:17,615:INFO:Initializing plot_model()
2022-10-23 13:52:17,616:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:17,616:INFO:Checking exceptions
2022-10-23 13:52:17,620:INFO:Preloading libraries
2022-10-23 13:52:17,647:INFO:Copying training dataset
2022-10-23 13:52:17,647:INFO:Plot type: error
2022-10-23 13:52:17,874:INFO:Fitting Model
2022-10-23 13:52:17,874:INFO:Scoring test/hold-out set
2022-10-23 13:52:18,443:INFO:Visual Rendered Successfully
2022-10-23 13:52:18,881:INFO:plot_model() successfully completed......................................
2022-10-23 13:52:19,255:INFO:Initializing plot_model()
2022-10-23 13:52:19,255:INFO:plot_model(plot=residuals_interactive, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:19,255:INFO:Checking exceptions
2022-10-23 13:52:19,266:INFO:Preloading libraries
2022-10-23 13:52:19,286:INFO:Copying training dataset
2022-10-23 13:52:19,286:INFO:Plot type: residuals_interactive
2022-10-23 13:52:19,502:INFO:Calculated model residuals
2022-10-23 13:52:20,653:INFO:Calculated Tunkey-Anscombe Plot
2022-10-23 13:52:21,023:INFO:Calculated Normal QQ Plot
2022-10-23 13:52:21,489:INFO:Calculated Scale-Location Plot
2022-10-23 13:52:21,969:INFO:Calculated Residual vs Leverage Plot inc. Cook's distance
2022-10-23 13:52:22,620:INFO:Visual Rendered Successfully
2022-10-23 13:52:22,810:INFO:plot_model() successfully completed......................................
2022-10-23 13:52:27,716:INFO:Initializing plot_model()
2022-10-23 13:52:27,718:INFO:plot_model(plot=feature_all, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:27,719:INFO:Checking exceptions
2022-10-23 13:52:27,728:INFO:Preloading libraries
2022-10-23 13:52:27,751:INFO:Copying training dataset
2022-10-23 13:52:27,751:INFO:Plot type: feature_all
2022-10-23 13:52:27,899:WARNING:No coef_ found. Trying feature_importances_
2022-10-23 13:52:28,736:INFO:Visual Rendered Successfully
2022-10-23 13:52:29,578:INFO:plot_model() successfully completed......................................
2022-10-23 13:52:39,588:INFO:Initializing plot_model()
2022-10-23 13:52:39,588:INFO:plot_model(plot=tree, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:39,588:INFO:Checking exceptions
2022-10-23 13:52:47,060:INFO:Initializing plot_model()
2022-10-23 13:52:47,065:INFO:plot_model(plot=residuals_interactive, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=122), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:52:47,066:INFO:Checking exceptions
2022-10-23 13:52:47,071:INFO:Preloading libraries
2022-10-23 13:52:47,098:INFO:Copying training dataset
2022-10-23 13:52:47,098:INFO:Plot type: residuals_interactive
2022-10-23 13:52:47,379:INFO:Calculated model residuals
2022-10-23 13:52:48,143:INFO:Calculated Tunkey-Anscombe Plot
2022-10-23 13:52:48,602:INFO:Calculated Normal QQ Plot
2022-10-23 13:52:49,103:INFO:Calculated Scale-Location Plot
2022-10-23 13:52:49,527:INFO:Calculated Residual vs Leverage Plot inc. Cook's distance
2022-10-23 13:52:50,045:INFO:Visual Rendered Successfully
2022-10-23 13:52:50,228:INFO:plot_model() successfully completed......................................
2022-10-23 13:54:09,297:INFO:Initializing plot_model()
2022-10-23 13:54:09,299:INFO:plot_model(plot=residuals, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:54:09,300:INFO:Checking exceptions
2022-10-23 13:54:09,315:INFO:Preloading libraries
2022-10-23 13:54:09,348:INFO:Copying training dataset
2022-10-23 13:54:09,348:INFO:Plot type: residuals
2022-10-23 13:54:10,355:INFO:Fitting Model
2022-10-23 13:54:10,616:INFO:Scoring test/hold-out set
2022-10-23 13:54:13,163:INFO:Visual Rendered Successfully
2022-10-23 13:54:13,447:INFO:plot_model() successfully completed......................................
2022-10-23 13:54:15,241:INFO:Initializing plot_model()
2022-10-23 13:54:15,241:INFO:plot_model(plot=feature, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, system=True)
2022-10-23 13:54:15,241:INFO:Checking exceptions
2022-10-23 13:54:15,258:INFO:Preloading libraries
2022-10-23 13:54:15,286:INFO:Copying training dataset
2022-10-23 13:54:15,288:INFO:Plot type: feature
2022-10-23 13:54:15,290:WARNING:No coef_ found. Trying feature_importances_
2022-10-23 13:54:15,766:INFO:Visual Rendered Successfully
2022-10-23 13:54:15,979:INFO:plot_model() successfully completed......................................
2022-10-23 13:54:25,932:INFO:Initializing predict_model()
2022-10-23 13:54:25,933:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca3d31550>)
2022-10-23 13:54:25,933:INFO:Checking exceptions
2022-10-23 13:54:25,933:INFO:Preloading libraries
2022-10-23 13:54:51,771:INFO:Initializing predict_model()
2022-10-23 13:54:51,773:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7facbfab8190>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7faca3d315e0>)
2022-10-23 13:54:51,773:INFO:Checking exceptions
2022-10-23 13:54:51,777:INFO:Preloading libraries
2022-10-23 13:54:51,784:INFO:Set up data.
2022-10-23 13:54:51,796:INFO:Set up index.
2022-10-23 13:54:57,847:INFO:Initializing save_model()
2022-10-23 13:54:57,851:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-23 13:54:57,852:INFO:Adding model into prep_pipe
2022-10-23 13:54:57,899:WARNING:Only Model saved as it was a pipeline.
2022-10-23 13:54:57,948:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-23 13:54:58,074:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-23 13:54:58,074:INFO:save_model() successfully completed......................................
2022-10-23 13:56:45,170:INFO:Initializing save_model()
2022-10-23 13:56:45,172:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-23 13:56:45,172:INFO:Adding model into prep_pipe
2022-10-23 13:56:45,211:WARNING:Only Model saved as it was a pipeline.
2022-10-23 13:56:45,262:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-23 13:56:45,321:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-23 13:56:45,323:INFO:save_model() successfully completed......................................
2022-10-23 13:57:38,790:INFO:Initializing save_model()
2022-10-23 13:57:38,792:INFO:save_model(model=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), model_name=MLOps_Pipeline, prep_pipe_=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                               mapping=[{'col': 'sex',
                                                                         'mapping': {nan: -1,
                                                                                     'female': 0,
                                                                                     'male': 1}},
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-23 13:57:38,792:INFO:Adding model into prep_pipe
2022-10-23 13:57:38,830:WARNING:Only Model saved as it was a pipeline.
2022-10-23 13:57:38,888:INFO:MLOps_Pipeline.pkl saved in current working directory
2022-10-23 13:57:38,928:INFO:Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))])
2022-10-23 13:57:38,928:INFO:save_model() successfully completed......................................
2022-10-23 13:57:59,614:INFO:Initializing load_model()
2022-10-23 13:57:59,615:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-23 13:58:28,368:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 13:58:28,373:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 13:58:28,374:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 13:58:28,374:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 13:58:32,891:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-23 13:58:34,285:INFO:Initializing load_model()
2022-10-23 13:58:34,286:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-23 14:06:42,948:INFO:Initializing predict_model()
2022-10-23 14:06:42,951:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f969b7a1820>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f969b76c1f0>)
2022-10-23 14:06:42,952:INFO:Checking exceptions
2022-10-23 14:06:42,952:INFO:Preloading libraries
2022-10-23 14:06:42,954:INFO:Set up data.
2022-10-23 14:06:43,001:INFO:Set up index.
2022-10-23 14:08:04,947:INFO:Initializing predict_model()
2022-10-23 14:08:04,950:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f969b777460>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f969b3858b0>)
2022-10-23 14:08:04,950:INFO:Checking exceptions
2022-10-23 14:08:04,950:INFO:Preloading libraries
2022-10-23 14:08:04,950:INFO:Set up data.
2022-10-23 14:08:04,956:INFO:Set up index.
2022-10-23 16:14:05,664:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 16:14:05,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 16:14:05,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 16:14:05,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 16:14:08,828:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-23 16:14:10,349:INFO:Initializing load_model()
2022-10-23 16:14:10,350:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-23 16:14:19,333:INFO:Initializing predict_model()
2022-10-23 16:14:19,333:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f8c17f86df0>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f8c17f6b280>)
2022-10-23 16:14:19,333:INFO:Checking exceptions
2022-10-23 16:14:19,333:INFO:Preloading libraries
2022-10-23 16:14:19,334:INFO:Set up data.
2022-10-23 16:14:19,340:INFO:Set up index.
2022-10-23 16:14:37,524:INFO:Initializing predict_model()
2022-10-23 16:14:37,524:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f8c17f63b80>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f8c17f6bca0>)
2022-10-23 16:14:37,524:INFO:Checking exceptions
2022-10-23 16:14:37,524:INFO:Preloading libraries
2022-10-23 16:14:37,524:INFO:Set up data.
2022-10-23 16:14:37,541:INFO:Set up index.
2022-10-23 18:05:59,699:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 18:05:59,701:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 18:05:59,701:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 18:05:59,701:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 18:06:02,902:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-23 18:06:04,362:INFO:Initializing load_model()
2022-10-23 18:06:04,362:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-23 18:06:31,043:INFO:Initializing predict_model()
2022-10-23 18:06:31,044:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f7fb2820a30>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7f7fb27eb1f0>)
2022-10-23 18:06:31,045:INFO:Checking exceptions
2022-10-23 18:06:31,045:INFO:Preloading libraries
2022-10-23 18:06:31,045:INFO:Set up data.
2022-10-23 18:06:31,052:INFO:Set up index.
2022-10-23 19:21:06,824:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 19:21:06,826:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 19:21:06,826:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 19:21:06,827:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-23 19:21:10,042:INFO:Soft dependency imported: prophet: 1.1.1
2022-10-23 19:21:11,439:INFO:Initializing load_model()
2022-10-23 19:21:11,439:INFO:load_model(model_name=MLOps_Pipeline, platform=None, authentication=None, verbose=True)
2022-10-23 19:21:31,123:INFO:Initializing predict_model()
2022-10-23 19:21:31,124:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc925fa0a00>, estimator=Pipeline(memory=Memory(location=/var/folders/sg/8dmzq1nn0wl6qmndt2sf_m1r0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('or...
                                                                        {'col': 'smoker',
                                                                         'mapping': {nan: -1,
                                                                                     'no': 0,
                                                                                     'yes': 1}}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', GradientBoostingRegressor(random_state=122))]), probability_threshold=None, encoded_labels=False, raw_score=False, drift_report=False, round=0, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x7fc925f6e1f0>)
2022-10-23 19:21:31,125:INFO:Checking exceptions
2022-10-23 19:21:31,125:INFO:Preloading libraries
2022-10-23 19:21:31,126:INFO:Set up data.
2022-10-23 19:21:31,149:INFO:Set up index.
